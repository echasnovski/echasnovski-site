[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n      Default\n      \n        project\n      \n      \n        website\n      \n      \n        format\n      \n      \n        brand\n      \n      \n        language\n      \n      \n        freeze\n      \n      \n        Title\n      \n      \n        Author\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Categories\n      \n      \n        path\n      \n      \n        outputHref\n      \n      \n        Modified - Oldest\n      \n      \n        Modified - Newest\n      \n      \n        Description\n      \n      \n        File Name\n      \n      \n        Modified - Oldest\n      \n      \n        Modified - Newest\n      \n      \n        Reading Time (Low to High)\n      \n      \n        Reading Time (High to Low)\n      \n      \n        Word Count (Low to High)\n      \n      \n        Word Count (High to Low)\n      \n      \n        slug\n      \n    \n  \n    \n      \n      \n    \n\n\n\n  \n    \n      Neovim now has built-in plugin manager\n    \n    \n      2025-07-04 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Neovim + mini.pick + nushell = CLI fuzzy picker. Why? Because why not.\n    \n    \n      2025-06-20 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      PSA for color scheme authors: you might want to adjust `PmenuSel`\n    \n    \n      2024-08-29 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      You can remove padding around Neovim instance with this one simple trick...\n    \n    \n      2024-08-01 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Neovim now has built-in commenting\n    \n    \n      2024-04-05 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Exactly 10 years ago Neovim had its first commit\n    \n    \n      2024-01-31 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Neovim now has its own default color scheme\n    \n    \n      2023-12-03 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Floating windows in Neovim can now have footer\n    \n    \n      2023-08-26 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Average Neovim color scheme\n    \n    \n      2023-04-24 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      You don't need 'vim-rooter' (usually) or How to set up smart autochange of current directory\n    \n    \n      2022-12-29 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Results of \"Neovim built-in options survey\"\n    \n    \n      2022-12-08 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Neovim built-in options survey needs your contribution\n    \n    \n      2022-11-22 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Create and apply randomly generated Base16 color scheme\n    \n    \n      2022-09-09 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Usage of 'after/ftplugin' directory for filetype-specific configuration\n    \n    \n      2022-09-02 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      I contributed to (mostly) 14 top-rated Neovim color schemes. Here are some observations\n    \n    \n      2022-07-20 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      StyLua now supports collapsing simple statements\n    \n    \n      2022-07-07 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Useful functions to explore Lua objects\n    \n    \n      2021-08-20 Evgeni Chasnovski\n      \n        neovim\n      \n        reddit\n      \n    \n    \n      \n    \n  \n\n  \n    \n      Count bounces in table tennis world record\n    \n    \n      2020-05-11 Evgeni Chasnovski\n      \n        python\n      \n        curiosity-project\n      \n        questionflow\n      \n    \n    \n      On May 7th 2020 Daniel Ives set a new world record by continuously bouncing table tennis ball for 5 hours, 21 minutes, and 4 seconds. But how many bounces did he actually make?\n    \n  \n\n  \n    \n      Statistical uncertainty with R and pdqr\n    \n    \n      2019-11-11 Evgeni Chasnovski\n      \n        rstats\n      \n        pdqr\n      \n        questionflow\n      \n    \n    \n      CRAN has accepted my 'pdqr' package. Here are important examples of how it can be used to describe and evaluate statistical uncertainty\n    \n  \n\n  \n    \n      Local randomness in R\n    \n    \n      2019-08-13 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      One approach of using random number generation inside a function without affecting outer state of random generator\n    \n  \n\n  \n    \n      Arguments of stats::density()\n    \n    \n      2019-08-06 Evgeni Chasnovski\n      \n        rstats\n      \n        pdqr\n      \n        questionflow\n      \n    \n    \n      Animated illustrations of how arguments affect output of `stats::density()`\n    \n  \n\n  \n    \n      Announcing pdqr\n    \n    \n      2019-08-01 Evgeni Chasnovski\n      \n        rstats\n      \n        pdqr\n      \n        questionflow\n      \n    \n    \n      Announcing 'pdqr': package for working with custom distribution functions\n    \n  \n\n  \n    \n      Transformers, glue!\n    \n    \n      2018-08-21 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      General description of transformers functionality in {glue} with some potentially useful examples\n    \n  \n\n  \n    \n      Elo and EloBeta models in snooker\n    \n    \n      2018-07-03 Evgeni Chasnovski\n      \n        rstats\n      \n        curiosity-project\n      \n        questionflow\n      \n    \n    \n      Research about adequacy of Elo based models applied to snooker match results. Contains a novel approach (EloBeta) targeted for sport results with variable \"best of N\" format\n    \n  \n\n  \n    \n      Animating mode variability with tidyverse and tweenr\n    \n    \n      2018-06-14 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Provided different probability distributions, animate independent sample distributions to demonstrate mode variability. There is a thorough code description with some subtle tips and tricks\n    \n  \n\n  \n    \n      Harry Potter and rankings with comperank\n    \n    \n      2018-05-31 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Ranking Harry Potter books with comperank package\n    \n  \n\n  \n    \n      General gems of comperes\n    \n    \n      2018-05-17 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Examples of exported functions from comperes package that could be useful for general tasks\n    \n  \n\n  \n    \n      Harry Potter and competition results with comperes\n    \n    \n      2018-05-09 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Exploration of Harry Potter Books Survey results with help of my new comperes package\n    \n  \n\n  \n    \n      Struggle with Harry Potter Data\n    \n    \n      2018-04-09 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Notes about creation of Harry Potter Books Survey. It is not over, I need your help\n    \n  \n\n  \n    \n      Tao of Tidygraph\n    \n    \n      2018-03-06 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Analysis of one interesting alliance graph within tidy network analysis framework\n    \n  \n\n  \n    \n      Combined outlier detection with dplyr and ruler\n    \n    \n      2017-12-26 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Overview of simple outlier detection methods with their combination using dplyr and ruler packages\n    \n  \n\n  \n    \n      Usage of ruler package\n    \n    \n      2017-12-05 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Usage examples of ruler package: dplyr-style exploration and validation of data frame like objects\n    \n  \n\n  \n    \n      Rule Your Data with Tidy Validation Reports. Design\n    \n    \n      2017-11-28 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      The story about design of ruler package: dplyr-style exploration and validation of data frame like objects\n    \n  \n\n  \n    \n      Store Data About Rows\n    \n    \n      2017-11-20 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Introduction to keyholder package. Tools for keeping track of information about rows\n    \n  \n\n  \n    \n      Mythical Generic Overhead\n    \n    \n      2017-11-05 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Computational overhead analysis of using generic+method approach instead of if-else sequence and switch statement\n    \n  \n\n  \n    \n      Highlight the Pipe. Pkgdown\n    \n    \n      2017-10-29 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Practical advices about customizing code highlighting on web pages created with pkgdown\n    \n  \n\n  \n    \n      Highlight the Pipe. Highlight.js\n    \n    \n      2017-10-20 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      Practical advices about customizing code highlighting on web pages with highlight.js\n    \n  \n\n  \n    \n      How to Scrape Pdf and Rmd to Get Inspiration\n    \n    \n      2017-10-13 Evgeni Chasnovski\n      \n        rstats\n      \n        questionflow\n      \n    \n    \n      The story of QuestionFlow origins\n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Neovim\n\nmini.nvim - Library of 40+ independent Lua modules improving overall Neovim experience. Creator and maintainer.\nNeovim - Vim-fork focused on extensibility and usability. Team member.\n\n\n\nR\n\npdqr Package with extensive functionality for creating, transforming, and summarizing custom random variables. Creator and maintainer.\nruler Functionality for creating tidy data validation reports. Creator and maintainer.\ncomperank Implementation of multiple algorithms for computing ranking and rating based on competition results. Creator and maintainer.\ninfer Tools for tidy statistical inference. Extensively refactored and enhanced code base, which wasn’t initially mine. Team member.\n\n\n\nPython\n\nrandomvars Scikit package for applied work with random variables. Creator and maintainer."
  },
  {
    "objectID": "blog/2024-01-31-neovim-10-year-anniversary.html",
    "href": "blog/2024-01-31-neovim-10-year-anniversary.html",
    "title": "Exactly 10 years ago Neovim had its first commit",
    "section": "",
    "text": "Originally posted on Reddit\nA link to first commit.\nTo celebrate this, Neovim team created an official store: https://store.neovim.io. It should be live now. All profit will go to benefit Neovim project."
  },
  {
    "objectID": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html",
    "href": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html",
    "title": "Animating mode variability with tidyverse and tweenr",
    "section": "",
    "text": "Not so long time ago I encountered the following task: given several groups of samples (one group - several samples from one distribution) make a visual presentation of sample mode variability in different groups (how sample mode changes between different samples). I decided to do this by animating these distributions with respect to their mode. The whole process can be summarised as follows:\n\nFor every sample:\n\nCount its values. Sample values are considered to a certain degree of precision, e.g. to the third digit, so talking about “counting values” has reasonable background.\nDivide counts by maximum count. The output is named “mode share distribution” (made up term, didn’t find any present one). For every unique value in sample the outcome is a number between 0 (value is totally absent) and 1 (value is a strict mode). Here value is considered to be “mode” if its “mode share” is above some threshold, say 0.9. For strict thresholds 1 is used.\n\nFor every set of group samples (first samples within groups, second samples, and so on) plot their “mode share distribution”. This is done by plotting several “mode share distributions” at once, in non-overlapping ridgeline-like fashion.\nAnimate between plots. This simultaneously shows a mode variability in different groups. As just changing pictures didn’t seem like a challenge, I decided to make smooth transitions between plots. Packages tweenr (CRAN version) and gganimate (original one by David Robinson, not a soon-to-be-revolutional one by Thomas Lin Pedersen) provide core functionality for this.\n\nMethods I ended up using (some of them were discovered during solving this task) contain surprisingly many subtle base R and tidyverse tricks. This motivated me to write a post about my solution. It heavily uses core tidyverse packages.\nFor educational purposes, I will slightly change the task: provided different probability distributions (by random generation R functions like rnorm), animate independent sample distributions to demonstrate mode variability."
  },
  {
    "objectID": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#plotting-data",
    "href": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#plotting-data",
    "title": "Animating mode variability with tidyverse and tweenr",
    "section": "Plotting data",
    "text": "Plotting data\n\n\n\n\n\n\nCode for computing plotting data\n\n\n\n\n\n# Compute data for plot. Basically, it:\n  # - Counts values in each group.\n  # - Normalizes by maximum count within each group (producing 'mode shares').\n  # - Computes whether certain value is mode according to rule \"`mode share` is\n    # note less then `mode_thres`\".\n  # - Produces plotting data (including colour and size of segments).\n# Here `tbl` should have 'distr' and 'value' columns.\n# `mode_thres` represents minimum 'mode share' for value to be considered mode.\nget_plot_data &lt;- function(tbl, mode_thres = 1) {\n  tbl %&gt;%\n    # Compute mode share distribution by counting value occurence in groups and\n      # normalize by maximum count within groups.\n    group_by(distr, value) %&gt;%\n    summarise(n = n()) %&gt;%\n    mutate(\n      modeShare = n / max(n),\n      isMode = modeShare &gt;= mode_thres\n    ) %&gt;%\n    ungroup() %&gt;%\n    # Prepare plot data\n    transmute(\n      distr, x = value,\n      # Distributions are plotted on integer levels of groups.\n      # Using factor 'distr' column is a way to control vertical order of\n        # distributions.\n      y = as.integer(as.factor(distr)),\n      # Here using 0.9 ensures that segments won't overlap\n      yend = y + modeShare * 0.9,\n      isMode,\n      colour = if_else(isMode, \"red\", \"black\"),\n      size = if_else(isMode, 2, 1)\n    )\n}\n\n\n\nFunction get_plot_data() takes tibble of samples and mode threshold (minimum ‘mode share’ for value to be considered mode). It produces output with one row per segment in the following format:\n\ndistr &lt;same type as in input&gt; : Name of distribution. Will be used as labels.\nx &lt;dbl&gt; : Coordinates of segment x axis (which is enough to define x coordinate of vertical segments).\ny &lt;dbl&gt; : The y coordinate of lower end of a segment. To control this, supply factor distr column: different distributions will be plotted at integer y coordinates in order defined by factor levels (from bottom to top).\nyend &lt;dbl&gt; : The y coordinate of upper end of a segment.\nisMode &lt;lgl&gt; : Logical value indicating whether this segment represents mode value.\ncolour &lt;chr&gt; : Colour of the segment (red for modes, black for others).\nsize &lt;dbl&gt; : Size of the segment line (2 for modes, 1 for others).\n\nget_plot_data(distr_data)\n## # A tibble: 297 x 7\n##   distr     x     y  yend isMode colour  size\n##   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;chr&gt;  &lt;dbl&gt;\n## 1 rbeta 0.19      1  1.02 FALSE  black      1\n## 2 rbeta 0.28      1  1.02 FALSE  black      1\n## 3 rbeta 0.290     1  1.02 FALSE  black      1\n## 4 rbeta 0.32      1  1.02 FALSE  black      1\n## 5 rbeta 0.33      1  1.02 FALSE  black      1\n## # ... with 292 more rows\nget_plot_data(distr_data, mode_thres = 0.01)\n## # A tibble: 297 x 7\n##   distr     x     y  yend isMode colour  size\n##   &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;chr&gt;  &lt;dbl&gt;\n## 1 rbeta 0.19      1  1.02 TRUE   red        2\n## 2 rbeta 0.28      1  1.02 TRUE   red        2\n## 3 rbeta 0.290     1  1.02 TRUE   red        2\n## 4 rbeta 0.32      1  1.02 TRUE   red        2\n## 5 rbeta 0.33      1  1.02 TRUE   red        2\n## # ... with 292 more rows\ndistr_data %&gt;%\n  mutate(distr = factor(distr, levels = c(\"runif\", \"rnorm\", \"rbeta\"))) %&gt;%\n  get_plot_data()\n## # A tibble: 297 x 7\n##   distr     x     y  yend isMode colour  size\n##   &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;lgl&gt;  &lt;chr&gt;  &lt;dbl&gt;\n## 1 runif  0        1  1.28 FALSE  black      1\n## 2 runif  0.01     1  1.39 FALSE  black      1\n## 3 runif  0.02     1  1.51 FALSE  black      1\n## 4 runif  0.03     1  1.51 FALSE  black      1\n## 5 runif  0.04     1  1.45 FALSE  black      1\n## # ... with 292 more rows\nTips and tricks:\n\nWhile plotting several distributions organized vertically, one can compute the base y coordinate for them with as.integer(as.factor(distr)). Using factor distr column is a way to control vertical order of distributions."
  },
  {
    "objectID": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#creating-plots",
    "href": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#creating-plots",
    "title": "Animating mode variability with tidyverse and tweenr",
    "section": "Creating plots",
    "text": "Creating plots\n\n\n\n\n\n\nCode for creating static plots\n\n\n\n\n\n# Adds segment layers. May be used in both static and animated plots.\n  # In static plots should be preceded by `ggplot(data)` in which `data` should\n    # have column `.frame` with all 1.\n  # In animated plots should be preceded by\n    # `ggplot(data_tweened, aes(frame = .frame))`.\n# Returns a list with results of `ggplot2` constructor functions. Elements will\n  # be used sequentially to build plot. This list should be added to `ggplot()`\n  # call with `+` function (like other `ggplot2` functionality).\nadd_layers &lt;- function() {\n  common_aes &lt;- aes(x = x, xend = x, y = y, yend = yend,\n                    colour = colour, size = size)\n  list(\n    # Plotting segments in two different layers to highlight mode segments.\n    geom_segment(\n      # Value of `data` argument in layer function can be a function! It will be\n        # applied to present data initialized in `ggplot()` call.\n      # Notation `. %&gt;% ...` creates a functional sequence.\n      data = . %&gt;% filter(!isMode), mapping = common_aes\n    ),\n    geom_segment(data = . %&gt;% filter(isMode), mapping = common_aes),\n    # Explicitly label distributions.\n    geom_label(\n      data = . %&gt;%\n        # Compute all possible labels in case of factor `distr`\n        distinct(.frame, distr) %&gt;%\n        complete(.frame, distr) %&gt;%\n        # Position label on the left side and in the middle of the group plot\n        mutate(x = -Inf, y = as.integer(as.factor(distr)) + 0.5),\n      mapping = aes(x = x, y = y, label = distr),\n      size = 5, hjust = -0.1\n    ),\n    # Tweak axes labels\n    scale_x_continuous(name = NULL),\n    scale_y_continuous(\n      name = NULL, labels = NULL, breaks = NULL, minor_breaks = NULL\n    ),\n    scale_colour_identity(),\n    scale_size_identity(),\n    # Manually fix plot range for x axis to (0, 1).\n    coord_cartesian(xlim = c(0, 1)),\n    # Add common subtitle to describe plot. Wrap `labs()` in `list()` as\n      # `labs()` itself returns a list which will be appended in a wrong way.\n    list(labs(\n      subtitle = paste0(\n        c(\"Rows represent counts of samples from different distributions.\",\n          \"Actual counts are normalized by maximum count to plot mode share.\",\n          \"Modes (in red) are values with mode share above some threshold.\"),\n        collapse = \"\\n\"\n      )\n    ))\n  )\n}\n\n# Wrapper for constructing static plot\nmake_static_plot &lt;- function(plot_data, title = NULL) {\n  plot_data %&gt;%\n    mutate(.frame = 1) %&gt;%\n    ggplot() +\n      add_layers() +\n      labs(title = title)\n}\n\n\n\nFunction make_static_plot() expects plot data (as output of get_plot_data()) and title of the plot. It returns a ggplot object with many segments and zooming into (0, 1) in x-axis.\nTips and tricks:\n\nFor effective code reuse while “ggplotting”, one can create custom function custom_fun() which should return a list with results of ggplot2 constructor functions. Usually, they are geoms but can be scales, theme elements and so on. This function can then be used as function layer with ggplot(data) + custom_fun().\nValue of data argument in layer function can be a function! This one I discovered during solving this task and was pleasantly surprised by this functionality. A function will be applied to present data initialized in ggplot() call. It is very useful when one needs to plot object for one subset of data over the other. A handy way to create those functions are with . %&gt;% ... notation which creates a functional sequence (one of the features of magrittr pipe). Here this trick is used to plot segments for mode values over the other segments."
  },
  {
    "objectID": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#example-plots",
    "href": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#example-plots",
    "title": "Animating mode variability with tidyverse and tweenr",
    "section": "Example plots",
    "text": "Example plots\ndistr_data %&gt;%\n  get_plot_data() %&gt;%\n  make_static_plot(\n    title = \"Mode share distributions for samples rounded to 2 digits\"\n  )\n\ndistr_data %&gt;%\n  get_plot_data(mode_thres = 0.8) %&gt;%\n  make_static_plot(\n    title = paste0(\n      c(\"Mode share distributions for samples rounded to 2 digits.\",\n        \"Relaxed definition of mode\"),\n      collapse = \" \"\n    )\n  )\n\ndistr_data %&gt;%\n  mutate(distr = factor(distr, levels = c(\"rnorm\", \"runif\", \"rbeta\"))) %&gt;%\n  get_plot_data() %&gt;%\n  make_static_plot(\n    title = \"Control order of distributions with factor `distr` column\"\n  )"
  },
  {
    "objectID": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#transitions",
    "href": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#transitions",
    "title": "Animating mode variability with tidyverse and tweenr",
    "section": "Transitions",
    "text": "Transitions\n\n\n\n\n\n\nCode for creating transitions\n\n\n\n\n\n# Analogous to `findInterval()` but:\n  # - Returns not the index of \"left\" `vec` element of interval but index of\n    # nearest element between two.\n  # - `vec` can be unordered.\nfind_nearest_ind &lt;- function(x, vec) {\n  if (length(vec) == 1) {\n    return(rep(1, length(x)))\n  }\n\n  # Produce sorted vector and use `findInterval` on it\n  vec_order &lt;- order(vec)\n  vec_sorted &lt;- vec[vec_order]\n  x_ind &lt;- findInterval(x, vec_sorted, all.inside = TRUE)\n\n  # Modify interval index by possibly adding 1 if right interval end is nearer.\n  x_nearest &lt;- x_ind + (vec_sorted[x_ind + 1] - x &lt; x - vec_sorted[x_ind])\n\n  vec_order[x_nearest]\n}\n\n# Completely match x to y\n# It computes:\n  # - Index of the nearest `y` value to `x` value for every index of `x`.\n  # - Similar data, but index of nearest `x` value to `y` values that were not\n    # chosen as nearest ones in the previous step.\n# Output is a tibble with 'from' and 'to' columns. In column 'from' all indices\n  # of `x` are present and in 'to' - all indices of `y` are present.\nmatch_nearest &lt;- function(x, y) {\n  matching_x &lt;- find_nearest_ind(x, y)\n  rest_y &lt;- setdiff(seq_along(y), matching_x)\n\n  if (length(rest_y) &gt; 0) {\n    matching_rest_y &lt;- find_nearest_ind(y[rest_y], x)\n\n    tibble(\n      from = c(seq_along(x), matching_rest_y),\n      to   = c(matching_x,   rest_y)\n    )\n  } else {\n    tibble(from = seq_along(x), to = matching_x)\n  }\n}\n\n# Return modified second argument with zero height segments in case first\n  # argument is `NULL`.\n# This function is essential for aesthetically pleasing animation in case of\n  # different distribution sets. Should be used in conjunction with\n  # `dplyr::full_join()`.\ncoalesce_segments &lt;- function(subject, reference) {\n  if (is.null(subject)) {\n    reference %&gt;% mutate(yend = y)\n  } else {\n    subject\n  }\n}\n\n# Prepare data for `tweenr::tween_states()`\n# `plot_data_from` and `plot_data_to` represent `get_plot_data()` outputs of\n  # two tables which plots should be animated one into another.\n# The idea is to ensure that every segment in both \"from\" and \"to\" data\n  # actively takes part in transition and not just pops up in the end.\n  # This is achieved by doing complete match within every distribution of\n  # segments in \"from\" and \"to\" plot data sets.\ntransition_nearest_value &lt;- function(plot_data_from, plot_data_to) {\n  nested_from &lt;- plot_data_from %&gt;% nest(-distr, .key = \"dataFrom\")\n  nested_to &lt;- plot_data_to %&gt;% nest(-distr, .key = \"dataTo\")\n\n  nested_transitions &lt;- nested_from %&gt;%\n    # Join two tables with plot data by distribution.\n    # Note the use of `full_join()` which enables animating data with different\n      # sets of distributions.\n    full_join(y = nested_to, by = \"distr\") %&gt;%\n    # Modify possibly missing distributions to appear \"from the ground\".\n    mutate(\n      dataFrom = map2(dataFrom, dataTo, coalesce_segments),\n      dataTo = map2(dataTo, dataFrom, coalesce_segments)\n    ) %&gt;%\n    # Compute matching by nearest x value\n    mutate(\n      matching = map2(dataFrom, dataTo, ~ match_nearest(.x[[\"x\"]], .y[[\"x\"]])),\n      transitionFrom = map2(dataFrom, matching, ~ .x[.y[[\"from\"]], ]),\n      transitionTo = map2(dataTo, matching, ~ .x[.y[[\"to\"]], ])\n    )\n\n  # Return a list with pair of tibbles where corresponding pairs of rows\n    # represent segements to be transfromed one into another.\n  list(\n    from = nested_transitions %&gt;% unnest(transitionFrom),\n    to = nested_transitions %&gt;% unnest(transitionTo)\n  )\n}\n\n\n\nThe tweenr function for producing transition between states is tween_states(). Its core input is a list of data frames where all elements must contain the same number of rows. Every row describes parameters of the certain plotting object which will be transformed between present states.\nAs our states consist from essentially different objects, we will operate on pairs of consecutive states. It is our duty to match pairs of objects that will be transformed one into another. Basically, it means to say for every segment into/from which other segment it will transformed. Outline of proposed transition algorithm of plot_data_from into plot_data_to is as follows:\n\nMatch distribution types from both data sets. This is done with full_join() from dplyr in combination with nest() from tidyr. Using this type of join function is essential: it enables creating transition for pair of plot data with different sets of distributions (which is an overkill but a rather cool feature gained with little effort). If in pair plot data there is no matching distribution, the result will be NULL.\nDo matching between same distributions:\n\nIf any plot data is NULL (using full_join() should ensure that there is maximum one NULL) copy pair plotting data and modify column yend to equal y. This models “raising” segments “from the ground” of respective distribution.\nIf both plot data are not NULL match every “from-row” with the “to-row” by nearest x coordinate (represents sample value). To ensure that no segment “pops out of nowhere”, do similar process in reverse: for every “to-row” that wasn’t matched in previous step.\n\n\nTips and tricks:\n\nIndex of the nearest reference point can be found by a slight modification of findInterval(). After computing x_ind with findInterval(x, vec, all.inside = TRUE) (here vec should be sorted increasingly), modify the result with x_ind + (vec_sorted[x_ind + 1] - x &lt; x - vec_sorted[x_ind]). This adds 1 (TRUE converted to numeric value) in case right end of interval is closer than left one.\nUsing full_join() can be very helpful in situation with different sets of groups. It enables later repairing of unmatched data."
  },
  {
    "objectID": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#creating-animations",
    "href": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#creating-animations",
    "title": "Animating mode variability with tidyverse and tweenr",
    "section": "Creating animations",
    "text": "Creating animations\n\n\n\n\n\n\nCode for creating animation\n\n\n\n\n\n# Tween consecutive plot data sets to be transformed one into another\n# The output will be data for cycled animation: last plot will be transformed\n  # into the first one.\ntween_cycled &lt;- function(plot_data_list, tweenlength = 2, statelength = 2,\n                         ease = \"linear\", nframes = 24) {\n  states &lt;- c(plot_data_list, plot_data_list[1])\n\n  # As in result of every `tweenr::tween_states()` call column `.frame` starts\n    # from 1, it is needed to offset frames for every pair.\n  frame_offset &lt;- (seq_along(plot_data_list) - 1) * nframes\n\n  map2(states[-length(states)], states[-1], transition_nearest_value) %&gt;%\n    map2_dfr(\n      frame_offset,\n      function(pair_tbls, f_offset) {\n        pair_mod &lt;- pair_tbls %&gt;%\n          # Converting column `distr` to factor is needed to avoid\n            # 'Error in col2rgb(d)' with not correct colour name.\n          # This is due to not optimal treatment of character columns in current\n            # CRAN version of `tweenr`.\n            # This seems to be solved in dev. version at 'thomasp85/tweenr'.\n            # However, for now it is rather unstable.\n          # Fortunately, converting to factor should give expected results as in\n            # `transition_nearest_value()` it is ensured common value set and\n            # order of their appearence by using `full_join()`.\n          map(. %&gt;% mutate(distr = as.factor(distr)))\n\n        # CRAN version of `tweenr` also changes levels of factors during\n          # `tween_states()`. This also should be solved in development version.\n        # For now, use workaround with manual patching of levels.\n        distr_levels &lt;- union(\n          levels(pair_mod[[1]]$distr), levels(pair_mod[[2]]$distr)\n        )\n\n        pair_mod %&gt;%\n          tweenr::tween_states(\n            tweenlength = tweenlength, statelength = statelength,\n            ease = ease, nframes = nframes\n          ) %&gt;%\n          mutate(\n            # Offset frames\n            .frame = .frame + f_offset,\n            # Repare columns after applying `tween_states()`\n            distr = factor(as.character(distr), levels = distr_levels),\n            isMode = as.logical(isMode),\n            colour = as.character(colour)\n          )\n      }\n    )\n}\n\n# Wrapper for constructing animation\nmake_animation &lt;- function(plot_data_list, title = NULL,\n                           tweenlength = 2, statelength = 2,\n                           ease = \"linear\", nframes = 24,\n                           filename = NULL) {\n  p &lt;- plot_data_list %&gt;%\n    tween_cycled(tweenlength = tweenlength, statelength = statelength,\n                 ease = ease, nframes = nframes) %&gt;%\n    # Construct plot\n    ggplot(aes(frame = .frame)) +\n      add_layers() +\n      labs(title = title)\n\n  gganimate(\n    p, filename = filename, title_frame = FALSE,\n    # Change resolution by supplying function for graphic device.\n    # Thanks to https://stackoverflow.com/a/46878163/7360839\n    ani.dev = function(...) {png(res = 100, ...)},\n    ani.width = 675, ani.height = 450\n  )\n}\n\n\n\nFunction make_animation(), besides arguments for tweenr and gganimate functions, expects a list of plotting data and a title.\nTips and tricks:\n\nOne can change resolution (for example, to 100 ppi) of output animation in gganimate by supplying ani.dev = function(...) {png(res = 100, ...)}. This trick is thanks to this StackOverflow answer."
  },
  {
    "objectID": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#example-animations",
    "href": "blog/2018-06-14-animating-mode-variability-with-tidyverse-and-tweenr.html#example-animations",
    "title": "Animating mode variability with tidyverse and tweenr",
    "section": "Example animations",
    "text": "Example animations\nWith the following animations we see that sample mode is very variable for uniform distribution (which is expected), rather variable for normal and very stable for beta distribution with shape parameters 5 and 1.\nCreated code enables animating samples with different distribution sets. It is also flexible enough to present evolution of plots for different roundings and definitions of mode.\n# Mode variability of different samples with values rounded to 2 digits\nsample_data %&gt;%\n  map(. %&gt;% mutate(value = round(value, 2))) %&gt;%\n  map(get_plot_data) %&gt;%\n  make_animation(title = \"Mode variability for independent samples\")\n\n# Animation for tables with different distribution sets\nsample_data[1:3] %&gt;%\n  # Convert `distr` to factor with common levels to ensure animation stability\n  map(~ mutate(., distr = factor(distr, levels = sort(names(r_funs))))) %&gt;%\n  # Remove distributions in tables\n  map2(names(r_funs), ~ filter(.x, as.character(distr) != .y)) %&gt;%\n  # The same animation process\n  map(. %&gt;% mutate(value = round(value, 2))) %&gt;%\n  map(get_plot_data) %&gt;%\n  make_animation(\n    title = \"Automated animation for different distribution sets\"\n  )\n\n# Evolution of modes for different mode thresholds\nmap(\n  seq(from = 1, to = 0, by = -0.05),\n  ~ get_plot_data(sample_data[[1]] %&gt;% mutate(value = round(value, 2)), .x)\n) %&gt;%\n  make_animation(\n    title = \"Evolution of modes for different mode thresholds (from 1 to 0)\",\n    nframes = 6\n  )\n\n# Mode variability of one sample with different roundings\nmap2(sample_data[1], 3:1, ~ mutate(.x, value = round(value, .y))) %&gt;%\n  map(get_plot_data) %&gt;%\n  make_animation(\n    title = \"Mode variability for different roundings of one sample\",\n    tweenlength = 3, statelength = 1\n  )"
  },
  {
    "objectID": "blog/2025-07-04-neovim-now-has-builtin-plugin-manager.html",
    "href": "blog/2025-07-04-neovim-now-has-builtin-plugin-manager.html",
    "title": "Neovim now has built-in plugin manager",
    "section": "",
    "text": "Originally posted on Reddit\nHello, Neovim users!\nAfter rounds of discussions and iterations, Neovim now has a minimal built-in plugin manager. Its functionality is present only on latest master branch and located in vim.pack module. It is planned to land in 0.12 (which is still not quite soon).\nYou can read more about the design and workflow here. You can also see the demo of the latest state in the PR’s first comment.\nThe overall design is based on ‘mini.deps’ module, but it is visibly reworked to fix some of its shortcomings and to fit “minimal built-in plugin manager” requirements.\nCouple of very important notes:\n\nI can not stress this enough, this is yet a WORK IN PROGRESS. There are many planned improvements. Expect breaking changes without notice. Only use it if you are comfortable fixing suddenly not working config.\nEarly testing of existing features is appreciated. Suggestions about new features will be met very conservatively (at least until there is an ongoing planned work).\nIt is not meant to provide all advanced plugin manager features. If you prefer to use a more capable plugin manager, it is totally normal. Probably even possible to use side-by-side with vim.pack.\n\nThere is still a long road ahead and we’ll walk it one step at a time. Thanks for reading and using Neovim!"
  },
  {
    "objectID": "blog/2025-06-20-neovim-minipick-nushell-cli-fuzzy-picker.html",
    "href": "blog/2025-06-20-neovim-minipick-nushell-cli-fuzzy-picker.html",
    "title": "Neovim + mini.pick + nushell = CLI fuzzy picker. Why? Because why not.",
    "section": "",
    "text": "Originally posted on Reddit\n\nHello, Neovim users!\nFor quite some time I was interested in trying out Nushell as my default shell. To be perfectly honest, I am not sure why. Probably because I am drawn to the idea of “piping structured data” and mastering a powerful tool for the future. Or maybe it is just pretty tables, who knows.\nSeveral weeks ago I decided to give it a try but only in Ghostty (terminal emulator I use for regular activity; as opposed to backup st with Zsh). It is pretty interesting to set up from ground up and use.\nSwitching from Zsh to Nushell very much reminds me of switching from Vim to Neovim just after the latter got first-class Lua support. Nu (language of Nushell) is a saner language than Bash to hack the config and add custom features (very much like Lua is to Vimscript). But it is not quite stable yet, so expecting something to break after new release is not baseless.\n\nAnyway, while writing my prompt from scratch (as one does) I also thought that it would be an interesting challenge to try to go without fzf in CLI and try to use fuzzy picking I have set up in Neovim with ‘mini.pick’. It turned out to be not as complicated as I feared at the beginning. The only downside is that Neovim always occupies full terminal window, so it is impossible to have small-ish picker as fzf.\nI believe the overall approach can be generalized to other shells and Neovim’s fuzzy pickers, so decided to share it here. Basically:\n\nThe general idea is to manually call Neovim with custom config (it can be regular config, but separate one feels cleaner to me) to fuzzy pick things. Choosing item(s) should write them into a special file . After that, shell reads the file and performs necessary actions.\nSo, to fuzzy pick something like files/subdirectories and insert item at cursor:\n\nWrite a global function in ‘init.lua’ that starts fuzzy picker for files (like using MiniPick.builtin.files()) or subdirectories (custom picker). Choosing item(s) should execute custom action and write to a dedicated file (like ‘/tmp/nvim/out-file’).\nWrite custom shell command/function that calls Neovim with a dedicated ‘init.lua’ and executes the necessary global Lua function (like with -c \"lua _G.pick_file_cli()\"). After calling nvim, the shell command/function should read the ‘/tmp/nvim/out-file’ file, delete it (to not reuse later), and insert its content at cursor.\nMap dedicated keys in shell to that command/function. Currently I have &lt;C-d&gt; for subdirectories and &lt;C-t&gt; for files.\n\nTo fuzzy pick from piped input, create a shell command/function that:\n\nWrites piped input to a dedicated file (like ‘/tmp/nvim/in-file’).\nCalls Neovim’s global function that reads from that file, fuzzy picks from items, writes chosen one(s) to ‘/tmp/nvim/out-file’.\nReads from ‘/tmp/nvim/out-file’ and returns its content.\n\n\nMy dedicated Neovim config for this is here (it assumes ‘mini.nvim’ is already installed as suggested in ’pack/*/start’ directory). The Nushell part of the approach is here.\nThe approach is not perfect and I’d recommend to daily drive it only if you understand how it works. But maybe the whole approach would interesting to someone.\nThanks for reading!"
  },
  {
    "objectID": "blog/2018-08-21-transformers-glue.html",
    "href": "blog/2018-08-21-transformers-glue.html",
    "title": "Transformers, glue!",
    "section": "",
    "text": "Package {glue} is designed as “small, fast, dependency free” tools to “glue strings to data in R”. To put simply, it provides concise and flexible alternatives for paste() with some additional features:\nlibrary(glue)\n\nx &lt;- 10\npaste(\"I have\", x, \"apples.\")\n## [1] \"I have 10 apples.\"\nglue(\"I have {x} apples.\")\n## I have 10 apples.\nRecently, fate lead me to try using {glue} in a package. I was very pleased to how it makes code more readable, which I believe is a very important during package development. However, I stumbled upon this pretty unexpected behavior:\ny &lt;- NULL\npaste(\"I have\", x, \"apples and\", y, \"oranges.\")\n## [1] \"I have 10 apples and  oranges.\"\nstr(glue(\"I have {x} apples and {y} oranges.\"))\n## Classes 'glue', 'character'  chr(0)\nIf one of the expressions is evaluated into NULL then the output becomes empty string. This was unintuitive result and for a while I thought about stop using {glue} because NULL is expected to be a valid input. However, if Jim Hester is package author, you should expect some sort of designed solution to any problem. This time wasn’t an exception: there is a transformers functionality.\nBasically, transformer is a function that changes the output of R expressions the way you want. As I wanted to make NULL visible, this is a perfect way to do it."
  },
  {
    "objectID": "blog/2018-08-21-transformers-glue.html#show-null",
    "href": "blog/2018-08-21-transformers-glue.html#show-null",
    "title": "Transformers, glue!",
    "section": "Show NULL",
    "text": "Show NULL\nBack to initial problem. We want NULL to be a valid R value for a glue():\nshow_null &lt;- function(x, val = \"NULL\") {\n  if (is.null(x)) {\n    val\n  } else {\n    x\n  }\n}\n\nglue_null &lt;- transforming_glue(show_null)\n\n# Example from Prologue\nglue_null(\"I have {x} apples and {y} oranges.\")\n## I have 10 apples and NULL oranges."
  },
  {
    "objectID": "blog/2018-08-21-transformers-glue.html#fixed-width-output",
    "href": "blog/2018-08-21-transformers-glue.html#fixed-width-output",
    "title": "Transformers, glue!",
    "section": "Fixed width output",
    "text": "Fixed width output\nWith {stringr} package you can force an output to be fixed width:\nstr_width &lt;- function(x, width) {\n  if (str_length(x) &gt; width) {\n    str_trunc(x, width, side = \"right\")\n  } else {\n    str_pad(x, width, side = \"right\")\n  }\n}\n\nglue_width &lt;- transforming_glue(partial(str_width, width = 10))\n\nshort_oh &lt;- \"Ooh!\"\nlong_oh &lt;- \"Oooooooooooh!\"\nglue_width(\"This puzzles ({short_oh}) and surprises ({long_oh}) me.\")\n## This puzzles (Ooh!      ) and surprises (Ooooooo...) me.\nNote usage of partial() here: it takes function along with its arguments’ values and modifies it by “pre-filling” those arguments."
  },
  {
    "objectID": "blog/2018-08-21-transformers-glue.html#enclose-output",
    "href": "blog/2018-08-21-transformers-glue.html#enclose-output",
    "title": "Transformers, glue!",
    "section": "Enclose output",
    "text": "Enclose output\nIn some situation you might want to explicitly show which strings represent R objects in the output. You can do that by enclosing the output in some sort of braces:\nenclose &lt;- function(x, start = \"&lt;\", end = \"&gt;\") {\n  paste0(start, x, end)\n}\n\nglue_enclose &lt;- transforming_glue(enclose)\n\nglue_enclose(\"What if I had {x} oranges?\")\n## What if I had &lt;10&gt; oranges?"
  },
  {
    "objectID": "blog/2018-08-21-transformers-glue.html#bizarro-encryption",
    "href": "blog/2018-08-21-transformers-glue.html#bizarro-encryption",
    "title": "Transformers, glue!",
    "section": "Bizarro encryption",
    "text": "Bizarro encryption\nOne possibly useful pattern is to encrypt the used data to prevent it from seeing by untrustworthy eyes. Here we will use simplified bizarro() example from this insightful UseR 2018 talk by the amazing Jennifer (Jenny) Bryan. Here glue_bizarro() “reverts” R objects based on their type.\nstr_reverse &lt;- function(x) {\n  vapply(\n    strsplit(x, \"\"),\n    FUN = function(z) paste(rev(z), collapse = \"\"),\n    FUN.VALUE = \"\"\n  )\n}\n\nbizarro &lt;- function(x) {\n  cls &lt;- class(x)[[1]]\n\n  switch(\n    cls,\n    logical = !x,\n    integer = -x,\n    numeric = -x,\n    character = str_reverse(x),\n    x\n  )\n}\n\nglue_bizarro &lt;- transforming_glue(bizarro)\n\nnew_fruit &lt;- \"pomegranate\"\nglue_bizarro(\n  \"Then I might have {x + 10} apples. Is that {TRUE}?\n   Maybe I want {new_fruit}?\"\n)\n## Then I might have -20 apples. Is that FALSE?\n## Maybe I want etanargemop?"
  },
  {
    "objectID": "blog/2018-08-21-transformers-glue.html#ultimate-example",
    "href": "blog/2018-08-21-transformers-glue.html#ultimate-example",
    "title": "Transformers, glue!",
    "section": "Ultimate example",
    "text": "Ultimate example\nUsing already familiar functional programming technique, we can create an ultimate glue() wrapper as a combination, or rather compose()-ition, of all previous examples. The most important part is supply them in correct order:\nglue_ultimate &lt;- transforming_glue(\n  compose(\n    enclose,\n    partial(str_width, width = 10),\n    # To ensure that input of `str_width()` is character\n    as.character,\n    show_null,\n    bizarro\n  )\n)\n\nglue_ultimate(\n  \"I have {x} apples and {y} oranges.\n   This puzzles ({short_oh}) and surprises ({long_oh}) me.\n   What if I had {x} oranges?\n   Then I might have {x + 10} apples. Is that {TRUE}?\n   Maybe I want {new_fruit}?\"\n)\n## I have &lt;-10       &gt; apples and &lt;NULL      &gt; oranges.\n## This puzzles (&lt;!hoO      &gt;) and surprises (&lt;!hooooo...&gt;) me.\n## What if I had &lt;-10       &gt; oranges?\n## Then I might have &lt;-20       &gt; apples. Is that &lt;FALSE     &gt;?\n## Maybe I want &lt;etanarg...&gt;?"
  },
  {
    "objectID": "blog/2017-10-20-highlight-the-pipe-highlight-js.html",
    "href": "blog/2017-10-20-highlight-the-pipe-highlight-js.html",
    "title": "Highlight the Pipe. Highlight.js",
    "section": "",
    "text": "Prologue\nWhile creating this site I had to encounter the topic of highlighting code on web pages. I decided to do that with the help of highlight.js functionality. After picking a style with R in mind, I arrived to the following question: is there an easy way to highlight pipe operator %&gt;% separately? As it turned out, the answer is “Yes”, but the journey through unexplored world of JavaScript was bumpy with a pleasant moment of meeting familiar name.\nSo this post is about adding custom rules for code highlighting in highlight.js, taking string %&gt;% as an example.\n\n\nOverview\nThe “Getting Started” part of Usage page says that to start using highlight.js on a web page the following code should be executed:\n&lt;link rel=\"stylesheet\" href=\"/path/to/styles/default.css\"&gt;\n&lt;script src=\"/path/to/highlight.pack.js\"&gt;&lt;/script&gt;\n&lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;\nThe description is “This will find and highlight code inside of &lt;pre&gt;&lt;code&gt; tags; it tries to detect the language automatically. If automatic detection doesn’t work for you, you can specify the language in the class attribute:”\n&lt;pre&gt;&lt;code class=\"html\"&gt;...&lt;/code&gt;&lt;/pre&gt;\nSo basically the process of highlighting the text inside &lt;pre&gt;&lt;code&gt;...&lt;/code&gt;&lt;/pre&gt; is the following:\n\nDetect language (either automatically or with class attribute inside &lt;pre&gt; or &lt;code&gt; tag).\nApply some complicated parsing with functionality sourced from “/path/to/highlight.pack.js”. This will, based on predefined rules, wrap some parts of text with &lt;span&gt;&lt;/span&gt; tags and appropriate class.\nApply CSS customization based on “/path/to/styles/default.css” file and classes of &lt;span&gt; tags created in the previous step.\n\nTo be more specific with code, this site uses at the time of writing this post (with help of Hugo and Minimal theme) the following code:\n&lt;link rel=\"stylesheet\" href=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/idea.min.css\"&gt;\n\n&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js\"&gt;&lt;/script&gt;\n\n&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/html.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/javascript.min.js\"&gt;&lt;/script&gt;\n\n&lt;script&gt;hljs.initHighlightingOnLoad();&lt;/script&gt;\nThe first block loads CSS code for “Idea” style, the second - JavaScript code for general highlight.js functionality, the third - code for parsing rules for specific languages (YAML and HTML) and the fourth initializes highlight.js. Basically, files yaml.min.js, html.min.js and javascript.min.js contain information about actual rules of code parsing.\n\n\nCustom parsing rules\nThe similar file but for R, with my custom indentation, looks like this:\nhljs.registerLanguage(\"r\",\n  function(e){\n    var r=\"([a-zA-Z]|\\\\.[a-zA-Z.])[a-zA-Z0-9._]*\";\n    return{\n      c:[e.HCM,\n        {b:r,l:r, k:\n          {keyword:\"function if in break next repeat else for return switch while try tryCatch stop warning require library attach detach source setMethod setGeneric setGroupGeneric setClass ...\",\n          literal:\"NULL NA TRUE FALSE T F Inf NaN NA_integer_|10 NA_real_|10 NA_character_|10 NA_complex_|10\"},\n        r:0},\n        {cN:\"number\",b:\"0[xX][0-9a-fA-F]+[Li]?\\\\b\",r:0},\n        {cN:\"number\",b:\"\\\\d+(?:[eE][+\\\\-]?\\\\d*)?L\\\\b\",r:0},\n        {cN:\"number\",b:\"\\\\d+\\\\.(?!\\\\d)(?:i\\\\b)?\",r:0},\n        {cN:\"number\",b:\"\\\\d+(?:\\\\.\\\\d*)?(?:[eE][+\\\\-]?\\\\d*)?i?\\\\b\",r:0},\n        {cN:\"number\",b:\"\\\\.\\\\d+(?:[eE][+\\\\-]?\\\\d*)?i?\\\\b\",r:0},\n        {b:\"`\",e:\"`\",r:0},\n        {cN:\"string\",c:[e.BE],v:[{b:'\"',e:'\"'},{b:\"'\",e:\"'\"}]}\n      ]\n    }\n  }\n);\nAfter first seeing this without indentation, as one string, I was a little bit intimidated. Fortunately, after some internet searching I found highlight.js github repository with very useful src directory. It contains subdirectories languages (for JavaScript rules like mentioned above) and styles (for styles’ CSS code).\nThe file for parsing R code is src/languages/r.js. Its core was written in the spring of 2012 by Joe Cheng, creator of the Shiny framework. Seeing familiar name during rough JavaScript journey somewhat cheered me up. After studying the code, many questions were answered:\n\nBy default the following pieces of code can be manually highlighted: comment, string, number, keyword, literal (TRUE, FALSE, NULL, NA, etc.).\nThose one- and two-letter variables in code are just short versions of more understandable className, begin, end, relevance, etc.\nTo add custom piece of code to highlight one should add appropriate class in the parsing rules. There is a thorough highlight.js documentation if you want to master the logic behind these rules. The most easy and understandable way of creating the rule is specifying regular expressions for beginning and ending of desired class. Note that if ending is omitted then it is just the regex for the class. For example:\n\n{className: \"pipe\", begin: \"%&gt;%\", relevance: 0}\nThis code finds string %&gt;% and wraps it as &lt;span class=\"hljs-pipe\"&gt;%&gt;%&lt;/span&gt; (note prefix “hljs-”). About relevance argument one can read here, as it is not very important for the current topic.\nWith this knowledge one can create other interesting rules:\n// Function parameters with good style as 'variable' + 'space' + '=' + 'space'\n{className: \"fun-param\", begin: \"([a-zA-Z]|\\\\.[a-zA-Z.])[a-zA-Z0-9._]*\\\\s+=\\\\s+\", relevance: 0},\n\n// Assign operator with good style\n{className: \"assign\", begin: \" &lt;- \", relevance: 0},\n\n// Adding to class 'keyword' the explicit use of function's package\n{className: \"keyword\", begin: \"([a-zA-Z]|\\\\.[a-zA-Z.])[a-zA-Z0-9._]*::\", relevance: 0},\n\n// Class for basic dplyr words with their scoped variants\n// Not included in this site highlighting rules\n{className: \"dplyr\", begin: \"tibble|mutate|select|filter|summari[sz]e|arrange|group_by\", end: \"[a-zA-Z0-9._]*\", relevance: 0}\nIt is important to add these rules in the appropriate places, because they are processed sequentially, so order matters. The final version of this site’s rules for R looks like this (click to unfold the spoiler):\n\n\n\n\n\n\ncustom.r.min.js\n\n\n\n\n\nhljs.registerLanguage(\"r\",\n  function(e){\n    var r=\"([a-zA-Z]|\\\\.[a-zA-Z.])[a-zA-Z0-9._]*\";\n    return{\n      c:[e.HCM,\n        {cN:\"fun-param\",b:\"([a-zA-Z]|\\\\.[a-zA-Z.])[a-zA-Z0-9._]*\\\\s+=\\\\s+\",r:0},\n        {cN:\"pipe\",b:\"%&gt;%\",r:0},\n        {cN:\"assign\",b:\" &lt;- \",r:0},\n        {cN:\"keyword\",b:\"([a-zA-Z]|\\\\.[a-zA-Z.])[a-zA-Z0-9._]*::\",r:0},\n  //      {cN:\"dplyr\",b:\"tibble|mutate|select|filter|summari[sz]e|arrange|group_by\",e:\"[a-zA-Z0-9._]*\",r:0},\n        {b:r,l:r, k:\n          {keyword:\"function if in break next repeat else for return switch while try tryCatch stop warning require library attach detach source setMethod setGeneric setGroupGeneric setClass ...\",\n          literal:\"NULL NA TRUE FALSE T F Inf NaN NA_integer_|10 NA_real_|10 NA_character_|10 NA_complex_|10\"},\n        r:0},\n        {cN:\"number\",b:\"0[xX][0-9a-fA-F]+[Li]?\\\\b\",r:0},\n        {cN:\"number\",b:\"\\\\d+(?:[eE][+\\\\-]?\\\\d*)?L\\\\b\",r:0},\n        {cN:\"number\",b:\"\\\\d+\\\\.(?!\\\\d)(?:i\\\\b)?\",r:0},\n        {cN:\"number\",b:\"\\\\d+(?:\\\\.\\\\d*)?(?:[eE][+\\\\-]?\\\\d*)?i?\\\\b\",r:0},\n        {cN:\"number\",b:\"\\\\.\\\\d+(?:[eE][+\\\\-]?\\\\d*)?i?\\\\b\",r:0},\n        {b:\"`\",e:\"`\",r:0},\n        {cN:\"string\",c:[e.BE],v:[{b:'\"',e:'\"'},{b:\"'\",e:\"'\"}]}\n      ]\n    }\n  }\n);\n\n\n\nThis code should be sourced on every page with highlighting. For this to be done with Hugo:\n\nSave this code into file static/js/custom.r.min.js.\nAdd the following line to the head of every web page (usually by modifying partial template for page’s header):\n\n&lt;script src=\"/js/custom.r.min.js\"&gt;&lt;/script&gt;\n\n\nCustom style\nStyling of the parsed code is done with CSS, so some knowledge of it is needed. This is done by properly adding CSS rules to every page with highlighting. For example, this site’s CSS rules for specifically R code highlighting look like this:\n/* Set colour for function parameters */\n.hljs-fun-param {\n    color: #ff4000;\n}\n\n/* Make the pipe and assign operator bold */\n.hljs-pipe, .hljs-assign {\n    font-weight: bold;\n}\nThe result looks like this:\n# R comment with %&gt;% and &lt;- .\niris_summary &lt;- iris %&gt;%\n  dplyr::group_by(Species) %&gt;%\n  dplyr::summarise(meanSepalLength = mean(Sepal.Length))\n\nstarts_with_str &lt;-function(x=c(\" %&gt;% \", \" &lt;- \")) {\n  paste0(\"Starts with\", x)\n}\nNotice the following:\n\nStrings %&gt;% and &lt;- are not specially highlighted inside comment or string.\nUse of dplyr:: is highlighted the same as keyword function.\nStrings = (in function parameters) and &lt;- should be surrounded by spaces (for which styling is also applied) to be correctly highlighted. This encourages tidyverse style guide.\n\n\n\nConclusions\n\nAsking questions about seemingly simple task can lead to the long journey of code exploration.\nMeeting familiar names during times of trouble can be inspiring.\nCreating custom rules for code highlighting with highlight.js is pretty straightforward for R people (after some JavaScript and CSS adjusting).\n\n\n\n\n\n\n\nsessionInfo()\n\n\n\n\n\n## R version 3.4.2 (2017-09-28)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.04.3 LTS\n##\n## Matrix products: default\n## BLAS: /usr/lib/openblas-base/libblas.so.3\n## LAPACK: /usr/lib/libopenblasp-r0.2.18.so\n##\n## locale:\n##  [1] LC_CTYPE=ru_UA.UTF-8       LC_NUMERIC=C\n##  [3] LC_TIME=ru_UA.UTF-8        LC_COLLATE=ru_UA.UTF-8\n##  [5] LC_MONETARY=ru_UA.UTF-8    LC_MESSAGES=ru_UA.UTF-8\n##  [7] LC_PAPER=ru_UA.UTF-8       LC_NAME=C\n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C\n## [11] LC_MEASUREMENT=ru_UA.UTF-8 LC_IDENTIFICATION=C\n##\n## attached base packages:\n## [1] methods   stats     graphics  grDevices utils     datasets  base\n##\n## loaded via a namespace (and not attached):\n##  [1] compiler_3.4.2  backports_1.1.1 bookdown_0.5    magrittr_1.5\n##  [5] rprojroot_1.2   tools_3.4.2     htmltools_0.3.6 yaml_2.1.14\n##  [9] Rcpp_0.12.13    stringi_1.1.5   rmarkdown_1.7   blogdown_0.2\n## [13] knitr_1.17      stringr_1.2.0   digest_0.6.12   evaluate_0.10.1"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html",
    "href": "blog/2019-08-01-announcing-pdqr.html",
    "title": "Announcing pdqr",
    "section": "",
    "text": "I have been working on ‘pdqr’ package for quite some time now. Initially it was intended only for creating custom distribution functions (analogues of base “p”, “d”, “q”, and “r” functions) from numeric sample. However, after couple of breakthrough ideas, it also became a set of tools for transforming and summarizing distributions.\nNow I would like to make it public with hope to get any feedback about the package: its general design, function and argument names, numerical issues, etc. Please, feel free to tweet your thoughts (preferably with #pdqr hashtag). If you have any suggestions or found a bug, don’t hesitate to create an issue.\nThis announcement post will be in form of a short cookbook for most common distribution problems, preceding with a little intro to package core structure. For more details go to these resources:\n\nPackage README contains more thorough overview of package structure.\nVignettes represent deeper dive into package’s main families of functions.\n\nWe’ll need the following setup:\n# Currently can only be installed from Github\nlibrary(pdqr)\nlibrary(magrittr)\n\n# Make reproducible randomness\nset.seed(101)"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#estimate-distribution-from-sample",
    "href": "blog/2019-08-01-announcing-pdqr.html#estimate-distribution-from-sample",
    "title": "Announcing pdqr",
    "section": "Estimate distribution from sample",
    "text": "Estimate distribution from sample\nProblem Create a pdqr-function representing an estimation of sample’s distribution.\nSolution The most important decision here is about distribution’s type: discrete or continuous. In first case, any new_*() function will use unique values of sample as distribution’s outcomes, and probabilities will be estimated by tabulating and normalizing:\n# Any pdqr class can be used to represent distribution\nmpg_dis &lt;- new_d(mtcars$mpg, type = \"discrete\")\n\n# Pdqr-functions have `print()` methods with concise summary\nmpg_dis\n## Probability mass function of discrete type\n## Support: [10.4, 33.9] (25 elements)\n\n# To visually inspect pdqr-function, use `plot()`\nplot(mpg_dis)\nFor continuous type, density() function will be used to estimate distribution’s density (if sample has 2 or more elements). Note that arguments of density() will affect greatly the output distribution (as always with kernel density estimation).\nmpg_con &lt;- new_d(mtcars$mpg, type = \"continuous\")\nmpg_con\n## Density function of continuous type\n## Support: ~[2.96996, 41.33004] (511 intervals)\n\nplot(mpg_con)\n\nThese newly created functions can be used to obtain probability or density of any point:\n# Probabilities\nmpg_dis(20:25)\n## [1] 0.0000 0.0625 0.0000 0.0000 0.0000 0.0000\n\n# Density values\nmpg_con(20:25)\n## [1] 0.06469730 0.06040205 0.05394401 0.04591992 0.03759639 0.03029240"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#estimate-distribution-of-positive-values",
    "href": "blog/2019-08-01-announcing-pdqr.html#estimate-distribution-of-positive-values",
    "title": "Announcing pdqr",
    "section": "Estimate distribution of positive values",
    "text": "Estimate distribution of positive values\nProblem Create a pdqr-function representing an estimation of sample’s distribution with assumption that it can produce only positive values.\nSolution This common task is done in two steps:\n\nEstimate sample distribution, as in previous step. It can result into unwanted distribution properties, like producing negative values. This is a known issue of boundary problem in kernel density estimation.\nModify distribution to have only positive support with form_resupport(). It has several methods with default one (\"reflect\") recommended to deal with boundary correction. It “reflects” density tails “inside” of a desired support interval. Its output for bounded distributions is usually a closer estimation of “true distribution”.\n\n# Exponential distribution can have only positive values\nexp_sample &lt;- rexp(100)\nd_exp &lt;- new_d(exp_sample, type = \"continuous\")\n\n  # Use `NA` to indicate that certain edge of support should remain unchanged\nd_exp_corr &lt;- form_resupport(d_exp, support = c(0, NA))\n  # Original kernel density estimation\nplot(\n  d_exp, main = \"Different estimations of exponential distribution\",\n  ylim = c(0, 1)\n)\n  # Corrected density\nlines(d_exp_corr, col = \"blue\")\n  # Reference distribution\nlines(seq(0, 6, by = 0.01), dexp(seq(0, 6, by = 0.01)), col = \"red\")"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#generate-sample-similar-to-input",
    "href": "blog/2019-08-01-announcing-pdqr.html#generate-sample-similar-to-input",
    "title": "Announcing pdqr",
    "section": "Generate sample similar to input",
    "text": "Generate sample similar to input\nProblem Generate sample of certain size from distribution that is an estimate to original sample’s distribution.\nSolution R-functions are used for generating samples. There are two ways of creating them:\n# Create new r-function from sample\nr_mpg_dis &lt;- new_r(mtcars$mpg, type = \"discrete\")\nr_mpg_dis(n = 10)\n##  [1] 15.2 15.2 10.4 16.4 15.5 30.4 18.1 21.5 30.4 15.2\n\n# Convert existing pdqr-function to be r-function\nr_mpg_con &lt;- as_r(mpg_con)\nr_mpg_con(n = 10)\n##  [1] 18.68990 20.91834 20.22863 15.94346 15.12797 35.04418 12.48795\n##  [8] 28.79146 15.24957 24.35035"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-sum-of-random-variables",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-sum-of-random-variables",
    "title": "Announcing pdqr",
    "section": "Compute sum of random variables",
    "text": "Compute sum of random variables\nProblem Compute pdqr-function representing sum of two independent random variables (for example, both being standard normal distributions).\nSolution First, you need to create pdqr-functions for random variables. Here standard normal distribution will be used. This can be done by converting base R distribution functions to be pdqr-functions. After that, using + gives distribution of sum of two random variables:\n(d_norm &lt;- as_d(dnorm))\n## Density function of continuous type\n## Support: ~[-4.75342, 4.75342] (10000 intervals)\n(d_norm_sum &lt;- d_norm + d_norm)\n## Density function of continuous type\n## Support: ~[-5.69484, 6.37105] (511 intervals)\n\nplot(d_norm_sum, main = \"Sum of two standard normal distributions\")\n  # \"True\" sum of two independent normal random variables\nlines(as_d(dnorm, sd = sqrt(2)), col = \"red\")\n\nHere you can see two very important features of ‘pdqr’:\n\nNormal distribution is approximated with piecewise-linear density on finite support (10000 intervals are used by default).\nDistribution of sum of random variables is computed with simulation: large samples are drawn from both distributions, sums of their values are computed (resulting in a sample from sum of random variables), and target distribution is created using one of new_*() functions."
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-mixture",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-mixture",
    "title": "Announcing pdqr",
    "section": "Compute mixture",
    "text": "Compute mixture\nProblem Compute mixture of distributions with different weights.\nSolution Specifically for this task, there is a form_mix() function. It takes a list of pdqr-functions with, possibly, a vector of their weights in mixture:\nd_norm_list &lt;- list(as_d(dnorm), as_d(dnorm, mean = 3, sd = 0.7))\n(d_norm_mix &lt;- form_mix(d_norm_list, weights = c(0.2, 0.8)))\n## Density function of continuous type\n## Support: ~[-4.75342, 6.3274] (20003 intervals)\n\nplot(d_norm_mix)\n\n# As with other pdqr-functions, it can be used for obtaining values\nd_norm_mix(seq(-1, 1, by = 0.2))\n##  [1] 0.04839424 0.05793842 0.06664505 0.07365417 0.07822190 0.07983544\n##  [7] 0.07836164 0.07411459 0.06792242 0.06120445 0.05609038"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-winsorized-sample-mean",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-winsorized-sample-mean",
    "title": "Announcing pdqr",
    "section": "Compute winsorized sample mean",
    "text": "Compute winsorized sample mean\nProblem Compute 10% winsorized sample mean.\nSolution This can be done by purposefully separating task into three steps:\n\nEstimate sample distribution. It can have any type, depending on problem. Type “discrete” is better suited to emulate traditional sample estimates.\nTransform distribution by winsorizing tails. There is a form_tails() functions, which takes pdqr-function and returns pdqr-function for distribution with modified tails (based on supplied method).\nCompute mean of winsorized distribution. Note that in case of other statistic (like variance, skewness, etc.) no bias correction is done: output is a summary of distribution created on previous step(s).\n\nmtcars$mpg %&gt;%\n  new_d(type = \"discrete\") %&gt;%\n  form_tails(level = 0.1, method = \"winsor\", direction = \"both\") %&gt;%\n  summ_mean()\n## [1] 20.19375"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-mode",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-mode",
    "title": "Announcing pdqr",
    "section": "Compute mode",
    "text": "Compute mode\nProblem Compute mode of distribution based on its sample.\nSolution Use summ_mode():\nmtcars$mpg %&gt;%\n  # Using \"continuous\" type will smooth input to get mode of density curve.\n  # With type \"discrete\" the most frequent input value is returned.\n  new_d(type = \"continuous\") %&gt;%\n  summ_mode()\n## [1] 17.90862"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-highest-density-region",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-highest-density-region",
    "title": "Announcing pdqr",
    "section": "Compute highest density region",
    "text": "Compute highest density region\nProblem Compute 90% highest density region (HDR) of a distribution.\nSolution There is a summ_hdr() function for this task. On a one dimensional line, HDR can only be a union of disjoint intervals. So the output of summ_hdr() is a region - data frame with rows representing different intervals of the union and two columns “left” and “right” for left and right ends of intervals.\nnorm_mix &lt;- form_mix(\n  list(as_d(dnorm), as_d(dnorm, mean = 5)),\n  weights = c(0.3, 0.7)\n)\n(norm_mix_hdr &lt;- summ_hdr(norm_mix, level = 0.9))\n##        left    right\n## 1 -1.342624 1.348071\n## 2  3.119697 6.870090\nThere is a region_*() family of functions to work with regions. Here are several examples:\n# Test if number is inside region\nregion_is_in(norm_mix_hdr, x = c(-10, 0, 3, 10))\n## [1] FALSE  TRUE FALSE FALSE\n\n# Compute region width (sum of widths of all disjoint intervals)\nregion_width(norm_mix_hdr)\n## [1] 6.441087\n\n# Draw regions\nplot(norm_mix)\nregion_draw(norm_mix_hdr)"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-separation-threshold",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-separation-threshold",
    "title": "Announcing pdqr",
    "section": "Compute separation threshold",
    "text": "Compute separation threshold\nProblem Compute a numerical threshold that optimally separates two distributions.\nSolution There is a summ_separation() function designed specifically for this type of problem. It has several methods with default one being “KS” (short for “Kolmogorov-Smirnov”): optimal separation is done at point where two CDFs differ the most.\nx &lt;- rnorm(100)\ny &lt;- rnorm(100, mean = 2)\n# Here \"true optimal\" is 1\nsumm_separation(f = new_d(x, \"continuous\"), g = new_d(y, \"continuous\"))\n## [1] 0.9763479"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-probability-of-being-greater",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-probability-of-being-greater",
    "title": "Announcing pdqr",
    "section": "Compute probability of being greater",
    "text": "Compute probability of being greater\nProblem Compute a probability that random draw from one distribution is greater than random draw from another distribution.\nSolution These type of summaries are usually done in two steps:\n\nCompute a boolean pdqr-function that represents a result of comparison between mentioned two random draws. It is a special kind of discrete distribution: it only has values 0 and 1 representing estimated relation being false and true respectively. Usually, they are the output of base R comparison operators: &gt;, &lt;, ==, etc.\nExtract a probability of a relation being true.\n\nd_norm &lt;- as_d(dnorm)\nd_unif &lt;- as_d(dunif)\n\n# The output of `&gt;` is a boolean pdqr-function. For convenience, printing\n# displays also a probability of 1, which is a probability of relation being\n# true. In this case, it should be read as \"probability that random draw from\n# first distribution is greater than a random draw from second one\".\n(norm_geq_unif &lt;- d_norm &gt; d_unif)\n## Probability mass function of discrete type\n## Support: [0, 1] (2 elements, probability of 1: ~0.31563)\n\n# Extract desired probability\nsumm_prob_true(norm_geq_unif)\n## [1] 0.3156265"
  },
  {
    "objectID": "blog/2019-08-01-announcing-pdqr.html#compute-distance-between-distributions",
    "href": "blog/2019-08-01-announcing-pdqr.html#compute-distance-between-distributions",
    "title": "Announcing pdqr",
    "section": "Compute distance between distributions",
    "text": "Compute distance between distributions\nProblem Compute distance between two distributions: positive value indicating “how far” two distributions are from each other. This is usually needed when comparing random outcomes of two (or more) options.\nSolution There is a summ_distance() function with many methods.\n# Default method is \"KS\" (short for \"Kolmogorov-Smirnov\"): the biggest\n# absolute difference between two CDFs\nsumm_distance(d_norm, d_unif, method = \"KS\")\n## [1] 0.5\n\n# There is also \"wass\" method (short for \"Wasserstein\"): average path density\n# point should go while transforming from one into another\nsumm_distance(d_norm, d_unif, method = \"wass\")\n## [1] 0.7006094"
  },
  {
    "objectID": "blog/2017-11-20-store-data-about-rows.html",
    "href": "blog/2017-11-20-store-data-about-rows.html",
    "title": "Store Data About Rows",
    "section": "",
    "text": "During development of my other R package (ruler), I encountered the following problem: how to track rows of data frame after application of some user defined function? It is assumed that this function takes data frame as input, subsets it (with possible creation of new columns, but not rows) and returns the result. The typical example using dplyr and magrittr’s pipe:\nsuppressMessages(library(dplyr))\n\n# Custom `mtcars` for more clear explanation\nmtcars_tbl &lt;- mtcars %&gt;%\n  select(mpg, vs, am) %&gt;%\n  as_tibble()\n\n# A handy way of creating function with one argument\nmodify &lt;- . %&gt;%\n  mutate(vs_am = vs * am) %&gt;%\n  filter(vs_am == 1) %&gt;%\n  arrange(desc(mpg))\n\n# The question is: which rows of `mtcars_tbl` are returned?\nmtcars_tbl %&gt;% modify()\n## # A tibble: 7 x 4\n##     mpg    vs    am vs_am\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  33.9     1     1     1\n## 2  32.4     1     1     1\n## 3  30.4     1     1     1\n## 4  30.4     1     1     1\n## 5  27.3     1     1     1\n## # ... with 2 more rows\nTo solve this problem I ended up creating package keyholder, which became my first CRAN release. You can install its stable version with :\ninstall.packages(\"keyholder\")\nThis post describes basis of design and main use cases of keyholder. For more information see its vignette Introduction to keyholder."
  },
  {
    "objectID": "blog/2017-11-20-store-data-about-rows.html#track-rows",
    "href": "blog/2017-11-20-store-data-about-rows.html#track-rows",
    "title": "Store Data About Rows",
    "section": "Track rows",
    "text": "Track rows\nTo track rows after application of user defined function one can create key with row number as values. keyholder has a wrapper use_id() for this:\n# `use_id()` removes all existing keys and creates key \".id\"\nmtcars_track &lt;- mtcars_tbl %&gt;%\n  use_id()\n\nmtcars_track %&gt;% pull_key(.id)\n##  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n## [24] 24 25 26 27 28 29 30 31 32\nNow rows are tracked:\nmtcars_track %&gt;%\n  modify() %&gt;%\n  pull_key(.id)\n## [1] 20 18 19 28 26  3 32\n\n# Make sure of correct result\nmtcars_tbl %&gt;%\n  mutate(id = seq_len(n())) %&gt;%\n  modify() %&gt;%\n  pull(id)\n## [1] 20 18 19 28 26  3 32\nThe reason for using “key id” instead of “column id” is that modify() hypothetically can perform differently depending on columns of its input. For example, it can use dplyr’s scoped variants of verbs or simply check input’s column structure."
  },
  {
    "objectID": "blog/2017-11-20-store-data-about-rows.html#restore-information",
    "href": "blog/2017-11-20-store-data-about-rows.html#restore-information",
    "title": "Store Data About Rows",
    "section": "Restore information",
    "text": "Restore information\nDuring development of tools for data analysis one can have a need to ensure that certain columns don’t change after application of some function. This can be achieved by keying those columns and restoring them later (note that this can change the order of columns.):\nweird_modify &lt;- . %&gt;% transmute(new_col = vs + 2 * am)\n\n# Suppose there is a need for all columns to stay untouched in the output\nmtcars_tbl %&gt;%\n  key_by(everything()) %&gt;%\n  weird_modify() %&gt;%\n  # This can be replaced by its scoped variant: restore_keys_all()\n  restore_keys(everything()) %&gt;%\n  unkey()\n## # A tibble: 32 x 4\n##   new_col   mpg    vs    am\n##     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1       2  21.0     0     1\n## 2       2  21.0     0     1\n## 3       3  22.8     1     1\n## 4       1  21.4     1     0\n## 5       0  18.7     0     0\n## # ... with 27 more rows"
  },
  {
    "objectID": "blog/2017-11-20-store-data-about-rows.html#hide-columns",
    "href": "blog/2017-11-20-store-data-about-rows.html#hide-columns",
    "title": "Store Data About Rows",
    "section": "Hide columns",
    "text": "Hide columns\nIn actual data analysis the following situation can happen: one should modify all but handful of columns with dplyr::mutate_if().\nis_integerish &lt;- function(x) {all(x == as.integer(x))}\n\nif_modify &lt;- . %&gt;% mutate_if(is_integerish, ~ . * 10)\n\nmtcars_tbl %&gt;% if_modify()\n## # A tibble: 32 x 3\n##     mpg    vs    am\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  21.0     0    10\n## 2  21.0     0    10\n## 3  22.8    10    10\n## 4  21.4    10     0\n## 5  18.7     0     0\n## # ... with 27 more rows\nSuppose column vs should appear unchanged in the output. This can be achieved in several ways, which differ slightly but significantly. The first one is to key by vs, apply function and restore vs from keys.\nmtcars_tbl %&gt;%\n  key_by(vs) %&gt;%\n  if_modify() %&gt;%\n  restore_keys(vs)\n## # A keyed object. Keys: vs\n## # A tibble: 32 x 3\n##     mpg    vs    am\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  21.0     0    10\n## 2  21.0     0    10\n## 3  22.8     1    10\n## 4  21.4     1     0\n## 5  18.7     0     0\n## # ... with 27 more rows\nThe advantage is that it doesn’t change the order of columns. The disadvantage is that it actually applies modification function to column, which can be undesirable in some cases.\nThe second approach is similar, but after keying by vs one can remove this column from data frame. This way column vs is moved to last column.\nmtcars_hidden_vs &lt;- mtcars_tbl %&gt;% key_by(vs, .exclude = TRUE)\n\nmtcars_hidden_vs\n## # A keyed object. Keys: vs\n## # A tibble: 32 x 2\n##     mpg    am\n## * &lt;dbl&gt; &lt;dbl&gt;\n## 1  21.0     1\n## 2  21.0     1\n## 3  22.8     1\n## 4  21.4     0\n## 5  18.7     0\n## # ... with 27 more rows\n\nmtcars_hidden_vs %&gt;%\n  if_modify() %&gt;%\n  restore_keys(vs)\n## # A keyed object. Keys: vs\n## # A tibble: 32 x 3\n##     mpg    am    vs\n##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n## 1  21.0    10     0\n## 2  21.0    10     0\n## 3  22.8    10     1\n## 4  21.4     0     1\n## 5  18.7     0     0\n## # ... with 27 more rows"
  },
  {
    "objectID": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html",
    "href": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html",
    "title": "Harry Potter and rankings with comperank",
    "section": "",
    "text": "Package comperank is on CRAN now. It offers consistent implementations of several ranking and rating methods. Originally, it was intended to be my first CRAN package when I started to build it 13 months ago. Back then I was very curious to learn about different ranking and rating methods that are used in sport. This led me to two conclusions:\n\nThere is an amazing book “Who’s #1” by Langville and Meyer which describes several ideas in great detail.\nAlthough there are some CRAN packages dedicated specifically to ranking methods (for example, elo, mvglmmRank), I didn’t find them to be tidy enough.\n\nThese discoveries motivated me to write my first ever CRAN package. Things didn’t turn out the way I was planning, and now comperank is actually my fourth. After spending some time writing it I realized that most of the package will be about storing and manipulating competition results in consistent ways. That is how comperes was born.\nAfter diverging into creating this site and writing ruler in pair with keyholder, a few months ago I returned to competition results and rankings. Gained experience helped me to improve functional API of both packages which eventually resulted into submitting them to CRAN."
  },
  {
    "objectID": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#massey-method",
    "href": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#massey-method",
    "title": "Harry Potter and rankings with comperank",
    "section": "Massey method",
    "text": "Massey method\nIdea of Massey method is that difference in ratings should be proportional to score difference in direct confrontations. Bigger value indicates better player competition performance.\nhp_cr_massey &lt;- hp_cr_paired %&gt;% rank_massey(keep_rating = TRUE)\nhp_cr_massey\n## # A tibble: 7 x 3\n##   player rating_massey ranking_massey\n##   &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n## 1 HP_1        -0.00870              5\n## 2 HP_2        -0.514                7\n## 3 HP_3         0.293                1\n## 4 HP_4         0.114                3\n## 5 HP_5         0.00195              4\n## 6 HP_6         0.124                2\n## 7 HP_7        -0.00948              6"
  },
  {
    "objectID": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#colley-method",
    "href": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#colley-method",
    "title": "Harry Potter and rankings with comperank",
    "section": "Colley method",
    "text": "Colley method\nIdea of Colley method is that ratings should be proportional to share of player’s won games. Bigger value indicates better player performance.\nhp_cr_colley &lt;- hp_cr_paired %&gt;% rank_colley(keep_rating = TRUE)\nhp_cr_colley\n## # A tibble: 7 x 3\n##   player rating_colley ranking_colley\n##   &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n## 1 HP_1           0.497              5\n## 2 HP_2           0.326              7\n## 3 HP_3           0.599              1\n## 4 HP_4           0.534              3\n## 5 HP_5           0.505              4\n## 6 HP_6           0.542              2\n## 7 HP_7           0.497              6\nBoth Massey and Colley give the same result differing from Exploration ranking in treating “HP_5” (“Order of the Phoenix”) and “HP_7” (“Deathly Hallows”) differently: “HP_5” moved up from 6-th to 4-th place."
  },
  {
    "objectID": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#keener-method",
    "href": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#keener-method",
    "title": "Harry Potter and rankings with comperank",
    "section": "Keener method",
    "text": "Keener method\nKeener method is based on the idea of “relative strength” - the strength of the player relative to the strength of the players he/she has played against. This is computed based on provided Head-to-Head values and some flexible algorithmic adjustments to make method more robust. Bigger value indicates better player performance.\nhp_cr_keener &lt;- hp_cr %&gt;%\n  rank_keener(!!! h2h_funs[\"mean_score_diff\"], keep_rating = TRUE)\nhp_cr_keener\n## # A tibble: 7 x 3\n##   player rating_keener ranking_keener\n##   &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n## 1 HP_1          0.147               5\n## 2 HP_2          0.0816              7\n## 3 HP_3          0.191               1\n## 4 HP_4          0.150               4\n## 5 HP_5          0.153               3\n## 6 HP_6          0.155               2\n## 7 HP_7          0.122               6\nResults for Keener method again raised “HP_5” one step up to third place."
  },
  {
    "objectID": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#markov-method",
    "href": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#markov-method",
    "title": "Harry Potter and rankings with comperank",
    "section": "Markov method",
    "text": "Markov method\nThe main idea of Markov method is that players “vote” for other players’ performance. Voting is done with Head-to-Head values and the more value the more “votes” gives player2 (“column-player”) to player1 (“row-player”). For example, if Head-to-Head value is “number of wins” then player2 “votes” for player1 proportionally to number of times player1 won in a matchup with player2.\nActual “voting” is done in Markov chain fashion: Head-to-Head values are organized in stochastic matrix which vector of stationary probabilities is declared to be output ratings. Bigger value indicates better player performance.\nhp_cr_markov &lt;- hp_cr %&gt;%\n  rank_markov(!!! h2h_funs[\"mean_score_diff\"], keep_rating = TRUE)\nhp_cr_markov\n## # A tibble: 7 x 3\n##   player rating_markov ranking_markov\n##   &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n## 1 HP_1          0.140               5\n## 2 HP_2          0.0500              7\n## 3 HP_3          0.196               1\n## 4 HP_4          0.168               2\n## 5 HP_5          0.135               6\n## 6 HP_6          0.167               3\n## 7 HP_7          0.143               4\nWe can see that Markov method put “HP_4” (“Goblet of Fire”) on second place. This is due to its reasonably good performance against the leader “HP_3” (“Prisoner of Azkaban”): mean score difference is only 0.05 in “HP_3” favour. Doing well against the leader in Markov method has a great impact on output ranking, which somewhat resonates with common sense."
  },
  {
    "objectID": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#offense-defense-method",
    "href": "blog/2018-05-31-harry-potter-and-rankings-with-comperank.html#offense-defense-method",
    "title": "Harry Potter and rankings with comperank",
    "section": "Offense-Defense method",
    "text": "Offense-Defense method\nThe idea of Offense-Defense (OD) method is to account for different abilities of players by combining different ratings:\n\nFor player which can achieve high Head-to-Head value (even against the player with strong defense) it is said that he/she has strong offense which results into high offensive rating.\nFor player which can force their opponents into achieving low Head-to-Head value (even if they have strong offense) it is said that he/she has strong defense which results into low defensive rating.\n\nOffensive and defensive ratings describe different skills of players. In order to fully rate players, OD ratings are computed: offensive ratings divided by defensive. The more OD rating the better player performance.\nhp_cr_od &lt;- hp_cr %&gt;%\n  rank_od(!!! h2h_funs[\"mean_score_diff\"], keep_rating = TRUE)\nprint(hp_cr_od, width = Inf)\n## # A tibble: 7 x 7\n##   player rating_off rating_def rating_od ranking_off ranking_def\n##   &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 HP_1         5.42      1.03      5.29            5           5\n## 2 HP_2         1.45      1.88      0.771           7           7\n## 3 HP_3         7.91      0.522    15.1             1           1\n## 4 HP_4         6.51      0.869     7.49            3           3\n## 5 HP_5         5.30      0.888     5.97            6           4\n## 6 HP_6         6.59      0.809     8.14            2           2\n## 7 HP_7         5.54      1.05      5.29            4           6\n##   ranking_od\n##        &lt;dbl&gt;\n## 1          5\n## 2          7\n## 3          1\n## 4          3\n## 5          4\n## 6          2\n## 7          6\nAll methods give almost equal results again differing only in ranks of “HP_5” and “HP_7”."
  },
  {
    "objectID": "blog/2022-11-22-neovim-builtin-options-survey-needs-your-contribution.html",
    "href": "blog/2022-11-22-neovim-builtin-options-survey-needs-your-contribution.html",
    "title": "Neovim built-in options survey needs your contribution",
    "section": "",
    "text": "Originally posted on Reddit\nHello, Neovim users!\nUpdate from 2022-12-08. Survey is closed. Here is a post with results.\nFor a long time I have been curious about how other people use built-in Neovim settings. For example, I am almost sure that most of you set termguicolors to leverage modern color schemes, but I have nothing to back it up.\nSo I decided to make a survey and it needs your contribution! Here is the link to Google form. It lists all steps needed to take part which should take at most 5 minutes of your time. Basically:\n\nYou’d need to download and execute my custom script (with this source code) inside your day-to-day Neovim config (needs at least Neovim 0.7.0). It will produce a formatted scratch buffer with all non-default values of built-in options.\nReview content for sensible information (paths, etc.).\nCopy all lines from created buffer and paste them into a single survey question.\nClick “Submit”. That’s it!\n\nIt doesn’t require logging into your Google account and won’t track your email address. However, please, submit your results only once. I solely rely on your honesty here.\nOther possible interesting questions this survey will help answer:\n\nWhat Leader key is used the most?\nTabs or spaces?\nAbsolute, relative, or no line numbers?\nTraditional or global statusline?\nPermanent tabline or not?\nUse persistent undo or not?\nshowmode or noshowmode?\nwrap or nowrap?\nAnd maybe more…\n\nWhat I plan to do with results: - The summary of results will be released in some way, shape, or form after survey is closed (at least two weeks from now when there is a 24 hours without new entries). It will be announced in this sub. - Possibly use the most commonly set non-default settings to power a Neovim variant, crowd-sourced version of tpope/vim-sensible.\nPlease, spread a word about this survey to make it more objective. I will consider this survey as passable if at least 100 people will take part. Thanks!"
  },
  {
    "objectID": "blog/2020-05-11-count-bounces-in-table-tennis-world-record.html",
    "href": "blog/2020-05-11-count-bounces-in-table-tennis-world-record.html",
    "title": "Count bounces in table tennis world record",
    "section": "",
    "text": "Prologue\nDan Ives is no stranger to participating in “prolonged” table tennis activities and capturing it on camera. He once said “table tennis” 100,000 times, which took him about 15 hours. With his father Peter he also set a world record for the longest table tennis rally which lasted for 8 hours, 40 minutes, and 10 seconds (8h40m10s as a shorter description of time period).\nOn May 7th 2020 Dan made a successful attempt to beat a world record for the longest duration to control a table tennis ball with a bat. He surpassed current record duration of 5h2m37s by 18 minutes and 27 seconds for a total of 5h21m4s. He also live streamed the event on his “TableTennisDaily” YouTube channel, which later was uploaded (important note for the future: this video is a result of live stream and not a “shot and uploaded” one). During cheering for Dan in real time I got curious about actual number of bounces he made.\nAnd thus the quest begins.\nAs counting manually is error-prone and extremely boring, I decided to do this programmatically. The idea of solution is straightforward: somehow extract audio from the world record video, detect bounces (as they have distinctive sound) and count them.\nThis post consists from two parts (if you just want to know a total count, skip right to Epilogue):\n\nDetecting section describes technical details about how I approached the task of detecting bounces.\nCounting section describes difficulties I had to overcome to produce a (reasonably accurate) count of bounces.\n\nFor more technical details you can look at this project’s Git repository.\n\n\nDetecting\nDetecting was done in two steps: get audio of world record and detect bounces.\nI used the following tools on Ubuntu 18.04 (in order of their usage; Python packages probably best to be installed in a separate environment):\n\nyou-get to download video with the lowest video quality.\nffmpeg to extract audio from downloaded video and split it into 1 hour chunks (except the last one). Splitting was done because of insufficient amount of RAM I have in order to analyze the whole audio file. Note that having these splits means that some bounces on the joints between consecutive audio-chunks won’t be detected.\nlibrosa to detect beats that were in the period from 00:01:14 to 05:20:15 (times of first and last bounces). Timestamps of those beats were considered to be timestamps of bounces.\n\n\n\nCounting\nSo the total number of detected bounces is 49923 with an average tempo of ~156.5 bounces per minute. Ant note that this doesn’t include possibly missing bounces due to splitting audio in 1 hour chunks, which introduced 5 joints between them.\nHowever, YouTube footage is not a “preshot and uploaded” one, but is a direct output of live stream. This resulted into some missing footage. Total time of record based on video footage is 5h19m1s (from 00:01:14 to 05:20:15 video timestamps). On the other hand, tablet, responsible for time tracking, shows total time of 5h21m4s (from 00:00:03 to 05:21:07 at corresponding video timestamps). So there is missing 2m3s of actual bouncing. They were results of video jumps due to, most probably, internet connection issues (I encourage everyone to believe in Dan’s honesty):\n\nFrom 02:32:24 to 02:32:25 in footage time there is a jump in “tablet time” from 02:31:13 to 02:31:24. This is a gap of 10 seconds.\nFrom 02:32:41 to 02:32:42 - tablet jumps from 02:31:41 to 02:32:12. Gap of 30 seconds.\nFrom 02:49:17 to 02:49:18 - tablet jumps from 02:48:48 to 02:48:59. Gap of 10 seconds.\nFrom 02:49:29 to 02:49:30 - tablet jumps from 02:49:10 to 02:49:41. Gap of 30 seconds.\nFrom 02:55:29 to 02:55:30 - tablet jumps from 02:55:41 to 02:55:52. Gap of 10 seconds.\nFrom 02:55:37 to 02:55:38 - tablet jumps from 02:55:59 to 02:56:30. Gap of 30 seconds.\nThe rest 3 seconds seems to be the result of my rounding and possibly some very small jumps.\n\nClose video timestamps and systematic length of jumps are another indicators of internet connection issues.\nKnowing that there is 2m3s of footage missing and that average tempo was ~156.5 bounces per minute, we can add 321 bounces to detected ones.\nFinally, the total number of bounces in Dan Ives world record can be estimated as 50244 bounces (error should be less than 100 bounces for sure).\nAnd thus the quest ends.\n\n\nEpilogue\n\nTools such as ‘you-get’, ‘ffmpeg’, and ‘librosa’ (and Python language in general) make a task as abstract as “count number of table tennis ball bounces in 5 and a half hour long YouTube video” reasonably easy.\nDuring his 5 hours, 21 minutes, and 4 seconds of world record, Dan Ives made around 50244 bounces (with error less than 100 bounces in either side).\n\n\n\n\n\n\n\nsessionInfo()\n\n\n\n\n\n## R version 4.0.0 (2020-04-24)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 18.04.4 LTS\n##\n## Matrix products: default\n## BLAS:   /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3\n## LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so\n##\n## locale:\n##  [1] LC_CTYPE=ru_UA.UTF-8       LC_NUMERIC=C\n##  [3] LC_TIME=ru_UA.UTF-8        LC_COLLATE=ru_UA.UTF-8\n##  [5] LC_MONETARY=ru_UA.UTF-8    LC_MESSAGES=ru_UA.UTF-8\n##  [7] LC_PAPER=ru_UA.UTF-8       LC_NAME=C\n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C\n## [11] LC_MEASUREMENT=ru_UA.UTF-8 LC_IDENTIFICATION=C\n##\n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base\n##\n## loaded via a namespace (and not attached):\n##  [1] compiler_4.0.0  magrittr_1.5    bookdown_0.18   tools_4.0.0\n##  [5] htmltools_0.4.0 yaml_2.2.1      Rcpp_1.0.4.6    stringi_1.4.6\n##  [9] rmarkdown_2.1   blogdown_0.18   knitr_1.28      stringr_1.4.0\n## [13] digest_0.6.25   xfun_0.13       rlang_0.4.6     evaluate_0.14"
  },
  {
    "objectID": "blog/2017-10-29-highlight-the-pipe-pkgdown.html",
    "href": "blog/2017-10-29-highlight-the-pipe-pkgdown.html",
    "title": "Highlight the Pipe. Pkgdown",
    "section": "",
    "text": "It felt really nice to achieve custom code highlighting on this site with highlight.js (see this post). After that, I found myself working with pkgdown, one of many great Hadley’s packages. It is “designed to make it quick and easy to build a website for your package”. It converts all package documentation into appropriate HTML pages. Naturally, I had the same question as before: is there an easy way to highlight pipe operator %&gt;% separately? This time the best answer I was able to come up with was “Yes, if you don’t mind some hacking.”\nThis post is about adding custom rules for code highlighting for pkgdown site, taking string %&gt;% as an example."
  },
  {
    "objectID": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-class",
    "href": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-class",
    "title": "Highlight the Pipe. Pkgdown",
    "section": "Add class",
    "text": "Add class\nThe following functions do the job of adding class to appropriate tags. Package xml2 should be installed.\nMain function arguments are:\n\nxpath - String containing an xpath (1.0) expression (use \"//pre//span\" for code highlighting tags).\npattern - Regular expression for tags’ text of interest.\nnew_class - String for class to add.\npath - Path to folder with html files (default to “docs”).\n\nxml_add_class_pattern &lt;- function(xpath, pattern, new_class, path = \"docs\") {\n  # Find HTML pages\n  html_files &lt;- list.files(\n    path = \"docs\",\n    pattern = \"\\\\.html\",\n    recursive = TRUE,\n    full.names = TRUE\n  )\n\n  lapply(html_files, function(file) {\n    page &lt;- xml2::read_html(file, encoding = \"UTF-8\")\n\n    matched_nodes &lt;- xml_find_all_patterns(page, xpath, pattern)\n    if (length(matched_nodes) == 0) {\n      return(NA)\n    }\n\n    xml_add_class(matched_nodes, new_class)\n\n    xml2::write_html(page, file, format = FALSE)\n  })\n\n  invisible(html_files)\n}\n\n# Add class `new_class` to nodes\nxml_add_class &lt;- function(x, new_class) {\n  output_class &lt;- paste(xml2::xml_attr(x, \"class\"), new_class)\n  mapply(xml2::xml_set_attr, x, output_class, MoreArgs = list(attr = \"class\"))\n\n  invisible(x)\n}\n\n# Find appropriate tags\n# To find &lt;span&gt; inside &lt;pre&gt; use `xpath = \"\\\\pre\\\\span\"`.\nxml_find_all_patterns &lt;- function(x, xpath, pattern, ns = xml2::xml_ns(x)) {\n  res &lt;- xml2::xml_find_all(x, xpath, ns)\n  is_matched &lt;- grepl(pattern, xml2::xml_text(res))\n\n  res[is_matched]\n}\nFor convenience one can define function high_pipe() for adding class pp to all &lt;span&gt; inside &lt;pre&gt; with text containing %&gt;%:\nhigh_pipe &lt;- function(path = \"docs\", new_class = \"pp\") {\n  xml_add_class_pattern(\"//pre//span\", \"%&gt;%\", new_class, path)\n}\nSo typical usage is as follows:\n\nRun pkgdown::build_site().\nRun highdown::high_pipe() (with working directory being package root)."
  },
  {
    "objectID": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-custom-css-rules",
    "href": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-custom-css-rules",
    "title": "Highlight the Pipe. Pkgdown",
    "section": "Add custom CSS rules",
    "text": "Add custom CSS rules\nFor adding custom CSS rules in pkgdown site create file pkgdown/extra.css in package root and edit it. For example, to make %&gt;% bold write the following:\n.pp {font-weight: bold;}"
  },
  {
    "objectID": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-custom-javascript",
    "href": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-custom-javascript",
    "title": "Highlight the Pipe. Pkgdown",
    "section": "Add custom JavaScript",
    "text": "Add custom JavaScript\nTo add custom JavaScript code to pkgdown site one should create and modify file pkgdown/extra.js in package root. Go here for code that initializes highlight.js and registers default R language parsing rules."
  },
  {
    "objectID": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#tweak-reference-page",
    "href": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#tweak-reference-page",
    "title": "Highlight the Pipe. Pkgdown",
    "section": "Tweak reference page",
    "text": "Tweak reference page\nFor highlight.js to work, code should be wrapped in &lt;pre&gt;&lt;span class=\"r\"&gt; tags. However, reference pages use only &lt;pre&gt;. To tweak these pages use the following function (with working directory being package root):\ntweak_ref_pages &lt;- function() {\n  # Find all reference pages\n  ref_files &lt;- list.files(\n    path = \"docs/reference/\",\n    pattern = \"\\\\.html\",\n    recursive = TRUE,\n    full.names = TRUE\n  )\n\n  lapply(ref_files, add_code_node)\n\n  invisible(ref_files)\n}\n\nadd_code_node &lt;- function(x) {\n  page &lt;- paste0(readLines(x), collapse = \"\\n\")\n\n  # Regular expression magic for adding &lt;code class = \"r\"&gt;&lt;/code&gt;\n  page &lt;- gsub('(&lt;pre.*?&gt;)', '\\\\1&lt;code class = \"r\"&gt;', page)\n  page &lt;- gsub('&lt;\\\\/pre&gt;', '&lt;\\\\/code&gt;&lt;\\\\/pre&gt;', page)\n\n  invisible(writeLines(page, x))\n}\nNote that as for 2017-10-27 this still can cause incorrect highlighting if some actual code is placed just after comment."
  },
  {
    "objectID": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-highlight.js-css-rules",
    "href": "blog/2017-10-29-highlight-the-pipe-pkgdown.html#add-highlight.js-css-rules",
    "title": "Highlight the Pipe. Pkgdown",
    "section": "Add highlight.js CSS rules",
    "text": "Add highlight.js CSS rules\nEdit pkgdown/extra.css for highlight.js classes. For template with Idea style along with R default classes look here."
  },
  {
    "objectID": "blog/2019-08-13-local-randomness-in-r.html",
    "href": "blog/2019-08-13-local-randomness-in-r.html",
    "title": "Local randomness in R",
    "section": "",
    "text": "Prologue\nLet’s say we have a deterministic (non-random) problem for which one of the solutions involves randomness. One very common example of such problem is a function minimization on certain interval: it can be solved non-randomly (like in most methods of optim()), or randomly (the simplest approach being to generate random set of points on interval and to choose the one with the lowest function value).\nWhat is a “clean” way of writing a function to solve the problem? The issue with direct usage of randomness inside a function is that it affects the state of outer random number generation:\n# Demo problem solving function\nminimize_f &lt;- function(f, from = 0, to = 1, n = 1e3) {\n  points &lt;- runif(n, min = from, max = to)\n\n  invisible(min(f(points)))\n}\n\n# Reference random number output\nset.seed(101)\nrunif(1)\n## [1] 0.3721984\n\n# Test random number output is different from reference one. But we want it to\n# be the same.\nset.seed(101)\nminimize_f(sin)\nrunif(1)\n## [1] 0.1016229\nSo how can we get “more clean” implementation which does not affect outer state? This short post is inspired by the following sources: this StackOverflow question by Yihui Xie and this cookbook advice.\n\n\nLocal randomness\nThe state of random number generation is stored in .Random.seed variable, which is “an integer vector” and it “can be saved and restored, but should not be altered by the user”. This gives us a very big hint about how to implement “local randomness”: capture state at the start of the function, make necessary computations, and restore state at the end. Bad news is, this also means that we enter here the dark realm of variables and their environments.\nHow to “save state”? In help page there is a note: “The object .Random.seed is only looked for in the user’s workspace”. Here “user’s workspace” seems to mean global environment, which should be addressed with variable .GlobalEnv. So, to “save state” we need to get a value of .Random.seed variable inside global environment. This is a job for get0():\nget_rand_state &lt;- function() {\n  # Using `get0()` here to have `NULL` output in case object doesn't exist.\n  # Also using `inherits = FALSE` to get value exactly from global environment\n  # and not from one of its parent.\n  get0(\".Random.seed\", envir = .GlobalEnv, inherits = FALSE)\n}\nHow to “restore state”? We need to assign certain value (of previously saved state) to a .Random.seed variable in global environment. This is a job for assign():\nset_rand_state &lt;- function(state) {\n  # Assigning `NULL` state might lead to unwanted consequences\n  if (!is.null(state)) {\n    assign(\".Random.seed\", state, envir = .GlobalEnv, inherits = FALSE)\n  }\n}\nHow to make “local randomness”? We can now save and restore random state. The final peace of a puzzle is to make restoration at the end of computations inside a function. This is a job for on.exit(): call for set_rand_state() should be wrapped in on.exit() to perform restoration exactly at the moment when function ends all operations it is supposed to do.\nNotes about positioning of calls inside a function:\n\nCall to get_rand_state() should be done right at the beginning of a function body to capture the state just before the function was called.\nSimply positioning call to set_rand_state() inside function body right before returning result might be not enough, because previous lines of code can terminate earlier (for example, with error). Function on.exit() guarantees execution of expression.\n\nGiving all that, the “clean” way of implementing “local randomness” is the following:\nmy_f &lt;- function() {\n  old_state &lt;- get_rand_state()\n  on.exit(set_rand_state(old_state))\n\n  # The rest of the code\n}\nLet’s check this solution on practice:\nminimize_f_clean &lt;- function(f, from = 0, to = 1, n = 1e3) {\n  old_state &lt;- get_rand_state()\n  on.exit(set_rand_state(old_state))\n\n  points &lt;- runif(n, min = from, max = to)\n\n  invisible(min(f(points)))\n}\n\n# Reference random number output (repeated for reading convenience)\nset.seed(101)\nrunif(1)\n## [1] 0.3721984\n\n# Output of `runif(1)` is the same as reference one, which was the goal\nset.seed(101)\nminimize_f_clean(sin)\nrunif(1)\n## [1] 0.3721984\n\n\nEpilogue\n\nCreating a function with “local randomness” although requires some dark R magic (with get0(), assign(), and on.exit() usage), is pretty straightforward.\nIf you have some non-trivial R problem, there is a good chance that Yihui Xie has already posted a question on StackOverflow about it.\n\n\n\n\n\n\n\nsessionInfo()\n\n\n\n\n\n## R version 3.6.1 (2019-07-05)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 18.04.3 LTS\n##\n## Matrix products: default\n## BLAS:   /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3\n## LAPACK: /usr/lib/x86_64-linux-gnu/libopenblasp-r0.2.20.so\n##\n## locale:\n##  [1] LC_CTYPE=ru_UA.UTF-8       LC_NUMERIC=C\n##  [3] LC_TIME=ru_UA.UTF-8        LC_COLLATE=ru_UA.UTF-8\n##  [5] LC_MONETARY=ru_UA.UTF-8    LC_MESSAGES=ru_UA.UTF-8\n##  [7] LC_PAPER=ru_UA.UTF-8       LC_NAME=C\n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C\n## [11] LC_MEASUREMENT=ru_UA.UTF-8 LC_IDENTIFICATION=C\n##\n## attached base packages:\n## [1] stats     graphics  grDevices utils     datasets  methods   base\n##\n## loaded via a namespace (and not attached):\n##  [1] compiler_3.6.1  magrittr_1.5    bookdown_0.11   tools_3.6.1\n##  [5] htmltools_0.3.6 yaml_2.2.0      Rcpp_1.0.1      stringi_1.4.3\n##  [9] rmarkdown_1.13  blogdown_0.12   knitr_1.23      stringr_1.4.0\n## [13] digest_0.6.19   xfun_0.7        evaluate_0.14"
  },
  {
    "objectID": "blog/2018-03-06-tao-of-tidygraph.html",
    "href": "blog/2018-03-06-tao-of-tidygraph.html",
    "title": "Tao of Tidygraph",
    "section": "",
    "text": "Quite some time ago I read the fantastic “Tao Te Programming” book by Patrick Burns. You can know this author from his everlasting work “The R Inferno”.\n“Tao Te Programming” is a vision about what a good programming should be. It is written in [programming] language-agnostic fashion but mainly with R in mind. It is organized in 81 chapters with some distinctive feature: at the end of the most chapters there are lists of chapters-“allies” and chapters-“opponents” to the chapter in question. During the reading I was really interested in the properties of graph that is created with these connections.\nFrom the other side of my life, I constantly hear about packages for tidy network analysis: tidygraph and ggraph. Both created by “compulsive package developer” (not sure, where to put “great” in this description) Thomas Lin Pedersen. They provide tidy tools for creating, manipulating and plotting network data. It happened, that I didn’t have an opportunity to work with them… until now."
  },
  {
    "objectID": "blog/2018-03-06-tao-of-tidygraph.html#creation",
    "href": "blog/2018-03-06-tao-of-tidygraph.html#creation",
    "title": "Tao of Tidygraph",
    "section": "Creation",
    "text": "Creation\nAlliance graph is a set of chapters-nodes which created based on “ally” and “opponent” connections. It is created using the following data from taoteprog (which I manually created while reading a book):\n\nttp_chapters - a tibble with chapter data (column chapter for its number and name for its name). The whole list of chapters can be found here.\nttp_edges - a tibble with data about chapter connections. It has the following columns:\n\nchapter1 for the chapter number at which list of connections is printed.\nchapter2 for the number of connected chapter.\ntype for connection type (either “ally” or “opponent”).\n\nNote that connections between chapters are not symmetrical, i.e. one chapter can be an “ally” or “opponent” of the another but not the other way around.\n\nttp_raw &lt;- tbl_graph(nodes = ttp_chapters, edges = ttp_edges, directed = TRUE)\nttp_raw\n## # A tbl_graph: 81 nodes and 124 edges\n## #\n## # A directed multigraph with 20 components\n## #\n## # Node Data: 81 x 2 (active)\n##   chapter name\n##     &lt;int&gt; &lt;chr&gt;\n## 1       1 Program\n## 2       2 Program Well\n## 3       3 Think Chess\n## 4       4 Carve Reality\n## 5       5 Solve the Problem\n## 6       6 Don't Solve the Problem\n## # ... with 75 more rows\n## #\n## # Edge Data: 124 x 3\n##    from    to type\n##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n## 1     4     9 ally\n## 2     4    54 ally\n## 3     5     6 opponent\n## # ... with 121 more rows\nThe main data structure of tidygraph is a tbl_graph which can be thought of as a combination of tibbles with data for nodes and edges. Note that values in from and to columns in edges tibble are the row indices of the nodes stored in nodes tibble, which will be a little confusing when nodes won’t cover all chapters.\n\n\n\n\n\n\n\n\n\n\n\nImportant features of this graph:\n\nThere are 2 loops in graph (chapters “Be Consistent”, “Follow The Way”). And all of them are of type “opponent”, which I consider to be a subtle joke rather than editor mistake. However, for exploration these edges will be removed.\nThere are 20 [weak] components in this graph (maximal groups of nodes where each pair can be connected by undirected path). The number is this big because there are many chapters with no “allies” and no “opponents” (isolated points in graph), which will be also removed for exploration.\n\nSo the graph of interest is constructed as follows:\nttp &lt;- ttp_raw %&gt;%\n  # Remove loops\n  activate(edges) %&gt;%\n  filter(!edge_is_loop()) %&gt;%\n  # Remove isolated nodes\n  activate(nodes) %&gt;%\n  filter(!node_is_isolated())\nttp\n## # A tbl_graph: 67 nodes and 122 edges\n## #\n## # A directed simple graph with 6 components\n## #\n## # Node Data: 67 x 2 (active)\n##   chapter name\n##     &lt;int&gt; &lt;chr&gt;\n## 1       4 Carve Reality\n## 2       5 Solve the Problem\n## 3       6 Don't Solve the Problem\n## 4       7 Enjoy Confusion\n## 5       8 Procrastinate\n## 6       9 Verbalize and Nounalize\n## # ... with 61 more rows\n## #\n## # Edge Data: 122 x 3\n##    from    to type\n##   &lt;int&gt; &lt;int&gt; &lt;chr&gt;\n## 1     1     6 ally\n## 2     1    44 ally\n## 3     2     3 opponent\n## # ... with 119 more rows\nSome explanations of the magic just happened:\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\nedge_is_loop(), node_is_isolated() and group_components() are functions of the same essence as n() in dplyr: they both should be called inside a graph computation functions."
  },
  {
    "objectID": "blog/2018-03-06-tao-of-tidygraph.html#visualization",
    "href": "blog/2018-03-06-tao-of-tidygraph.html#visualization",
    "title": "Tao of Tidygraph",
    "section": "Visualization",
    "text": "Visualization\nttp %&gt;%\n  ggraph(layout = \"nicely\") +\n    geom_edge_link(\n      aes(colour = type),\n      arrow = arrow(length = unit(1.5, \"mm\")),\n      start_cap = circle(3, \"mm\"),\n      end_cap = circle(3, \"mm\")\n    ) +\n    geom_node_text(aes(label = chapter), size = 5) +\n    scale_edge_colour_manual(values = c(ally = \"#22B022\",\n                                        opponent = \"#A4AAF6\")) +\n    theme_graph() +\n    labs(\n      title = '\"Tao Te Programming\" alliance graph',\n      subtitle = \"Nodes represent chapter numbers, edges - connections\",\n      caption = \"@echasnovski\"\n    )\n\nImportant notes about creation of this plot:\n\nggraph() is the equivalent of ggplot() in ggplot2 and also returns a ggplot object. ggraph() takes a layout argument for the type of graph arrangement in plot.\ngeom_edge_link() and geom_node_text() are very much ggplot2-like functions: they create layers of plot (for links and nodes respectively).\ntheme_graph() is a pre-installed ggraph theme."
  },
  {
    "objectID": "blog/2018-03-06-tao-of-tidygraph.html#exploration",
    "href": "blog/2018-03-06-tao-of-tidygraph.html#exploration",
    "title": "Tao of Tidygraph",
    "section": "Exploration",
    "text": "Exploration\nNotes about graph structure:\n⬛ There are two relatively big components (with 16 and 40 nodes) and 4 small ones (one with 5 nodes and three with 2).\n⬛ There are considerably more “ally” connections then “opponent” which can be confirmed by the following code:\nttp_edges %&gt;% count(type)\n## # A tibble: 2 x 2\n##   type         n\n##   &lt;chr&gt;    &lt;int&gt;\n## 1 ally        88\n## 2 opponent    36\n⬛ The graph itself can be considered as signed graph, i.e. a graph “in which each edge has a positive or negative sign”. This type of graphs can be studied for special community detection: nodes within common community should be primarily connected with positive (“ally”) edges and between communities - by negative (“opponent”) ones.\nInteresting problem description and solution are presented in this paper. I implemented the variation of the suggested approach, which can be found in my raw analysis on github (or here for its html output). Implementation is rather verbose with no very special insight, so I decided to not include it here."
  },
  {
    "objectID": "blog/2023-04-24-average-color-scheme.html",
    "href": "blog/2023-04-24-average-color-scheme.html",
    "title": "Average Neovim color scheme",
    "section": "",
    "text": "Originally posted on Reddit\n\n\nHello, Neovim users!\nFor quite some time I was wondering how would “average popular Neovim color scheme” look like. I mean, it would certainly contain much purple and blue, but how much exactly? It is not a trivial thing to compute.\nThankfully, the recent release of ‘mini.colors’ makes this task much more manageable.\nI decided that general approach to averaging color schemes would be as follows:\n\nHighlight group will be present in output if it is present (explicitly defined and differs from Neovim’s built-in values) in all reference color schemes.\nAttribute of highlight group (like foreground, background, underline, etc.) is present if it is present in all reference color schemes.\n\nOf course, there are other ways to tackle this problem, but such a strict approach will ensure that output palette is as consistent as possible. Otherwise output will contain many different but similar colors, which doesn’t seem good.\nFor reference color schemes I decided to go through awesome-neovim and pick top 5 Lua implemented color schemes with most stars. Here is the final list (as of 2023-04-20):\n\n\n\nColor scheme\nStargazers\n\n\n\n\nfolke/tokyonight.nvim\n3476 stars\n\n\ncatppuccin/nvim\n2639 stars\n\n\nrebelot/kanagawa.nvim\n2219 stars\n\n\nEdenEast/nightfox.nvim\n2055 stars\n\n\nprojekt0n/github-nvim-theme\n1379 stars\n\n\n\n(Full disclosure: initially I wanted to go with “top 5 Neovim color schemes” which would have included sainnhe/everforest as fifth one. But although its colors are sooooooo nice, it is quite different in terms of which attributes it defines for certain highlight groups. Including it instead of ‘projekt0n/github-nvim-theme’ resulted in not very coherent output. So in the end it was decided to take “top 5 Lua color schemes”.)\nThe result is in the post. Yes, it is blue-purple-ish, as expected. Also in my opinion it does have a quality of “looks like tokyonight/catppuccin/kanagawa, but not quite”.\nHere is a gist with colorscheme files for average dark and light variants (put any of them into ~/.config/nvim/colors and use :colorscheme command as usual) along with a script used to create them. Please don’t expect much from actually applying color scheme files, as they define only basic highlight groups with very limited advanced support like many plugins, tree-sitter, semantic tokens, etc.\nBesides fulfilling general curiosity, I also plan to use this result to make a data-driven decisions for a new upcoming color scheme generator in ‘mini.nvim’.\nHope you enjoyed reading this!"
  },
  {
    "objectID": "blog/2022-09-02-usage-of-after-ftplugin-directory.html",
    "href": "blog/2022-09-02-usage-of-after-ftplugin-directory.html",
    "title": "Usage of ‘after/ftplugin’ directory for filetype-specific configuration",
    "section": "",
    "text": "Originally posted on Reddit\nDo you create any configuration that is specific to a certain file type (like setting local options or buffer-local variables, creating local mappings, running some function, etc.)?\n\nNo. Sooner or later you probably will. So let’s pretend that the answer is…\nYes. Do you use autocommands with FileType event for this?\n\nNo. Good. There is probably nothing valuable for you to read here. Sorry for taking your time.\nYes. Although totally doable, there is another approach which is not popularized much. You can create ‘after/ftplugin’ directory within your configuration and put filetype-named files to be sourced for this file type. As a bonus, this is technically a way to create a “filetype plugin”, so you can now say that you are a plugin author and maintainer. Example: ‘after/ftplugin/lua.lua’ will be sourced for ‘lua’ file types (specifically, every time filetype buffer option is set to ‘lua’). Don’t forget to use local variants of setting options (:setlocal or vim.opt_local) and creating mappings (vim.api.nvim_buf_set_keymap()).\n\n\nThis is meant as a Friday post to make more people aware of ‘after/ftplugin’ directory, because I find this approach really more structured than using autocommands. For reference, here is how I do it.\nSome further reading:\n\n:h ftplugin (more in-depth about this approach; not really needed for simple use cases)\n:h ftplugin-special (notes about special things to use in these files)\n:h lua-vim-setlocal (use vim.opt_local to set option locally)\n:h after-directory (use ‘after’ directory so that these options are not overridden by defaults)"
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html",
    "title": "Elo and EloBeta models in snooker",
    "section": "",
    "text": "For many years I’ve been following snooker as a sport. It has it all: hypnotic beauty of smart play, elegance of cue strikes and psychological tension of competition. The one thing I don’t like that much is its ranking system.\nGenerally speaking, current snooker ranking is based on player accomplishments in tournaments (events) with different “weights” for different tournaments. Long time ago, it just used World Championships. Then, after more events had emerged, there was a table of points player could earn for winning at certain stage of tournament. Now it has the form of “rolling” sum of prize money player won during (roughly) past two calendar years.\nThis system has two main advantages: it is simple (win more money -&gt; rise in rankings) and predictabile (want to get certain ranking -&gt; win certain amount of money, other things being equal). The problem is that this type of rankings doesn’t account the strength (skill, form) of player’s opponents. The usual counter-argument for this is that if player reached high stage of tournament then he/she is “strong” at this moment of time (“weak players don’t win tournaments”). Well, it does sound quite convincing. However, in snooker, as in any sport, the role of chance should be taken into account: if player is “weaker” it doesn’t mean that he can’t ever win in a match with “stronger” player. It means that this happens less often then the other way around. Here where Elo model comes into play.\nThe idea behind Elo model is that each player is associated with numerical rating. The assumption is that a result of a game between two players can be predicted based on difference of their ratings: more value indicates more probability of “stronger” (with higher rating) player to win. Elo ranking is based on current player “strength” derived by wins against other players. This eliminates main disadvantage of current official ranking system. It is also capable of updating player rating during tournament to numerically react to player’s strong tournament performance.\nHaving some practical experience with Elo ratings, I think it can do well in snooker too. However, there is one obstacle: it is devised for competitions with uniform type of games. Yes, there are some variations to account for home field advantage in football or first move advantage in chess (both by adding fixed amount of rating points to “less advantageous” player). In snooker, however, matches are played in the “best of \\(N\\)” format: the first one to win \\(n = \\frac{N + 1}{2}\\) frames wins a match. We will also call this format “\\(n\\) to win”.\nIntuitively, winning a “10 to win” match (final of major tournament) should be harder for “weaker” player then “4 to win” match (first rounds of current Home Nations tournaments). This is taken into account by my proposed EloBeta model.\nTo celebrate actual start of 2018/19 snooker season I decided to write this post in which I explore adequacy of both Elo and EloBeta models on snooker match results. Note that the goal is not to build models for forecasting and gambling purposes but for assessing players “strength” and creating “fair” ranking.\nThe idea of using Elo rating in snooker is not new at all. There are works on this topic, for example:\n\nSnooker Analyst provides “Elo inspired” (more like Bradley–Terry model) rating system based on the idea of updating rating based on difference between “actual frames won” and “expected frames won”. This approach is a little bit questionable. Surely, more frame difference should indicate more difference in strength, however, achieving that is not player’s goal. In snooker aim is “just” to win match, i.e. get certain amount of frame wins before the opponent.\nThis forum discussion with implementation of basic Elo model.\nPossibly, there are other works that I’ve missed. I will highly appreciate any information on this topic."
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#elo",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#elo",
    "title": "Elo and EloBeta models in snooker",
    "section": "Elo",
    "text": "Elo\n\n\n\n\n\n\nCode for Elo model\n\n\n\n\n\n#' @details This function is vectorized by all its arguments. Also usage of\n#' `...` is crucial to allow supplying unrelated arguments in the future.\n#'\n#' @return A probability of player 1 (rating `rating1`) wins in a match with\n#'   player 2 (rating `rating2`). Here difference in ratings directly affects\n#'   the outcome.\nelo_win_prob &lt;- function(rating1, rating2, ksi = 400, ...) {\n  norm_rating_diff &lt;- (rating2 - rating1) / ksi\n\n  1 / (1 + 10^norm_rating_diff)\n}\n\n#' @return A rating function for Elo model that can be supplied to\n#'   `comperank::add_iterative_ratings()`.\nelo_fun_gen &lt;- function(K, ksi = 400) {\n  function(rating1, score1, rating2, score2) {\n    comperank::elo(rating1, score1, rating2, score2, K = K, ksi = ksi)[1, ]\n  }\n}\n\n\n\nElo model updates ratings by the following steps:\n\nCompute probability (before the match) of certain player to win the match. Probability of one player (we will call him/her “first”) with “identifier” \\(i\\) and rating \\(r_i\\) winning against the other player (“second”) with “identifier” \\(j\\) and rating \\(r_j\\) is equal to\n\\[Pr(r_i , r_j) = \\frac{1}{1 + 10^{(r_j - r_i)/400}}\\]\nThis way of computing probability is aligned with third model assumption.\nDifference normalization by 400 is a mathematical way to say which difference is considered “big”. This can be replaced by a model parameter \\(\\xi\\), however this only affects the spread of future ratings and is often an overkill. Number 400 is fairly standard in chess world.\nIn general approach probability is equal to \\(L(r_j - r_i)\\) where \\(L(x)\\) is some strictly increasing function with values from 0 to 1. We will use logistic curve to compute winning probability. More thorough study can be found in this article.\nObtain match result \\(S\\). In basic model it is 1 if first player wins (second player loses), 0.5 in case of draw and 0 if second player wins.\nUpdate ratings:\n\n\\(\\delta = K \\cdot (S - Pr(r_i , r_j))\\). This is the value by which ratings will change. It introduces the “K factor” (only model parameter). Less \\(K\\) (ratings being equal) means less change in ratings - model is more conservative, i.e. more wins is needed to “prove” increase in “strength”. On the other hand, more \\(K\\) means more “trust” to the current results than current ratings. Choosing “optimal” \\(K\\) is a way to produce “good” ranking system.\n\\(r_i^{(new)} = r_i + \\delta\\), \\(r_j^{(new)} = r_j - \\delta\\).\n\n\nNotes:\n\nAs one can see from rating update formulas, the sum of ratings for all ranked players doesn’t change over time: rating increase of one rating can be only done by taking this amount from another player.\nPlayers without any matches played are associated with initial rating 0. The usual value is 1500, however I don’t see any other reason except psychological for this. With previous note, using 0 means that sum of all ratings will always be 0, which is kind of beautiful.\nIt is needed some matches to be played in order for rating to represent player’s “strength”. This introduces a problem: newly added players start with rating 0 which is almost surely not the lowest among current players. In other words, newcomers are considered to be “stronger” than some other players. This should be dealt with by external procedures of rating updates when introducing new player: maybe he/she should start with some low rating while compensating overall sum decrease by uniform increasing of other players’ ratings.\nWhy this procedure makes sense? In case of equal ratings \\(\\delta\\) always equals \\(0.5 \\cdot K\\). Let’s say, for example, that \\(r_i = 0\\) and \\(r_j = 400\\). It means that probability of first player winning is \\(\\frac{1}{1 + 10} \\approx 0.0909\\), i.e. he/she will win 1 out of 11 matches.\n\nIn case of win, he/she will be awarded with approximately \\(0.909 \\cdot K\\) increase, which is more than in case of equal ratings.\nIf he/she is defeated, then rating is decreased by approximately \\(0.0909 \\cdot K\\), which is less than in case of equal ratings.\n\nThis shows that Elo model is aligned with fifth model assumption: winning against “stronger” opponent leads to bigger increase in rating than winning against “weaker” opponent and vice versa.\n\nOf course, Elo model has its (fairly high-level) practical issues. However, the most important for our research is that “it thinks” that all matches are played in uniform conditions. It means, that match length isn’t taken into account: winning in “4 to win” match is rewarded the same as winning in “10 to win” match. That is where EloBeta comes into play."
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#elobeta",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#elobeta",
    "title": "Elo and EloBeta models in snooker",
    "section": "EloBeta",
    "text": "EloBeta\n\n\n\n\n\n\nCode for EloBeta model\n\n\n\n\n\n#' @details This function is vectorized by all its arguments.\n#'\n#' @return A probability of player 1 (rating `rating1`) wins in a match with\n#'   player 2 (rating `rating2`). Match is played until either player wins\n#'   `frames_to_win` frames. Here difference in ratings directly affects\n#'   the probability of winning one frame.\nelobeta_win_prob &lt;- function(rating1, rating2, frames_to_win, ksi = 400, ...) {\n  prob_frame &lt;- elo_win_prob(rating1 = rating1, rating2 = rating2, ksi = ksi)\n\n  # Probability that first player wins `frames_to_win` frames sooner than second\n    # player based on probability of first player to win one frame `prob_frame`.\n    # Frames are treated as independent games.\n  pbeta(prob_frame, frames_to_win, frames_to_win)\n}\n\n#' @return Match result in terms of player 1 win: 1 if he/she wins, 0.5 in case\n#'   of a draw, and 0 if he/she loses.\nget_match_result &lt;- function(score1, score2) {\n  # There are no ties in snooker but this handles general case\n  near_score &lt;- dplyr::near(score1, score2)\n\n  dplyr::if_else(near_score, 0.5, as.numeric(score1 &gt; score2))\n}\n\n#' @return A rating function for EloBeta model that can be supplied to\n#'   `comperank::add_iterative_ratings()`.\nelobeta_fun_gen &lt;- function(K, ksi = 400) {\n  function(rating1, score1, rating2, score2) {\n    prob_win &lt;- elobeta_win_prob(\n      rating1 = rating1, rating2 = rating2,\n      frames_to_win = pmax(score1, score2), ksi = ksi\n    )\n\n    match_result &lt;- get_match_result(score1, score2)\n    delta &lt;- K * (match_result - prob_win)\n\n    c(rating1 + delta, rating2 - delta)\n  }\n}\n\n\n\nIn Elo model difference in ratings directly affects the outcome probability of winning the whole match. The main idea behind EloBeta model is to make difference in ratings directly affect the outcome of one frame and actually compute the probability of a player winning \\(n\\) frames before his opponent.\nThe question is: how to compute this probability? Turns out, this is one of the oldest task in the history of probability theory and has its own name - Problem of points. Very nice description can be found in this post. Using its notation, the outcome probability equals to:\n\\[\nP(n, n) = \\sum\\limits_{j = n}^{2n-1}{{{2n-1} \\choose {j}} p^j (1-p)^{2n-1-j}}\n\\]\nHere \\(P(n, n)\\) is a probability of first player to win in “\\(n\\) to win” match; \\(p\\) is his/her probability to win one frame (\\(1-p\\) - opponents probability). With this approach one assumes that result of frames inside a match is independent one from another. This is arguable, but necessary assumption (in terms of this model).\nIs there a way to compute this faster? It turns out, that the answer is yes. After hours of formula wrangling, practical experiments and internet search I found the following property of regularized incomplete beta function \\(I_x(a, b)\\). By substituting \\(m = k,~ n = 2k - 1\\) into that property and changing \\(k\\) into \\(n\\) we obtain that \\(P(n, n) = I_p(n, n)\\).\nThis is also very good news for R users, because \\(I_p(n, n)\\) can be computed as simply as pbeta(p, n, n). Note that the general case probability of winning \\(n\\) frames before opponent wins \\(m\\) can also be computed as \\(I_p(n, m)\\) and pbeta(p, n, m) respectively. This introduces the reach field of updating winning probabilities during the mach.\nSo the procedure of updating ratings within EloBeta model is as follows (given ratings \\(r_i, r_j\\), number of frames \\(n\\) to win in match and match outcome \\(S\\), as in Elo model):\n\nCompute probability of first player to win the frame: \\(p = Pr(r_i , r_j) = \\frac{1}{1 + 10^{(r_j - r_i)/400}}\\).\nCompute probability of this player to win the match: \\(Pr^{Beta}(r_i, r_j) = I_p(n, n)\\). For example, if \\(p\\) equals 0.4 then probability to win “4 to win” match drops to r round(pbeta(0.4, 4, 4), 2) and with “18 to win” to r round(pbeta(0.4, 18, 18), 2).\nUpdate ratings:\n\n\\(\\delta = K \\cdot (S - Pr^{Beta}(r_i , r_j))\\).\n\\(r_i^{(new)} = r_i + \\delta\\), \\(r_j^{(new)} = r_j - \\delta\\).\n\n\nNote that, as difference in ratings affects probability of winning one frame (not the whole match), we should expect lower optimal \\(K\\): part of \\(\\delta\\)’s value comes from amplifying effect of \\(Pr^{Beta}(r_i, r_j)\\).\nThe idea of computation match result based on probability of winning one frame is not very novel. On this site, authored by François Labelle, you can find online computation of probability of winning a “best of \\(N\\)” match (with some other functionality). I was very glad to notice that results of our computations match. However, I didn’t find any sources of incorporating this into Elo updating procedure. I will highly appreciate any information on this topic.\nI’ve only managed to found this post and this description of Elo system on backgammon internet server (FIBS). Here different matches are handled by multiplying rating difference by square root of match length. However, there seems to be no strong theoretical reason for this."
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#data",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#data",
    "title": "Elo and EloBeta models in snooker",
    "section": "Data",
    "text": "Data\n\n\n\n\n\n\nCode for data creation\n\n\n\n\n\n# Function to split cases between \"train\", \"validation\", and \"test\" types\nsplit_cases &lt;- function(n, props = c(0.5, 0.25, 0.25)) {\n  breaks &lt;- n * cumsum(head(props, -1)) / sum(props)\n  id_vec &lt;- findInterval(seq_len(n), breaks, left.open = TRUE) + 1\n\n  c(\"train\", \"validation\", \"test\")[id_vec]\n}\n\npro_players &lt;- snooker_players %&gt;% filter(status == \"pro\")\n\n# Matches only between pro players.\npro_matches_all &lt;- snooker_matches %&gt;%\n  # Using actually happened matches\n  filter(!walkover1, !walkover2) %&gt;%\n  # Filter matches only between pro players\n  semi_join(y = pro_players, by = c(player1Id = \"id\")) %&gt;%\n  semi_join(y = pro_players, by = c(player2Id = \"id\")) %&gt;%\n  # Add 'season' column\n  left_join(\n    y = snooker_events %&gt;% select(id, season), by = c(eventId = \"id\")\n  ) %&gt;%\n  # Ensure arranging by end date\n  arrange(endDate) %&gt;%\n  # Prepare for widecr format\n  transmute(\n    game = seq_len(n()),\n    player1 = player1Id, score1, player2 = player2Id, score2,\n    matchId = id, endDate, eventId, season,\n    # Compute match type (\"train\", \"validation\", or \"test\") with 50/25/25 split\n    matchType = split_cases(n())\n  ) %&gt;%\n  # Convert to widecr format\n  as_widecr()\n\n# Matches only between pro players in not invitational events (which by\n  # quantity is dominated by Championship League).\npro_matches_off &lt;- pro_matches_all %&gt;%\n  anti_join(\n    y = snooker_events %&gt;% filter(type == \"Invitational\"),\n    by = c(eventId = \"id\")\n  )\n\n# Split confirmation\nget_split &lt;- . %&gt;% count(matchType) %&gt;% mutate(share = n / sum(n))\n\n# This should give 50/25/25 split (train/validation/test).\npro_matches_all %&gt;% get_split()\n\n# This gives different split because invitational events aren't spread evenly\n  # during season. However, this way matches are split based on the same\n  # __time__ breaks as in `pro_matches_all`. This ensures that matches with same\n  # type represent identical __time periods__.\npro_matches_off %&gt;% get_split()\n\n# Grid for K factor\nk_grid &lt;- 1:100\ntot_n_players &lt;- pro_matches_all %&gt;%\n  as_longcr() %&gt;%\n  distinct(player) %&gt;%\n  nrow()\n\nget_mean_nmatches &lt;- . %&gt;% as_longcr() %&gt;%\n  count(player) %&gt;%\n  summarise(meanMatches = mean(n)) %&gt;%\n  pull(meanMatches) %&gt;%\n  round(digits = 1)\n\nall_matches_per_player &lt;- get_mean_nmatches(pro_matches_all)\noff_matches_per_player &lt;- get_mean_nmatches(pro_matches_off)\n\npro_matches_off_split &lt;- pro_matches_off %&gt;%\n  get_split() %&gt;%\n  transmute(matchType, share = round(share * 100, 1)) %&gt;%\n  tibble::deframe()\n\noff_split &lt;- pro_matches_off_split[c(\"train\", \"validation\", \"test\")] %&gt;%\n  paste0(collapse = \"/\")\n\n\n\nWe will use snooker data from comperank package. The original source is snooker.org site. Results are taken from the following matches:\n\nMatch is from 2016/17 or 2017/18 seasons.\nMatch is a part of “professional” snooker event. That is:\n\nIt has “Invitational”, “Qualifying”, or “Ranking” type. We will also differ two sets of matches: “all matches” (from all these events) and “official matches” (not from invitational events). There are two main reasons behind it:\n\nIn invitational events not all players are given opportunity to change their ratings (which isn’t a clearly bad thing under Elo and EloBeta models).\nBelief that players “take seriously” only official ranking events. Note that most of “Invitational” events are from “Championship League” which, I think, are treated by most players as practice with opportunity to win money, i.e. “not very seriously”. Their presence can affect outcome ratings.\n\nBesides “Championship League”, other invitational events are: “2016 China Championship”, both “Champion of Champions”, both “Masters”, “2017 Hong Kong Masters”, “2017 World games”, “2017 Romanian Masters”.\nIt describes traditional snooker (not 6 Red or Power Snooker) between individual players (not teams).\nBoth genders can take part (not only men or women).\nPlayers of all ages can take part (not only seniors or under 21).\nIt is not “Shoot-Out” as those events are treated differently in snooker.org database.\n\nMatch actually happened: its result is due to actual play from both players.\nMatch is between two professionals (pros). List of professionals is taken as for 2017/18 season (r tot_n_players players). This seems like the most controversial decision, as removing “pro-ama” (“ama” for “amateur”) and “ama-ama” matches leads to “closing eyes” on pros’ losses to amateurs, and thus giving unfair advantage to those pros. I think this choice is necessary to ensure absence of rating inflation which will happen if matches with amateurs are taken into account. Another possibility would be to study pros and amas together, but this seems unreasonable to me within this research. Professional’s loss to amateur is treated as loss of opportunity to increase rating.\n\nThe final numbers of used matches are r nrow(pro_matches_all) for “all matches” and r nrow(pro_matches_off) for “official matches” (r all_matches_per_player and r off_matches_per_player matches per player respectively)."
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#methodology",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#methodology",
    "title": "Elo and EloBeta models in snooker",
    "section": "Methodology",
    "text": "Methodology\n\n\n\n\n\n\nCode for methodology functions\n\n\n\n\n\n#' @param matches A `longcr` or `widecr` object with column `matchType` (with\n#'   type of match for the experiment: \"train\", \"validation\", or \"test\").\n#' @param test_type A type of match to be used for computing goodness of fit.\n#'   For experiment correctness, all matches with this type should happen later\n#'   than all other (\"warm-up\") matches. This means having bigger values in\n#'   `game` column.\n#' @param k_vec Vector of \"K factor\" values to compute goodness of fit.\n#' @param rate_fun_gen Function that, given \"K factor\" value, returns rating\n#'   function that can be supplied to `comperank::add_iterative_ratings()`.\n#' @param get_win_prob Function to compute rating probability based on\n#'   ratings of players (`rating1`, `rating2`) and number of frames needed to\n#'   win in a match (`frames_to_win`). __Note__ that it should be vectorized by\n#'   all its arguments.\n#' @param initial_ratings Initial ratings in format ready for\n#'   `comperank::add_iterative_ratings()`.\n#'\n#' @details This function computes:\n#' - History of iterative ratings after arranging `matches` by `game` column.\n#' - For matches with type equals to `test_type`:\n#'     - Probability of player 1 winning.\n#'     - Match result in terms of player 1 win: 1 if he/she wins, 0.5 in case of\n#'     a draw, and 0 if he/she loses.\n#' - Goodness of fit in the form of RMSE: square root of mean square error,\n#' where \"error\" is difference between predicted probability and match result.\n#'\n#' @return A tibble with columns 'k' for \"K factor\" and 'goodness' for RMSE\n#'   goodness of fit.\ncompute_goodness &lt;- function(matches, test_type, k_vec, rate_fun_gen,\n                             get_win_prob, initial_ratings = 0) {\n  cat(\"\\n\")\n  map_dfr(k_vec, function(cur_k) {\n    # Track execution\n    cat(cur_k, \" \")\n    matches %&gt;%\n      arrange(game) %&gt;%\n      add_iterative_ratings(\n        rate_fun = rate_fun_gen(cur_k), initial_ratings = initial_ratings\n      ) %&gt;%\n        left_join(y = matches %&gt;% select(game, matchType), by = \"game\") %&gt;%\n        filter(matchType %in% test_type) %&gt;%\n        mutate(\n          # Number of frames needed to win in a match\n          framesToWin = pmax(score1, score2),\n          # Probability of player 1 winning a match with `frame_to_win` frames\n            # needed to win.\n          winProb = get_win_prob(\n            rating1 = rating1Before, rating2 = rating2Before,\n            frames_to_win = framesToWin\n          ),\n          result = get_match_result(score1, score2),\n          squareError = (result - winProb)^2\n        ) %&gt;%\n        summarise(goodness = sqrt(mean(squareError)))\n  }) %&gt;%\n    mutate(k = k_vec) %&gt;%\n    select(k, goodness)\n}\n\n#' A wrapper for `compute_goodness()` to be used with design matrix data.\ncompute_goodness_wrap &lt;- function(matches_name, test_type, k_vec,\n                                  rate_fun_gen_name, win_prob_fun_name,\n                                  initial_ratings = 0) {\n  matches_tbl &lt;- get(matches_name)\n  rate_fun_gen &lt;- get(rate_fun_gen_name)\n  get_win_prob &lt;- get(win_prob_fun_name)\n\n  compute_goodness(\n    matches_tbl, test_type, k_vec, rate_fun_gen, get_win_prob, initial_ratings\n  )\n}\n\n#' Function to perform experiment.\n#'\n#' @param test_type Vector of values for `test_type` for `compute_goodness()`.\n#' @param rating_type Names of rating models.\n#' @param data_type Suffixes of data types.\n#' @param k_vec,initial_ratings Values for `compute_goodnes()`\n#'\n#' @details This function generates design matrix and computes multiple values\n#' of goodness of fit for different combinations of rating and data types. For\n#' this to work, variables with the following combinations of names should be\n#' created in the global environment:\n#' - \"pro_matches_\" + `&lt;test type&gt;` + `&lt;data type&gt;` for matches data.\n#' - `&lt;rating type&gt;` + \"_fun_gen\" for rating function generators.\n#' - `&lt;rating type&gt;` + \"_win_prob\" for functions that compute win probability.\n#'\n#' @return A tibble with columns:\n#' - __testType__ &lt;chr&gt; : Test type identifier.\n#' - __ratingType__ &lt;chr&gt; : Rating type identifier.\n#' - __dataType__ &lt;chr&gt; : Data type identifier.\n#' - __k__ &lt;dbl/int&gt; : Value of \"K factor\".\n#' - __goodness__ &lt;dbl&gt; : Value of goodness of fit.\ndo_experiment &lt;- function(test_type = c(\"validation\", \"test\"),\n                          rating_type = c(\"elo\", \"elobeta\"),\n                          data_type = c(\"all\", \"off\"),\n                          k_vec = k_grid,\n                          initial_ratings = 0) {\n  crossing(\n    testType = test_type, ratingType = rating_type, dataType = data_type\n  ) %&gt;%\n    mutate(\n      dataName = paste0(\"pro_matches_\", testType, \"_\", dataType),\n      kVec = rep(list(k_vec), n()),\n      rateFunGenName = paste0(ratingType, \"_fun_gen\"),\n      winProbFunName = paste0(ratingType, \"_win_prob\"),\n      initialRatings = rep(list(initial_ratings), n()),\n      experimentData = pmap(\n        list(dataName, testType, kVec,\n             rateFunGenName, winProbFunName, initialRatings),\n        compute_goodness_wrap\n      )\n    ) %&gt;%\n    unnest(experimentData) %&gt;%\n    select(testType, ratingType, dataType, k, goodness)\n}\n\n\n\nTo find “optimal” value of \\(K\\) we will use the even grid \\(K = 1, 2, ..., 100\\). Accounting for greater values seems to be unreasonable which is confirmed by the experiment. The following procedure is used to find it:\n\nFor every \\(K\\):\n\nCompute history of iterative ratings of certain model based on certain data set. It means that ratings of players would be known before every match. This is done with add_iterative_ratings() function from comperank package. This procedure corresponds to “live ratings” which update after every match.\nBased on data, starting from a certain (distant from the beginning) moment in time, compute goodness of model fit. We will use RMSE between probability of first player to win (computed based on model) and match result. That is \\(RMSE = \\sqrt{\\frac{1}{|T|} \\sum\\limits_{t \\in T}{(S_t - P_t)^2}}\\), where \\(T\\) - indices of used matches, \\(|T|\\) - number of used matches, \\(S_t\\) - result of match for first player, \\(P_t\\) - probability of first player to win the match (computed based on model). Not including matches from the beginning of data is needed for ratings to “catch up” to “current strength” from initial ratings.\n\nThe value of \\(K\\) with stable minimal RMSE is said to be optimal. Here by “stable” we mean that small RMSE values is present in some neighborhood of optimal \\(K\\) (will be controlled not very strictly by looking at graphs). Values of RMSE lower 0.5 (value for “model” with constant 0.5 probability) will be considered a success.\n\nAs one of the goals is to study stability of models, data will be split into three subsets: “train”, “validation” and “test”. They are ordered in time, i.e. any “train”/“validation” match has ending time earlier than any “validation”/“test” match. I decided to do actual split in 50/25/25 proportion for “all matches”. Split of “official matches” is done by removing from “all matches” invitational events. It gives split not totally in desired proportion, but rather r off_split. However, this approach ensures that matches with same type in both match data represent identical time periods.\nExperiment will be performed for all combinations of the following variables:\n\nType of model: Elo and EloBeta.\nType of match data: “All matches” and “official matches”.\nExperiment type: “Validation” (“validation” matches are used for computing RMSE after “warming up” on “train” matches) and “Test” (“test” matches are used after “warming up” on both “train” and “validation” ones)."
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#results",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#results",
    "title": "Elo and EloBeta models in snooker",
    "section": "Results",
    "text": "Results\n\n\n\n\n\n\nCode for doing experiment\n\n\n\n\n\npro_matches_validation_all &lt;- pro_matches_all %&gt;% filter(matchType != \"test\")\npro_matches_validation_off &lt;- pro_matches_off %&gt;% filter(matchType != \"test\")\npro_matches_test_all &lt;- pro_matches_all\npro_matches_test_off &lt;- pro_matches_off\n# Takes some time to run\nexperiment_tbl &lt;- do_experiment()\n\n\n\n\n\n\n\n\n\nCode for producing results of experiment\n\n\n\n\n\nexperiment_tbl &lt;- readRDS(\n  \"2018-07-03-elo-and-elobeta-models-in-snooker_results.rds\"\n)\ncap_first &lt;- function(x) {\n    paste0(toupper(substring(x, 1, 1)), substring(x, 2))\n}\n\nplot_data &lt;- experiment_tbl %&gt;%\n  unite(group, ratingType, dataType) %&gt;%\n  mutate(\n    testType = cap_first(testType),\n    groupName = recode(\n      group, elo_all = \"Elo, all matches\", elo_off = \"Elo, official matches\",\n      elobeta_all = \"EloBeta, all matches\",\n      elobeta_off = \"EloBeta, official matches\"\n    ),\n    # Ensure preferred order. This is needed because sorting of strings will\n      # give \"Elo, all matches\", \"EloBeta, all matches\", \"EloBeta, official\n      # matches\", and \"Elo, official matches\" as, apperently, non-letters are\n      # ignored while sorting.\n    groupName = factor(groupName, levels = unique(groupName))\n  )\n\ncompute_optimal_k &lt;- . %&gt;% group_by(testType, groupName) %&gt;%\n  slice(which.min(goodness)) %&gt;%\n  ungroup()\ncompute_k_labels &lt;- . %&gt;% compute_optimal_k() %&gt;%\n  mutate(label = paste0(\"K = \", k)) %&gt;%\n  group_by(groupName) %&gt;%\n  # If optimal K within future facet is on the right, it needs a little\n    # adjustment to the right. If on the left - full and a little adjustment to\n    # the left.\n  mutate(hjust = - (k == max(k)) * 1.1 + 1.05) %&gt;%\n  ungroup()\n\nplot_experiment_results &lt;- function(results_tbl) {\n  ggplot(results_tbl) +\n    geom_hline(\n      yintercept = 0.5, colour = \"#AA5555\", size = 0.5, linetype = \"dotted\"\n    ) +\n    geom_line(aes(k, goodness, colour = testType)) +\n    geom_vline(\n      data = compute_optimal_k,\n      mapping = aes(xintercept = k, colour = testType),\n      linetype = \"dashed\", show.legend = FALSE\n    ) +\n    geom_text(\n      data = compute_k_labels,\n      mapping = aes(k, Inf, label = label, hjust = hjust),\n      vjust = 1.2\n    ) +\n    facet_wrap(~ groupName) +\n    scale_colour_manual(\n      values = c(Validation = \"#377EB8\", Test = \"#FF7F00\"),\n      guide = guide_legend(\n        title = \"Experiment\", reverse = TRUE,\n        override.aes = list(size = 4)\n      )\n    ) +\n    labs(\n      x = \"K factor\", y = \"Goodness of fit (RMSE)\",\n      title = \"Best goodness of fit of Elo and EloBeta models are almost equal\",\n      subtitle = paste0(\n        'Using official matches (without invitational events) gives more ',\n        'stable results.\\n',\n        'All optimal K values from test experiment (with longer \"warm up\") are',\n        ' lower than from validation experiment.'\n      )\n    ) +\n    theme(title = element_text(size = 14), strip.text = element_text(size = 12))\n}\n\nplot_experiment_results(plot_data)\n\n\n\n\nFrom the experiment we can make the following conclusions:\n\nAs it was expected, optimal \\(K\\) values for EloBeta are lower than for Elo.\nUsing official matches (without invitational events) gives more stable results (“Validation” and “Test” results differ less). This shouldn’t be considered as a point that professionals take invitational events not seriously. Probably, this is due to quality of match results from “Championship League”: it has rather unpredictable “3 to win” format and tight schedule.\nChange in RMSE for optimal \\(K\\) is not substantial. That is, RMSE didn’t change drastically after computing optimal \\(K\\) in “Validation” and applying it in “Test” experiment. Moreover, on “official matches” it even decreased.\nAll optimal K values from test experiment (with longer “warm up”) are lower than from validation experiment. This may be the result of longer “warm up” or just a feature of particular data.\nBest goodness of Elo and EloBeta fits are almost the same. Both are stable and below 0.5. Data for “official matches” (as they demonstrate stable behavior) is presented below. As results don’t differ that much, we will round optimal \\(K\\) to a factor of 5: for Elo model it is 30 and for EloBeta - 10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup\nOptimal K\nRMSE\n\n\n\n\nElo, all matches\n24\n0.465\n\n\nElo, official matches\n29\n0.455\n\n\nEloBeta, all matches\n10\n0.462\n\n\nEloBeta, official matches\n11\n0.453\n\n\n\nBased on these results, I am inclined to conclude that Elo model with \\(K = 30\\) and EloBeta model with \\(K = 10\\) can be usefully applied to officially ranked snooker matches. However, EloBeta model accounts for different “\\(n\\) to win” matches, so it should be preferred over Elo."
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#top-16-by-the-end-of-201718",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#top-16-by-the-end-of-201718",
    "title": "Elo and EloBeta models in snooker",
    "section": "Top 16 by the end of 2017/18",
    "text": "Top 16 by the end of 2017/18\n\n\n\n\n\n\nCode for 2017/18 top 16\n\n\n\n\n\n# Helper function\ngather_to_longcr &lt;- function(tbl) {\n  bind_rows(\n    tbl %&gt;% select(-matches(\"2\")) %&gt;% rename_all(funs(gsub(\"1\", \"\", .))),\n    tbl %&gt;% select(-matches(\"1\")) %&gt;% rename_all(funs(gsub(\"2\", \"\", .)))\n  ) %&gt;%\n    arrange(game)\n}\n\n# Extract best \"K factor\" value\nbest_k &lt;- experiment_tbl %&gt;%\n  filter(testType == \"test\", ratingType == \"elobeta\", dataType == \"off\") %&gt;%\n  slice(which.min(goodness)) %&gt;%\n  pull(k)\n\n  #!!! Round to \"pretty\" number as it doesn't affect result that much!!!\nbest_k &lt;- round(best_k / 5) * 5\n\n# Compute ratings at the end of the data\nelobeta_ratings &lt;- rate_iterative(\n  pro_matches_test_off, elobeta_fun_gen(best_k), initial_ratings = 0\n) %&gt;%\n  rename(ratingEloBeta = rating_iterative) %&gt;%\n  arrange(desc(ratingEloBeta)) %&gt;%\n  left_join(\n    y = snooker_players %&gt;% select(id, playerName = name), by = c(player = \"id\")\n  ) %&gt;%\n  mutate(rankEloBeta = order(ratingEloBeta, decreasing = TRUE)) %&gt;%\n  select(player, playerName, ratingEloBeta, rankEloBeta)\n\nelobeta_top16 &lt;- elobeta_ratings %&gt;%\n  filter(rankEloBeta &lt;= 16) %&gt;%\n  mutate(\n    rankChr = formatC(rankEloBeta, width = 2, format = \"d\", flag = \"0\"),\n    ratingEloBeta = round(ratingEloBeta, 1)\n  )\n\n\n\nTop 16 by EloBeta model at the end of 2017/18 season looks like this (official data is also taken from snooker.org site):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlayer\nEloBeta rank\nEloBeta rating\nOfficial rank\nOfficial rating\nEloBeta rank increase\n\n\n\n\nRonnie O’Sullivan\n1\n128.8\n2\n905 750\n1\n\n\nMark J Williams\n2\n123.4\n3\n878 750\n1\n\n\nJohn Higgins\n3\n112.5\n4\n751 525\n1\n\n\nMark Selby\n4\n102.4\n1\n1 315 275\n-3\n\n\nJudd Trump\n5\n92.2\n5\n660 250\n0\n\n\nBarry Hawkins\n6\n83.1\n7\n543 225\n1\n\n\nDing Junhui\n7\n82.8\n6\n590 525\n-1\n\n\nStuart Bingham\n8\n74.3\n13\n324 587\n5\n\n\nRyan Day\n9\n71.9\n16\n303 862\n7\n\n\nNeil Robertson\n10\n70.6\n10\n356 125\n0\n\n\nShaun Murphy\n11\n70.1\n8\n453 875\n-3\n\n\nKyren Wilson\n12\n70.1\n9\n416 250\n-3\n\n\nJack Lisowski\n13\n68.8\n26\n180 862\n13\n\n\nStephen Maguire\n14\n63.7\n17\n291 025\n3\n\n\nMark Allen\n15\n63.7\n12\n332 450\n-3\n\n\nYan Bingtao\n16\n61.6\n23\n215 125\n7\n\n\n\nSome observations:\n\nCurrent official #1 Mark Selby is ranked 3 places lower in EloBeta. This might be a sign that current distribution of prize money doesn’t quite reflect efforts needed to win them (on average).\nMost “underrated” players according to official ranking are Jack Lisowski (astonishing 13 place difference), Ryan Day and Yan Bingtao (both have 7 place difference).\nStuart Bingham is ranked 5 positions higher by EloBeta probably because he didn’t play for six month due to WPBSA ban. His EloBeta rating didn’t change during this period but in official rating he lost points because of its “rolling” nature. This case demonstrates one important differences between two approaches: official system is good to account for “not playing” and EloBeta is good to account for “playing”.\nJudd Trump and Neil Robertson are ranked the same under both methods.\nWith EloBeta model Allister Carter (officially ranked #11), Anthony McGill (#14) and Luca Brecel (#15) are not in top 16. Instead, Jack Lisowski (#26), Yan Bingtao (#23) and Stephen Maguire (#17) are in.\n\nHere is an example of predictions Based on EloBeta model. The probability of 16-th player (r last_top16_player$playerName[1]) win one frame in a match against first player (r first_top16_player$playerName[1]) equals to r example_prob(1). In “4 to win” match it drops to r example_prob(4), in “10 to win” - r example_prob(10) and in World Championship final “18 to win” - r example_prob(18). In my opinion, these numbers might be close to reality."
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#collective-evolution-of-elobeta-ratings",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#collective-evolution-of-elobeta-ratings",
    "title": "Elo and EloBeta models in snooker",
    "section": "Collective evolution of EloBeta ratings",
    "text": "Collective evolution of EloBeta ratings\n\n\n\n\n\n\nCode for rating evolution\n\n\n\n\n\n# Helper data\nseasons_break &lt;- ISOdatetime(2017, 5, 2, 0, 0, 0, tz = \"UTC\")\n\n  # Compute evolution of ratings\nelobeta_history &lt;- pro_matches_test_off %&gt;%\n  add_iterative_ratings(elobeta_fun_gen(best_k), initial_ratings = 0) %&gt;%\n  gather_to_longcr() %&gt;%\n  left_join(y = pro_matches_test_off %&gt;% select(game, endDate), by = \"game\")\n\n  # Generate plot\nplot_all_elobeta_history &lt;- function(history_tbl) {\n  history_tbl %&gt;%\n    mutate(isTop16 = player %in% elobeta_top16$player) %&gt;%\n    ggplot(aes(endDate, ratingAfter, group = player)) +\n      geom_step(data = . %&gt;% filter(!isTop16), colour = \"#C2DF9A\") +\n      geom_step(data = . %&gt;% filter(isTop16), colour = \"#22A01C\") +\n      geom_hline(yintercept = 0, colour = \"#AAAAAA\") +\n      geom_vline(\n        xintercept = seasons_break, linetype = \"dotted\",\n        colour = \"#E41A1C\", size = 1\n      ) +\n      geom_text(\n        x = seasons_break, y = Inf, label = \"End of 2016/17\",\n        colour = \"#E41A1C\", hjust = 1.05, vjust = 1.2\n      ) +\n      scale_x_datetime(date_labels = \"%Y-%m\") +\n      labs(\n        x = NULL, y = \"EloBeta rating\",\n        title = paste0(\n          \"Most of current top 16 established at the end of 2016/17 season\"\n        ),\n        subtitle = paste0(\n          \"Winning of event is well noticable as rapid increase without \",\n          \"descrease at the end.\"\n        )\n      ) +\n      theme(title = element_text(size = 14))\n}\n\nplot_all_elobeta_history(elobeta_history)"
  },
  {
    "objectID": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#evolution-of-elobeta-top-16",
    "href": "blog/2018-07-03-elo-and-elobeta-models-in-snooker.html#evolution-of-elobeta-top-16",
    "title": "Elo and EloBeta models in snooker",
    "section": "Evolution of EloBeta top 16",
    "text": "Evolution of EloBeta top 16\n\n\n\n\n\n\nCode for rating evolution of top 16\n\n\n\n\n\n# Compute plot data\ntop16_rating_evolution &lt;- elobeta_history %&gt;%\n  # Using `inner_join` to leave only players from `elobeta_top16`\n  inner_join(y = elobeta_top16 %&gt;% select(-ratingEloBeta), by = \"player\") %&gt;%\n  # Leave games only from 2017/18 season\n  semi_join(\n    y = pro_matches_test_off %&gt;% filter(season == 2017), by = \"game\"\n  ) %&gt;%\n  mutate(playerLabel = paste(rankChr, playerName))\n\n  # Generate plot\nplot_top16_elobeta_history &lt;- function(elobeta_history) {\n  ggplot(elobeta_history) +\n    geom_step(aes(endDate, ratingAfter, group = player), colour = \"#22A01C\") +\n    geom_hline(yintercept = 0, colour = \"#AAAAAA\") +\n    geom_rug(\n      data = elobeta_top16,\n      mapping = aes(y = ratingEloBeta), sides = \"r\"\n    ) +\n    facet_wrap(~ playerLabel, nrow = 4, ncol = 4) +\n    scale_x_datetime(date_labels = \"%Y-%m\") +\n    labs(\n      x = NULL, y = \"EloBeta rating\",\n      title = \"Rating evolution for EloBeta top 16 (as of 2017/18 end)\",\n      subtitle = paste0(\n        \"Ronnie O'Sullivan and Mark J Williams did very well in 2017/18 \",\n        \"season.\\n\",\n        \"As did Jack Lisowski: rise from negative rating to place 13.\"\n      )\n    ) +\n    theme(title = element_text(size = 14), strip.text = element_text(size = 12))\n}\n\nplot_top16_elobeta_history(top16_rating_evolution)"
  },
  {
    "objectID": "blog/2017-12-04-usage-of-ruler-package.html",
    "href": "blog/2017-12-04-usage-of-ruler-package.html",
    "title": "Usage of ruler package",
    "section": "",
    "text": "My previous post tells a story about design of my ruler package, which presents tools for “… creating data validation pipelines and tidy reports”. This package offers a framework for exploring and validating data frame like objects using dplyr grammar of data manipulation.\nThis post is intended to show some close to reality ruler usage examples. Described methods and approaches reflect package design. Along the way you will learn why Yoda and Jabba the Hutt are “outliers” among core “Star Wars” characters.\nFor more information see README (for relatively brief comprehensive introduction) or vignettes (for more thorough description of package capabilities).\nBeware of a lot of code."
  },
  {
    "objectID": "blog/2017-12-04-usage-of-ruler-package.html#data",
    "href": "blog/2017-12-04-usage-of-ruler-package.html#data",
    "title": "Usage of ruler package",
    "section": "Data",
    "text": "Data\n■ Does starwars have 1) number of rows 1a) more than 50; 1b) less than 60; 2) number of columns 2a) more than 10; 2b) less than 15?\ncheck_data_dims &lt;- data_packs(\n  check_dims = . %&gt;% summarise(\n    nrow_low = nrow(.) &gt;= 50, nrow_up = nrow(.) &lt;= 60,\n    ncol_low = ncol(.) &gt;= 10, ncol_up = ncol(.) &lt;= 15\n  )\n)\n\nstarwars %&gt;%\n  expose(check_data_dims) %&gt;%\n  get_exposure()\n##   Exposure\n##\n## Packs info:\n## # A tibble: 1 x 4\n##         name      type             fun remove_obeyers\n##        &lt;chr&gt;     &lt;chr&gt;          &lt;list&gt;          &lt;lgl&gt;\n## 1 check_dims data_pack &lt;S3: data_pack&gt;           TRUE\n##\n## Tidy data validation report:\n## # A tibble: 1 x 5\n##         pack    rule   var    id value\n##        &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;\n## 1 check_dims nrow_up  .all     0 FALSE\nThe result is interpreted as follows:\n\nData was exposed to one rule pack for data as a whole (data rule pack) named “check_dims”. For it all obeyers (data units which follow specified rule) were removed from validation report.\nCombination of var equals .all and id equals 0 means that data as a whole is validated.\nInput data frame doesn’t obey (because value is equal to FALSE) rule nrow_up from rule pack check_dims.\n\n■ Does starwars have enough rows for characters 1) with blond hair; 2) humans; 3) humans with blond hair?\ncheck_enough_rows &lt;- data_packs(\n  enough_blond = . %&gt;% filter(hair_color == \"blond\") %&gt;%\n    summarise(is_enough = n() &gt; 10),\n  enough_humans = . %&gt;% summarise(\n    is_enough = sum(species == \"Human\", na.rm = TRUE) &gt; 30\n  ),\n  ehough_blond_humans = . %&gt;% filter(\n    hair_color == \"blond\", species == \"Human\"\n  ) %&gt;%\n    summarise(is_enough = n() &gt; 5)\n)\n\nstarwars %&gt;%\n  expose(check_enough_rows) %&gt;%\n  get_exposure()\n##   Exposure\n##\n## Packs info:\n## # A tibble: 3 x 4\n##                  name      type             fun remove_obeyers\n##                 &lt;chr&gt;     &lt;chr&gt;          &lt;list&gt;          &lt;lgl&gt;\n## 1        enough_blond data_pack &lt;S3: data_pack&gt;           TRUE\n## 2       enough_humans data_pack &lt;S3: data_pack&gt;           TRUE\n## 3 ehough_blond_humans data_pack &lt;S3: data_pack&gt;           TRUE\n##\n## Tidy data validation report:\n## # A tibble: 2 x 5\n##                  pack      rule   var    id value\n##                 &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;\n## 1        enough_blond is_enough  .all     0 FALSE\n## 2 ehough_blond_humans is_enough  .all     0 FALSE\nNew information gained from example:\n\nRule specification functions can be supplied with multiple rule packs all of which will be independently used during exposing.\n\n■ Does starwars have enough numeric columns?\ncheck_enough_num_cols &lt;- data_packs(\n  enough_num_cols = . %&gt;% summarise(\n    is_enough = sum(map_lgl(., is.numeric)) &gt; 1\n  )\n)\n\nstarwars %&gt;%\n  expose(check_enough_num_cols) %&gt;%\n  get_report()\n## Tidy data validation report:\n## # A tibble: 0 x 5\n## # ... with 5 variables: pack &lt;chr&gt;, rule &lt;chr&gt;, var &lt;chr&gt;, id &lt;int&gt;,\n## #   value &lt;lgl&gt;\n\nIf no breaker is found get_report() returns tibble with zero rows and usual columns."
  },
  {
    "objectID": "blog/2017-12-04-usage-of-ruler-package.html#group",
    "href": "blog/2017-12-04-usage-of-ruler-package.html#group",
    "title": "Usage of ruler package",
    "section": "Group",
    "text": "Group\n■ Does group defined by hair color and gender have a member from Tatooine?\nhas_hair_gender_tatooine &lt;- group_packs(\n  hair_gender_tatooine = . %&gt;%\n    group_by(hair_color, gender) %&gt;%\n    summarise(has_tatooine = any(homeworld == \"Tatooine\")),\n  .group_vars = c(\"hair_color\", \"gender\"),\n  .group_sep = \"__\"\n)\n\nstarwars %&gt;%\n  expose(has_hair_gender_tatooine) %&gt;%\n  get_report()\n## Tidy data validation report:\n## # A tibble: 12 x 5\n##                   pack         rule                 var    id value\n##                  &lt;chr&gt;        &lt;chr&gt;               &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;\n## 1 hair_gender_tatooine has_tatooine      auburn__female     0 FALSE\n## 2 hair_gender_tatooine has_tatooine  auburn, grey__male     0 FALSE\n## 3 hair_gender_tatooine has_tatooine auburn, white__male     0 FALSE\n## 4 hair_gender_tatooine has_tatooine      blonde__female     0 FALSE\n## 5 hair_gender_tatooine has_tatooine          grey__male     0 FALSE\n## # ... with 7 more rows\n\ngroup_packs() needs grouping columns supplied via .group_vars.\nColumn var of validation report contains levels of grouping columns to identify group. By default their are pasted together with .. To change that supply .group_sep argument.\n12 combinations of hair_color and gender don’t have a character from Tatooine. They are “auburn”-“female”, “auburn, grey”-“male” and so on."
  },
  {
    "objectID": "blog/2017-12-04-usage-of-ruler-package.html#column",
    "href": "blog/2017-12-04-usage-of-ruler-package.html#column",
    "title": "Usage of ruler package",
    "section": "Column",
    "text": "Column\n■ Does every list-column have 1) enough average length; 2) enough unique elements?\ncheck_list_cols &lt;- col_packs(\n  check_list_cols = . %&gt;%\n    summarise_if(\n      is.list,\n      rules(\n        is_enough_mean = mean(map_int(., length)) &gt;= 1,\n        length(unique(unlist(.))) &gt;= 10\n      )\n    )\n)\n\nstarwars %&gt;%\n  expose(check_list_cols) %&gt;%\n  get_report()\n## Tidy data validation report:\n## # A tibble: 3 x 5\n##              pack           rule       var    id value\n##             &lt;chr&gt;          &lt;chr&gt;     &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;\n## 1 check_list_cols is_enough_mean  vehicles     0 FALSE\n## 2 check_list_cols is_enough_mean starships     0 FALSE\n## 3 check_list_cols        rule..2     films     0 FALSE\n\nTo specify rule functions inside dplyr’s scoped verbs use ruler::rules(). It powers correct output interpretation during exposing process and imputes missing rule names based on the present rules in current rule pack.\nColumns vehicles and starships don’t have enough average length and column films doesn’t have enough unique elements.\n\n■ Are all values of column birth_year non-NA?\nstarwars %&gt;%\n  expose(\n    col_packs(\n      . %&gt;% summarise_at(\n        vars(birth_year = \"birth_year\"),\n        rules(all_present = all(!is.na(.)))\n      )\n    )\n  ) %&gt;%\n  get_report()\n## Tidy data validation report:\n## # A tibble: 1 x 5\n##          pack        rule        var    id value\n##         &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;\n## 1 col_pack..1 all_present birth_year     0 FALSE\n\nTo correctly validate one column with scoped dplyr verb it should be a named argument inside vars. It is needed for correct interpretation of rule pack output."
  },
  {
    "objectID": "blog/2017-12-04-usage-of-ruler-package.html#row",
    "href": "blog/2017-12-04-usage-of-ruler-package.html#row",
    "title": "Usage of ruler package",
    "section": "Row",
    "text": "Row\n■ Has character appeared in enough films? As character is defined by row, this is a row pack.\nhas_enough_films &lt;- row_packs(\n  enough_films = . %&gt;% transmute(is_enough = map_int(films, length) &gt;= 3)\n)\n\nstarwars %&gt;%\n  expose(has_enough_films) %&gt;%\n  get_report() %&gt;%\n  left_join(y = starwars %&gt;% transmute(id = 1:n(), name),\n            by = \"id\") %&gt;%\n  print(.validate = FALSE)\n## Tidy data validation report:\n## # A tibble: 64 x 6\n##           pack      rule   var    id value              name\n##          &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;             &lt;chr&gt;\n## 1 enough_films is_enough  .all     8 FALSE             R5-D4\n## 2 enough_films is_enough  .all     9 FALSE Biggs Darklighter\n## 3 enough_films is_enough  .all    12 FALSE    Wilhuff Tarkin\n## 4 enough_films is_enough  .all    15 FALSE            Greedo\n## 5 enough_films is_enough  .all    18 FALSE  Jek Tono Porkins\n## # ... with 59 more rows\n\n64 characters haven’t appeared in 3 films or more. Those are characters described in starwars in rows 8, 9, etc. (counting based on input data).\n\n■ Is character with height less than 100 a droid?\nis_short_droid &lt;- row_packs(\n  is_short_droid = . %&gt;% filter(height &lt; 100) %&gt;%\n    transmute(is_droid = species == \"Droid\")\n)\n\nstarwars %&gt;%\n  expose(is_short_droid) %&gt;%\n  get_report() %&gt;%\n  left_join(y = starwars %&gt;% transmute(id = 1:n(), name, height),\n            by = \"id\") %&gt;%\n  print(.validate = FALSE)\n## Tidy data validation report:\n## # A tibble: 5 x 7\n##             pack     rule   var    id value                  name height\n##            &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;                 &lt;chr&gt;  &lt;int&gt;\n## 1 is_short_droid is_droid  .all    19 FALSE                  Yoda     66\n## 2 is_short_droid is_droid  .all    29 FALSE Wicket Systri Warrick     88\n## 3 is_short_droid is_droid  .all    45 FALSE              Dud Bolt     94\n## 4 is_short_droid is_droid  .all    72 FALSE         Ratts Tyerell     79\n## 5 is_short_droid is_droid  .all    73    NA                R4-P17     96\n\nOne can expose only subset of rows by using filter or slice. The value of id column in result will reflect row number in the original input data frame. This feature is powered by keyholder package. In order to use it, rule pack should be created using its supported functions.\nvalue equal to NA is treated as rule breaker.\n5 “not tall” characters are not droids."
  },
  {
    "objectID": "blog/2017-12-04-usage-of-ruler-package.html#cell",
    "href": "blog/2017-12-04-usage-of-ruler-package.html#cell",
    "title": "Usage of ruler package",
    "section": "Cell",
    "text": "Cell\n■ Is non-NA numeric cell not an outlier based on z-score? This is a bit tricky. To present outliers as rule breakers one should ask whether cell is not outlier.\nz_score &lt;- function(x, ...) {abs(x - mean(x, ...)) / sd(x, ...)}\n\ncell_isnt_outlier &lt;- cell_packs(\n  dbl_not_outlier = . %&gt;%\n    transmute_if(\n      is.numeric,\n      rules(isnt_out = z_score(., na.rm = TRUE) &lt; 3 | is.na(.))\n    )\n)\n\nstarwars %&gt;%\n  expose(cell_isnt_outlier) %&gt;%\n  get_report() %&gt;%\n  left_join(y = starwars %&gt;% transmute(id = 1:n(), name),\n            by = \"id\") %&gt;%\n  print(.validate = FALSE)\n## Tidy data validation report:\n## # A tibble: 4 x 6\n##              pack     rule        var    id value                  name\n##             &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt; &lt;int&gt; &lt;lgl&gt;                 &lt;chr&gt;\n## 1 dbl_not_outlier isnt_out     height    19 FALSE                  Yoda\n## 2 dbl_not_outlier isnt_out       mass    16 FALSE Jabba Desilijic Tiure\n## 3 dbl_not_outlier isnt_out birth_year    16 FALSE Jabba Desilijic Tiure\n## 4 dbl_not_outlier isnt_out birth_year    19 FALSE                  Yoda\n\n4 non-NA numeric cells appear to be an outlier within their column."
  },
  {
    "objectID": "blog/2024-08-01-remove-padding-around-neovim-instance.html",
    "href": "blog/2024-08-01-remove-padding-around-neovim-instance.html",
    "title": "You can remove padding around Neovim instance with this one simple trick…",
    "section": "",
    "text": "Originally posted on Reddit\n\n\n\nLeft: with \"frame\" from terminal emulator; Right: without that \"frame\"\n\n\n(Sorry for a slightly clickbait-y title. Always wanted to use one of those :) )\nIf you have different background color in your terminal emulator and Neovim, then chances are that you experience this weird “frame” around your Neovim instance. Like the one shown in the left part of the picture.\nThis is because CLI programs occupy screen estate based on the cell grid with cells having same width and height. If pixel dimension(s) of terminal emulator’s window are not multiple of cell pixel dimension(s), there is a gap between edge(s) of rendered CLI program and window edge(s).\nUsual answers to this issue are:\n\nUse same background color in Neovim and terminal emulator. Works, but is too restrictive.\nAdjust window dimensions or DPI. Works, but is too restrictive.\nUse GUI (like Neovide). Works, but… you get the idea.\n\n\nAs it turns out, this can be solved by keeping terminal background’s color in sync with Neovim’s background color. This is possible thanks to a dark magic called “Operating System Commands XTerm Control Sequences” or OSC control sequences for short. In particular, OSC 11 and OSC 111, which your terminal should support (most modern feature rich ones do: Kitty, WezTerm, Alacritty, etc.).\nJust add the following snippet to your ‘init.lua’ (credit to u/gpanders from this comment):\nvim.api.nvim_create_autocmd({ \"UIEnter\", \"ColorScheme\" }, {\n  callback = function()\n    local normal = vim.api.nvim_get_hl(0, { name = \"Normal\" })\n    if not normal.bg then return end\n    io.write(string.format(\"\\027]11;#%06x\\027\\\\\", normal.bg))\n  end,\n})\n\nvim.api.nvim_create_autocmd(\"UILeave\", {\n  callback = function() io.write(\"\\027]111\\027\\\\\") end,\n})\nAnd that’s it. It synchronizes on every enter/exit Neovim instance and after loading new color scheme. And it even works with &lt;C-z&gt; and later fg! Couple of caveats, though:\n\nMake sure to have this executed before you load color scheme. Otherwise there will be no event for it to sync. Alternatively, add an explicit call to the first callback function and it should work as is.\nIt will not sync if you manually set Normal highlight group. It must be followed by the ColorScheme event.\n\n\nAlso, if you want a slightly more robust, maintained, and tested version, there is now a new setup_termbg_sync() in ‘mini.misc’ module of ‘mini.nvim’. It also checks if OSC 11 is supported by terminal emulator, uses only it without OSC 111, and synchronizes immediately."
  },
  {
    "objectID": "blog/2022-09-09-random-base16-color-scheme.html",
    "href": "blog/2022-09-09-random-base16-color-scheme.html",
    "title": "Create and apply randomly generated Base16 color scheme",
    "section": "",
    "text": "Originally posted on Reddit\n\nHello, Neovim users!\nI decided to share a bit silly, but fun way to choose a color scheme you like. Maybe even have this in your startup config, if you feel particularly adventurous :)\nIt uses mini.base16 module of mini.nvim for both palette generation and color scheme application. Granted, results are not that different from one another (mainly because they share same highlight group definitions and idea behind making distinctive accent colors), but vary quite a bit.\nHere is a gist with code. It has the instructions of how to use it."
  },
  {
    "objectID": "blog/2019-11-11-statistical-uncertainty-with-pdqr.html",
    "href": "blog/2019-11-11-statistical-uncertainty-with-pdqr.html",
    "title": "Statistical uncertainty with R and pdqr",
    "section": "",
    "text": "I am glad to announce that my latest, long written R package ‘pdqr’ is accepted to CRAN. It provides tools for creating, transforming and summarizing custom random variables with distribution functions (as base R ‘p*()’, ‘d*()’, ‘q*()’, and ‘r*()’ functions). You can read a brief overview in one of my previous posts.\nWe will need the following setup:\nlibrary(pdqr)\nlibrary(magrittr)\n\n# For the sake of reproducibility\nset.seed(20191111)"
  },
  {
    "objectID": "blog/2019-11-11-statistical-uncertainty-with-pdqr.html#general-description",
    "href": "blog/2019-11-11-statistical-uncertainty-with-pdqr.html#general-description",
    "title": "Statistical uncertainty with R and pdqr",
    "section": "General description",
    "text": "General description\nStatistical estimation usually has the following setup. There is a sample (observed, usually randomly chosen, set of values of measurable quantities) from some general population (whole set of values of the same measurable quantities). We need to make conclusions about the general population based on a sample. This is done by computing summary values (called statistics) of a sample, and making reasonable assumptions (with process usually called inference) about how these values are close to values that potentially can be computed based on whole general population. Thus, summary value based on a sample (sample statistic) is an estimation of potential summary value based on a general population (true value).\nHow can we make inference about quality of this estimation? This question itself describes statistical uncertainty and can be unfolded into a deep philosophical question about probability, nature, and life in general. Basically, the answer depends on assumptions about the relation between sample, general population, and statistic.\nFor me, the most beautiful inferential approach is bootstrap. It has the following key assumption: process of producing samples from general population can be simulated by doing random sampling with replacement from present sample. In other words, we agree (and practice often agrees with us) that random sampling with replacement from current sample (sometimes called bootstrap sampling) has a “close enough” behavior to the “true nature” of how initial sample was created. Numerical estimation of “how close” is also an interesting problem, but it is a more complicated topic."
  },
  {
    "objectID": "blog/2019-11-11-statistical-uncertainty-with-pdqr.html#computation-with-pdqr",
    "href": "blog/2019-11-11-statistical-uncertainty-with-pdqr.html#computation-with-pdqr",
    "title": "Statistical uncertainty with R and pdqr",
    "section": "Computation with pdqr",
    "text": "Computation with pdqr\nNatural way of computing bootstrap quantities is straightforward: produce \\(B\\) random bootstrap samples, for each one compute value of statistic under question, and summarize sample of statistic values with numerical quantity (usually with some center and spread values).\nThere are many ways of performing bootstrap in R, like boot::boot(), rsample::bootstraps(), and others. In turn, ‘pdqr’ offers its own way of describing and doing bootstrap inference for one-dimensional numeric sample(s):\n\nCreate a random variable (in the form of pdqr-function with new_*() family) based on initial sample. This random variable already describes a general population with “bootstrap assumption”: it will produce values based on initial sample. Type of this variable determines the type of bootstrap:\n\nType \"discrete\" describes ordinary bootstrap. Only values from initial sample can be produced.\nType \"continuous\" describes smooth bootstrap. Initial sample is smoothed by doing kernel density estimation with density() function and random variable produces values from distribution with that density.\n\nTransform created random variable into one that produces statistic values obtained with bootstrap. Sometimes this can be done with basic mathematical operations like +, min, etc. But usually this is done with form_estimate() function: it creates many (10000 by default) bootstrap samples, for each computes statistic value, and creates its own random variable in the form of pdqr-function (class and type are preserved from supplied random variable, but this can be adjusted). It needs at least three arguments:\n\nf: pdqr-function representing random variable. In described setup it is created as a result of “Create” step.\nstat: statistic function that accepts numeric vector of size sample_size and returns single numeric or logical output.\nsample_size: Size of a sample that each bootstrap draw should produce. In described setup it should be equal to number of elements in initial sample.\n\nSummarize distribution of statistic. Usually this is point measure of center or spread, or interval.\n\n\nExample 1: single numerical estimate\nMean value of ‘mpg’ variable in mtcars dataset is 20.090625. However, having in mind statistical uncertainty, we can ask how precise is this estimation? This can, and should, be reformulated in the following question: if we repeat sampling sets of 32 cars from general population of all cars, how close their ‘mpg’ sample means will be to each other? This can be answered by computing bootstrap distribution of sample means (pipe %&gt;% function from ‘magrittr’ package is used to simplify notation):\n# Using ordinary bootstrap\nd_mpg_dis_mean &lt;- mtcars$mpg %&gt;% \n  new_d(type = \"discrete\") %&gt;% \n  form_estimate(stat = mean, sample_size = nrow(mtcars))\n\n  # Spread of this bootstrap distribution describes the precision of estimation:\n  # bigger values indicate lower precision\nsumm_sd(d_mpg_dis_mean)\n## [1] 1.04067\n\n  # This discrete distribution has the following d-function\nplot(\n  d_mpg_dis_mean,\n  main = \"Ordinary bootstrap distribution of 'mpg' sample mean\"\n)\n\nIf modeling assumption about continuous nature of ‘mpg’ variable is reasonable (which it seems so), you can use “smooth bootstrap” by changing type of initial pdqr-function:\n# Using smooth bootstrap with `type = \"continuous\"`\nd_mpg_con_mean &lt;- mtcars$mpg %&gt;% \n  new_d(type = \"continuous\") %&gt;% \n  form_estimate(stat = mean, sample_size = nrow(mtcars))\n\n  # Spread is higher in this case because kernel density estimation with\n  # `density()` function extends support during creation of pdqr-function on the\n  # bootstrap step\nsumm_sd(d_mpg_con_mean)\n## [1] 1.153957\n\nplot(\n  d_mpg_con_mean,\n  main = \"Smooth bootstrap distribution of 'mpg' sample mean\"\n)\n\nOne can also do ordinary bootstrap but represent bootstrap distribution of sample mean with continuous random variable:\n# Using ordinary bootstrap, but treating sample mean as continuous\nd_mpg_con_mean_2 &lt;- mtcars$mpg %&gt;% \n  new_d(type = \"discrete\") %&gt;% \n  form_estimate(\n    stat = mean, sample_size = nrow(mtcars),\n    # Create continuous pdqr-function from bootstrap sample means\n    args_new = list(type = \"continuous\")\n  )\n\nsumm_sd(d_mpg_con_mean_2)\n## [1] 1.063524\n\nplot(\n  d_mpg_con_mean_2,\n  main = \"Ordinary bootstrap distribution of 'mpg' continuous sample mean\"\n)\n\nIn this case, sample mean has standard deviation from 1.04067 to 1.1539572 (depends on assumptions about data generating process).\n\n\nExample 2: single logical estimate\nShare of 4-cylinder cars in mtcars is equal to 0.34375. However, it might happen that we don’t care about actual value, but only if it is bigger 0.3 or not. In present data it is bigger, but how sure we can be about that? In other words: if we repeat sampling sets of 32 cars from general population of all cars, which part of it will have share of 4-cylinder cars bigger than 0.3?. Here is the way of computing that with ‘pdqr’:\n# If statistic returns logical value (indicating presence of some feature in\n# sample), output estimate pdqr-function is \"boolean\": \"discrete\" type function\n# with elements being exactly 0 (indicating `FALSE`) and 1 (indicating `TRUE`).\nd_cyl_lgl &lt;- mtcars$cyl %&gt;% \n  new_d(type = \"discrete\") %&gt;% \n  form_estimate(\n    stat = function(x) {mean(x == 4) &gt; 0.3},\n    sample_size = nrow(mtcars)\n  )\n\nd_cyl_lgl\n## Probability mass function of discrete type\n## Support: [0, 1] (2 elements, probability of 1: 0.7113)\n\n  # To extract certain probability from boolean pdqr-function, use\n  # `summ_prob_*()` functions\nsumm_prob_true(d_cyl_lgl)\n## [1] 0.7113\nsumm_prob_false(d_cyl_lgl)\n## [1] 0.2887\nIn this case, estimated probability that share of 4-cylinder cars in general population is more than 0.3 is 0.7113.\n\n\nExample 3: comparison of estimates\nIn mtcars there are 19 cars with automatic transmission (‘am’ variable is 0) and 13 with manual (‘am’ variable is 1). We might be concerned with the following question: are cars with automatic transmission heavier than cars with manual transmission? This is an example of question where reformulating is very crucial, because it leads to completely different methodologies. Basically, it is all about dealing with statistical uncertainty and how to measure that one numerical set is bigger than the other.\nFirst, rather verbose, way of expanding this question is this one: if we randomly choose a car with automatic transmission (uniformly on set of all cars with automatic transmission) and a car with manual (uniformly on set of all cars with manual transmission), what is the probability that weight of the first one is bigger than the second one?. With ‘pdqr’ this can be computed straightforwardly by comparing two random variables (which is implemented exactly like the question above; read more here):\n# Seems reasonable to treat weight as continuous random variable. Note that this\n# means use of kernel density estimation, which can lead to random variable that\n# returns negative values. As weight can be only positive, it is a good idea to\n# ensure that. Package 'pdqr' has `form_resupport()` function for that.\nd_wt_am0 &lt;- mtcars$wt[mtcars$am == 0] %&gt;%\n  new_d(type = \"continuous\") %&gt;% \n  # Ensure that returned values are only positive\n  form_resupport(c(0, NA))\nd_wt_am1 &lt;- mtcars$wt[mtcars$am == 1] %&gt;%\n  new_d(type = \"continuous\") %&gt;% \n  form_resupport(c(0, NA))\n\n# Comparing two pdqr-functions with `&gt;=` results into boolean pdqr-function\nsumm_prob_true(d_wt_am0 &gt;= d_wt_am1)\n## [1] 0.9209063\nSo in this case the answer is that probability of “automatic” cars being heavier than “manual” ones is around 0.921.\nSecond way of understanding question about comparing is the following: is average weight of “automatic” cars bigger than of “manual”?. This type of questions are more widespread in statistical practice. Having to deal with statistical uncertainty, this should be reformulated: if we repeat sampling (in parallel pairs) sets of 19 “automatic” cars and of 13 “manual” cars, which part of the set pairs will have mean weight of “automatic” cars bigger? This question implies creating bootstrap distribution of sample means for “automatic” and “manual” cars with the following comparing:\nd_wt_am0_mean &lt;- d_wt_am0 %&gt;% \n  form_estimate(stat = mean, sample_size = sum(mtcars$am == 0)) %&gt;% \n  # Ensure \"positiveness\" of random variable\n  form_resupport(c(0, NA))\nd_wt_am1_mean &lt;- d_wt_am1 %&gt;% \n  form_estimate(stat = mean, sample_size = sum(mtcars$am == 1)) %&gt;% \n  form_resupport(c(0, NA))\n\n# Comparing two random variables representing sample means\nsumm_prob_true(d_wt_am0_mean &gt;= d_wt_am1_mean)\n## [1] 1\nSo in this case the answer is that probability of “automatic” cars being heavier than “manual” ones is 1.\nComputed results can have decisively different outcomes. If researcher sets a standard 0.95 rule, first variant would imply that conclusion ‘“automatic” cars are heavier than “manual”’ isn’t significant, while the second would imply otherwise."
  },
  {
    "objectID": "blog/2022-12-29-you-dont-need-vimrooter-usually.html",
    "href": "blog/2022-12-29-you-dont-need-vimrooter-usually.html",
    "title": "You don’t need ‘vim-rooter’ (usually) or How to set up smart autochange of current directory",
    "section": "",
    "text": "Originally posted on Reddit\nHello, Neovim users!\nThe airblade/vim-rooter plugin is an essential part of my Neovim workflow. It automatically changes current directory (:h current-directory) for every buffer to a more natural one (like to path of its Git repository). For me this has the following benefits:\n\nMost file explorers open closer to the file of current buffer and not inside directory in which Neovim was started.\nSearching files with Telescope is more natural.\nBuilt-in terminal opens in directory I usually want it to open: in root based on current buffer file.\n\nHowever, starting from Neovim 0.8 ‘vim-rooter’ is (almost) obsolete. Using an amazing vim.fs module you can set up basic autochange of current directory using these lines of code (call them somewhere during your Neovim startup):\n-- Array of file names indicating root directory. Modify to your liking.\nlocal root_names = { '.git', 'Makefile' }\n\n-- Cache to use for speed up (at cost of possibly outdated results)\nlocal root_cache = {}\n\nlocal set_root = function()\n  -- Get directory path to start search from\n  local path = vim.api.nvim_buf_get_name(0)\n  if path == '' then return end\n  path = vim.fs.dirname(path)\n\n  -- Try cache and resort to searching upward for root directory\n  local root = root_cache[path]\n  if root == nil then\n    local root_file = vim.fs.find(root_names, { path = path, upward = true })[1]\n    if root_file == nil then return end\n    root = vim.fs.dirname(root_file)\n    root_cache[path] = root\n  end\n\n  -- Set current directory\n  vim.fn.chdir(root)\nend\n\nlocal root_augroup = vim.api.nvim_create_augroup('MyAutoRoot', {})\nvim.api.nvim_create_autocmd('BufEnter', { group = root_augroup, callback = set_root })\nOf course this implementation has certain limitations: - The main one is a lack of ability to use glob patterns to find root directory. However, although solvable, I think the most common use case is to define root based on some pre-determined set of actual file names, so this shouldn’t be a big issue. - It caches results to speed up computations with a downside of result possibly being outdated. Usually not an issue, though. Remove all cache related code to not do that.\nInitially I planned to make a separate ‘mini.root’ module, but realized that it would mostly be a reimplementation of vim.fs. So instead I decided to add setup_auto_root() function to mini.misc. If you want a more tested and documented solution, check it out and tell me what you think. Thanks!"
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "",
    "text": "During the process of data analysis one of the most crucial steps is to identify and account for outliers, observations that have essentially different nature than most other observations. Their presence can lead to untrustworthy conclusions. The most complicated part of this task is to define a notion of “outlier”. After that, it is straightforward to identify them based on given data.\nThere are many techniques developed for outlier detection. Majority of them deal with numerical data. This post will describe the most basic ones with their application using dplyr and ruler packages.\nAfter reading this post you will know:\n\nMost basic outlier detection techniques.\nA way to implement them using dplyr and ruler.\nA way to combine their results in order to obtain a new outlier detection method.\nA way to discover notion of “diamond quality” without prior knowledge of this topic (as a happy consequence of previous point)."
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#z-score",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#z-score",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Z-score",
    "text": "Z-score\nZ-score, also called a standard score, of an observation is [broadly speaking] a distance from the population center measured in number of normalization units. The default choice for center is sample mean and for normalization unit is standard deviation.\n⬛ Observation is not an outlier based on z-score if its absolute value of default z-score is lower then some threshold (popular choice is 3).\nHere is the function for identifying non-outliers based on z-score:\nisnt_out_z &lt;- function(x, thres = 3, na.rm = TRUE) {\n  abs(x - mean(x, na.rm = na.rm)) &lt;= thres * sd(x, na.rm = na.rm)\n}\nIt takes a numeric vector as input and returns logical vector of the same length indicating whether input value is a non-outlier."
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#z-score-with-mad",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#z-score-with-mad",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Z-score with MAD",
    "text": "Z-score with MAD\nMedian Absolute Deviation is a robust normalization unit based on median as a population center. In order to use MAD “as a consistent estimator for the estimation of the standard deviation” one takes its value multiplied by a factor. This way base R function mad is implemented.\n⬛ Observation is not an outlier based on MAD if its absolute value of z-score with median as center and MAD as normalization unit is lower then some threshold (popular choice is 3).\nisnt_out_mad &lt;- function(x, thres = 3, na.rm = TRUE) {\n  abs(x - median(x, na.rm = na.rm)) &lt;= thres * mad(x, na.rm = na.rm)\n}"
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#tukeys-fences",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#tukeys-fences",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Tukey’s fences",
    "text": "Tukey’s fences\nTukey’s fences is a technique used in box plots. The non-outlier range is defined with \\([Q_1 - k(Q_3 - Q_1),~ Q_3 + k(Q_3 - Q_1)]\\), where \\(Q_1\\) and \\(Q_3\\) are the lower and upper quartiles respectively, \\(k\\) - some nonnegative constant (popular choice is 1.5).\n⬛ Observation is not an outlier based on Tukey’s fences if its value lies in non-outlier range.\nisnt_out_tukey &lt;- function(x, k = 1.5, na.rm = TRUE) {\n  quar &lt;- quantile(x, probs = c(0.25, 0.75), na.rm = na.rm)\n  iqr &lt;- diff(quar)\n\n  (quar[1] - k * iqr &lt;= x) & (x &lt;= quar[2] + k * iqr)\n}"
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#mahalanobis-distance",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#mahalanobis-distance",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Mahalanobis distance",
    "text": "Mahalanobis distance\nAll previous approaches were created for univariate numerical data. To detect outliers in multivariate case one can use Mahalanobis distance to reduce to univariate case and then apply known techniques.\n⬛ Observation is not an outlier based on Mahalanobis distance if its distance is not an outlier.\nmaha_dist &lt;- . %&gt;% select_if(is.numeric) %&gt;%\n    mahalanobis(center = colMeans(.), cov = cov(.))\n\nisnt_out_maha &lt;- function(tbl, isnt_out_f, ...) {\n  tbl %&gt;% maha_dist() %&gt;% isnt_out_f(...)\n}\nThis function takes as input a data frame of interest (with possible non-numeric columns which are ignored) and function performing univariate outlier detection. It returns a logical vector of the same length as number of rows in input data frame.\nTo read more about practical usefulness of Mahalanobis distance in detecting outliers go to Steffen’s very helpful post."
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#definition-of-non-outlier-row",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#definition-of-non-outlier-row",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Definition of non-outlier row",
    "text": "Definition of non-outlier row\nPackage ruler, based on dplyr grammar of data manipulation, offers tools for validating the following data units: data as a whole, group [of rows] as a whole, column as a whole, row as a whole, cell. Our primary interest is row as a whole. However, using this framework, we can construct several approaches for definition of the non-outlier row:\n\nRow is not an outlier based on some column if it doesn’t contain outlier (computed based on the target column) on the intersection with that column. In other words, first a univariate outlier detection is performed based solely on data from target column and then all rows containing non-outliers are named non-outlier rows.\nRow is not an outlier based on Mahalanobis distance if its distance (computed based on the selected numeric columns) is not an outlier.\nRow is not an outlier based on grouping if it is a part of a non-outlier group [of rows]. A group [of rows] is not an outlier if its summary value is not an outlier among summary values of other groups.\n\nNote that all listed approached depend on the choice of the univariate outlier detection method. We will use all three previously listed univariate techniques.\nisnt_out_funs &lt;- funs(\n  z = isnt_out_z,\n  mad = isnt_out_mad,\n  tukey = isnt_out_tukey\n)"
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#implementation",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#implementation",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Implementation",
    "text": "Implementation\nIn ruler framework rules are defined in packs (to learn more go to ruler README and vignettes).\n\nColumn based non-outlier rows\nFor diamonds dataset rules for column based non-outlier rows can be defined based on 7 numeric columns and 3 presented univariate detection methods. There is a convenient way of computing all them at once using scoped variant of dplyr::transmute():\ndiamonds %&gt;% transmute_if(is.numeric, isnt_out_funs)\n## # A tibble: 53,940 x 21\n##   carat_z depth_z table_z price_z   x_z   y_z   z_z carat_mad depth_mad\n##     &lt;lgl&gt;   &lt;lgl&gt;   &lt;lgl&gt;   &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;     &lt;lgl&gt;     &lt;lgl&gt;\n## 1    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE      TRUE\n## 2    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE      TRUE\n## 3    TRUE   FALSE   FALSE    TRUE  TRUE  TRUE  TRUE      TRUE     FALSE\n## 4    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE      TRUE\n## 5    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE      TRUE\n## # ... with 5.394e+04 more rows, and 12 more variables: table_mad &lt;lgl&gt;,\n## #   price_mad &lt;lgl&gt;, x_mad &lt;lgl&gt;, y_mad &lt;lgl&gt;, z_mad &lt;lgl&gt;,\n## #   carat_tukey &lt;lgl&gt;, depth_tukey &lt;lgl&gt;, table_tukey &lt;lgl&gt;,\n## #   price_tukey &lt;lgl&gt;, x_tukey &lt;lgl&gt;, y_tukey &lt;lgl&gt;, z_tukey &lt;lgl&gt;\nThe result has outputs for 21 methods. Their names are of the form &lt;column name&gt;_&lt;method name&gt;. So the name ‘carat_z’ is interpreted as result of univariate method with name ‘z’ for column with name ‘carat’.\n\n\nMahalanobis based non-outlier rows\nTo define non-outlier rows based on Mahalanobis distance one should apply univariate method for distances computed for some subset of numeric columns. To simplify a little bit, we will one “subset” with all numeric columns and all listed methods:\ndiamonds %&gt;%\n  transmute(maha = maha_dist(.)) %&gt;%\n  transmute_at(vars(maha = maha), isnt_out_funs)\n## # A tibble: 53,940 x 3\n##   maha_z maha_mad maha_tukey\n##    &lt;lgl&gt;    &lt;lgl&gt;      &lt;lgl&gt;\n## 1   TRUE     TRUE       TRUE\n## 2   TRUE    FALSE      FALSE\n## 3   TRUE    FALSE      FALSE\n## 4   TRUE     TRUE       TRUE\n## 5   TRUE     TRUE       TRUE\n## # ... with 5.394e+04 more rows\nThe result has outputs for 3 methods. Their names are considered as method names. Note that with this approach outlier rows are not only the ones far from multivariate center, but also the ones that are unnaturally close to it.\n\n\nGroup based non-outlier rows\nDefinition of non-outlier rows based on grouping depends on group summary function and univariate outlier detection method. As grouping column we will choose all non-numeric columns (cut, color and clarity) united into one called group (for later easier imputation of non-outlier rows). As reasonable summary functions we will choose mean value of some numeric column (total of 7 functions):\ndata_tbl &lt;- diamonds %&gt;%\n  unite(col = \"group\", cut, color, clarity)\n\ncompute_group_non_outliers &lt;- . %&gt;%\n  # Compute per group mean values of columns\n  group_by(group) %&gt;%\n  summarise_if(is.numeric, mean) %&gt;%\n  ungroup() %&gt;%\n  # Detect outliers among groups\n  mutate_if(is.numeric, isnt_out_funs) %&gt;%\n  # Remove unnecessary columns\n  select_if(Negate(is.numeric))\n\ndata_tbl %&gt;% compute_group_non_outliers()\n## # A tibble: 276 x 22\n##        group carat_z depth_z table_z price_z   x_z   y_z   z_z carat_mad\n##        &lt;chr&gt;   &lt;lgl&gt;   &lt;lgl&gt;   &lt;lgl&gt;   &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;     &lt;lgl&gt;\n## 1  Fair_D_I1   FALSE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE     FALSE\n## 2  Fair_D_IF    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE\n## 3 Fair_D_SI1    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE\n## 4 Fair_D_SI2    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE\n## 5 Fair_D_VS1    TRUE    TRUE    TRUE    TRUE  TRUE  TRUE  TRUE      TRUE\n## # ... with 271 more rows, and 13 more variables: depth_mad &lt;lgl&gt;,\n## #   table_mad &lt;lgl&gt;, price_mad &lt;lgl&gt;, x_mad &lt;lgl&gt;, y_mad &lt;lgl&gt;,\n## #   z_mad &lt;lgl&gt;, carat_tukey &lt;lgl&gt;, depth_tukey &lt;lgl&gt;, table_tukey &lt;lgl&gt;,\n## #   price_tukey &lt;lgl&gt;, x_tukey &lt;lgl&gt;, y_tukey &lt;lgl&gt;, z_tukey &lt;lgl&gt;\nThe result has outputs for 21 methods applied to the 276 groups. Their names are of the form &lt;column name for summary function&gt;_&lt;method name&gt;. So the name ‘carat_z’ is interpreted as result of method ‘z’ for summary function equal to mean value of ‘carat’ column. Column group defines names of the groupings."
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#exposure",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#exposure",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Exposure",
    "text": "Exposure\nColumn and Mahalanobis based definition of non-outlier rows can be expressed with row packs and group based - as group packs.\nrow_packs_isnt_out &lt;- row_packs(\n  # Non-outliers based on some column\n  column = . %&gt;% transmute_if(is.numeric, isnt_out_funs),\n  # Non-outliers based on Mahalanobis distance\n  maha = . %&gt;% transmute(maha = maha_dist(.)) %&gt;%\n    transmute_at(vars(maha = maha), isnt_out_funs)\n)\n\ngroup_packs_isnt_out &lt;- group_packs(\n  # Non-outliers based on grouping\n  group = compute_group_non_outliers,\n  .group_vars = \"group\"\n)\nApplication of all those packs is called exposing process. The result is an exposure from which we can extract tidy data validation report using get_report.\n# Don't remove obeyers to compute total number of applied rules\nfull_report &lt;- data_tbl %&gt;%\n  expose(row_packs_isnt_out, group_packs_isnt_out,\n         .remove_obeyers = FALSE) %&gt;%\n  get_report()\n\nused_rules &lt;- full_report %&gt;%\n  distinct(pack, rule)\n\nbreaker_report &lt;- full_report %&gt;%\n  filter(!(value %in% TRUE))\nused_rules contains data about all definitions of non-outlier rows applied to data. They are encoded with combination of columns pack and rule.\nbreaker_report contains data about data units that break certain rules. Packs column and maha has actual row numbers of data_tbl listed in id column of report (for rows which should be considered as outliers).\nOn the other hand, pack group defines group pack and is represented in breaker_report with id 0. To obtain row outliers based on grouping we need to expand those rows with information about rows in the data that belong to those groups. This can be done using dplyr::left_join():\ngroup_breakers &lt;- breaker_report %&gt;%\n  # Filter group packs\n  filter(pack == \"group\") %&gt;%\n  # Expand rows by matching group with its rows\n  select(-id) %&gt;%\n  left_join(\n    y = data_tbl %&gt;% transmute(var = group, id = 1:n()),\n    by = \"var\"\n  ) %&gt;%\n  select(pack, rule, var, id, value)\n\noutliers &lt;- bind_rows(\n  breaker_report %&gt;% filter(pack != \"group\"),\n  group_breakers\n) %&gt;%\n  select(pack, rule, id)\n\n# Not all group based definitions resulted with outliers\noutliers %&gt;%\n  count(pack, rule) %&gt;%\n  filter(pack == \"group\") %&gt;%\n  print(n = Inf)\n## # A tibble: 13 x 3\n##     pack        rule     n\n##    &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;\n##  1 group   carat_mad    37\n##  2 group carat_tukey    37\n##  3 group     carat_z    29\n##  4 group   depth_mad  1093\n##  5 group depth_tukey  1016\n##  6 group     depth_z   156\n##  7 group   price_mad   209\n##  8 group price_tukey  1146\n##  9 group     price_z    44\n## 10 group   table_mad   920\n## 11 group table_tukey     8\n## 12 group     table_z     7\n## 13 group         z_z    23\nTibble outliers contains data about outlier rows. Combination of columns pack and rule defines non-outlier/outlier definition approach and column id defines row number of input data frame that should be considered an outlier based on the definition.\nDefinitions with most outliers are as follows:\noutliers %&gt;%\n  count(pack, rule, sort = TRUE)\n## # A tibble: 37 x 3\n##     pack        rule     n\n##    &lt;chr&gt;       &lt;chr&gt; &lt;int&gt;\n## 1   maha    maha_mad  6329\n## 2   maha  maha_tukey  5511\n## 3 column   price_mad  5386\n## 4 column price_tukey  3540\n## 5 column   table_mad  2560\n## # ... with 32 more rows\nTwo out of three Mahalanobis based definition yielded the most row outliers."
  },
  {
    "objectID": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#combination",
    "href": "blog/2017-12-26-combined-outlier-detection-with-dplyr-and-ruler.html#combination",
    "title": "Combined outlier detection with dplyr and ruler",
    "section": "Combination",
    "text": "Combination\nGiven outliers data frame, one can do whatever he/she wants to identify outliers. Here we will use the basic combination approach based on average score.\nCombined outlier detection score for certain row can be defined as share of applied methods that tagged it as outlier. Alternatively one can define it just as number of those methods as it will only change absolute value of the result and not the order.\noutlier_score &lt;- outliers %&gt;%\n  group_by(id) %&gt;%\n  # nrow(used_rules) equals total number of applied methods\n  summarise(score = n() / nrow(used_rules))\n\n# Top 10 outliers\noutlier_score %&gt;% arrange(desc(score)) %&gt;% slice(1:10)\n## # A tibble: 10 x 2\n##       id     score\n##    &lt;int&gt;     &lt;dbl&gt;\n##  1 26432 0.5777778\n##  2 27416 0.5777778\n##  3 27631 0.5777778\n##  4 27131 0.4666667\n##  5 23645 0.4222222\n##  6 26445 0.4222222\n##  7 26745 0.4000000\n##  8 27430 0.4000000\n##  9 15952 0.3777778\n## 10 17197 0.3777778\nFinally we will tag those rows as strong outliers which has score more than 0.2 (subjective threshold which should be researched more).\ndiam_tbl &lt;- diamonds %&gt;%\n  mutate(id = 1:n()) %&gt;%\n  left_join(y = outlier_score, by = \"id\") %&gt;%\n  mutate(\n    score = coalesce(score, 0),\n    is_out = if_else(score &gt; 0.2, \"Outlier\", \"Not outlier\")\n  )\n\n# Total number of outliers\nsum(diam_tbl$score &gt; 0.2)\nTibble diam_tbl is basically the diamonds but with three more columns: id for row number, score for combined outlier score and is_out for non-outlier/outlier tag.\nPlots illustrating strong outliers:\ntheme_set(theme_bw())\n\nplot_outliers &lt;- function(tbl, x, y, facet_var) {\n  tbl %&gt;%\n    arrange(is_out) %&gt;%\n    ggplot(aes_string(x, y, colour = \"is_out\")) +\n      geom_point() +\n      facet_wrap(facets = facet_var) +\n      scale_colour_manual(values = c(\"#AAAAAA\", \"#004080\")) +\n      guides(colour = guide_legend(title = NULL,\n                                   override.aes = list(size = 4))) +\n      labs(title = paste0(\"Strong outliers illustration by \", facet_var)) +\n      theme(legend.position = \"bottom\",\n            legend.text = element_text(size = 14))\n}\n\ndiam_tbl %&gt;% plot_outliers(\"carat\", \"price\", facet_var = \"cut\")\n\ndiam_tbl %&gt;% plot_outliers(\"x\", \"depth\", facet_var = \"color\")\n\ndiam_tbl %&gt;% plot_outliers(\"price\", \"table\", facet_var = \"clarity\")\n\nBased on those plots we see the complicated nature of “strong outliers”. They are not necessary located “on the edge” of two-dimensional scatter plots, but most extreme cases are tagged as outliers.\nAlso one interesting observation: most outliers are concentrated in the combination of “Fair” cut, “J” colour and “I1” clarity which are worst options among their features. The reason of this effect is group-based definitions of non-outliers which tagged certain groups more than others:\nbreaker_report %&gt;%\n  filter(pack == \"group\") %&gt;%\n  count(var, sort = TRUE) %&gt;%\n  print(n = 10)\n## # A tibble: 47 x 2\n##            var     n\n##          &lt;chr&gt; &lt;int&gt;\n##  1   Fair_D_I1     7\n##  2   Fair_J_I1     7\n##  3 Fair_H_VVS1     6\n##  4  Ideal_J_I1     6\n##  5 Fair_J_VVS1     5\n##  6 Fair_G_VVS1     4\n##  7 Fair_D_VVS1     3\n##  8   Fair_E_I1     3\n##  9   Fair_F_I1     3\n## 10   Fair_H_I1     3\n## # ... with 37 more rows\nHere we see that “Fair” cut is among majority of top breaker groups. There are also some interesting combinations: Fair_D_I1 (“worst”-“best”-“worst”), Fair_J_I1 (“worst”-“worst”-“worst”), Ideal_J_I1 (“best”-“worst”-“worst”).\nThis fact might be interpreted as suggested combined outlier detection approach discovered notion of diamond quality without prior knowledge about it."
  },
  {
    "objectID": "blog/2024-04-05-neovim-now-has-builtin-commenting.html",
    "href": "blog/2024-04-05-neovim-now-has-builtin-commenting.html",
    "title": "Neovim now has built-in commenting",
    "section": "",
    "text": "Originally posted on Reddit\nThe PR for built-in commenting has been merged into Nightly builds. There is more info in the initial PR comment and help entry, but for the lazy:\n\nAll it does is out of the box mappings:\n\ngc operator (Normal and Visual mode) to toggle comments.\ngc textobject (Operator-pending mode) as operator target.\ngcc for toggling comments in current line (basically a convenient gc_ remap).\n\nAlso, of course, dot-repeat, [count] support, etc.\nThis is basically a simplified version of ‘mini.commment’ with mostly default config.\nThe pad_comment_parts is false meaning that ‘commentstring’ option is taken as is, without forcing single space padding. This is planned to be addressed by adjusting default ‘commentstring’ values some time later (at least after 0.10.0 release) or can be done by users themselves in their configs.\nOn the surface it is quite close to ‘tpope/vim-commentary’, but with some difference in how it handles blank lines: blank lines are commented but do not affect the toggle action decision. See example in this comment.\n\nSo if you are using only basics of ‘mini.comment’ (no custom hooks, options, or callable ‘commentstring’s) or ’tpope/vim-commentary’, you might find the new built-in commenting on Nightly to be enough."
  },
  {
    "objectID": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html",
    "href": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html",
    "title": "Rule Your Data with Tidy Validation Reports. Design",
    "section": "",
    "text": "Some time ago I had a task to write data validation code. As for most R practitioners, this led to exploration of present solutions. I was looking for a package with the following features:\n\nRelatively small amount of time should be spent learning before comfortable usage. Preferably, it should be built with tidyverse in mind.\nIt should be quite flexible in terms of types of validation rules.\nPackage should offer functionality for both validations (with relatively simple output format) and assertions (with relatively flexible behaviour).\nPipe-friendliness.\nValidating only data frames would be enough.\n\nAfter devoting couple of days to research, I didn’t find any package fully (subjectively) meeting my needs (for a composed list look here). So I decided to write one myself. More precisely, it turned out into not one but two packages: ruler and keyholder, which powers some of ruler’s functionality.\nThis post is a rather long story about key moments in the journey of ruler’s design process. To learn other aspects see its README (for relatively brief introduction) or vignettes (for more thorough description of package capabilities)."
  },
  {
    "objectID": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#dplyr-data-units",
    "href": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#dplyr-data-units",
    "title": "Rule Your Data with Tidy Validation Reports. Design",
    "section": "Dplyr data units",
    "text": "Dplyr data units\nI started with an attempt of simple and clear formulation of validation: it is a process of checking whether something satisfies certain conditions. As it was enough to be only validating data frames, something should be thought of as parts of data frame which I will call data units. Certain conditions might be represented as functions, which I will call rules, associated with some data unit and which return TRUE, if condition is satisfied, and FALSE otherwise.\nI decided to make dplyr package a default tool for creating rules. The reason is, basically, because it satisfies most conditions I had in mind. Also I tend to use it for interactive validation of data frames, as, I am sure, many more R users. Its pipe-friendliness creates another important feature: interactive code can be transformed into a function just by replacing the initial data frame variable by a dot .. This will create a functional sequence, “a function which applies the entire chain of right-hand sides in turn to its input.”.\ndplyr offers a set of tools for operating with the following data units (see comments):\nis_integerish &lt;- function(x) {all(x == as.integer(x))}\nz_score &lt;- function(x) {abs(x - mean(x)) / sd(x)}\n\nmtcars_tbl &lt;- mtcars %&gt;% as_tibble()\n\n# Data frame as a whole\nvalidate_data &lt;- . %&gt;% summarise(nrow_low = n() &gt;= 15,\n                                 nrow_up = n() &lt;= 20)\nmtcars_tbl %&gt;% validate_data()\n## # A tibble: 1 x 2\n##   nrow_low nrow_up\n##      &lt;lgl&gt;   &lt;lgl&gt;\n## 1     TRUE   FALSE\n\n# Group as a whole\nvalidate_groups &lt;- . %&gt;% group_by(vs, am) %&gt;%\n  summarise(vs_am_low = n() &gt;= 7) %&gt;%\n  ungroup()\nmtcars_tbl %&gt;% validate_groups()\n## # A tibble: 4 x 3\n##      vs    am vs_am_low\n##   &lt;dbl&gt; &lt;dbl&gt;     &lt;lgl&gt;\n## 1     0     0      TRUE\n## 2     0     1     FALSE\n## 3     1     0      TRUE\n## 4     1     1      TRUE\n\n# Column as a whole\nvalidate_columns &lt;- . %&gt;%\n  summarise_if(is_integerish, funs(is_enough_sum = sum(.) &gt;= 14))\nmtcars_tbl %&gt;% validate_columns()\n## # A tibble: 1 x 6\n##   cyl_is_enough_sum hp_is_enough_sum vs_is_enough_sum am_is_enough_sum\n##               &lt;lgl&gt;            &lt;lgl&gt;            &lt;lgl&gt;            &lt;lgl&gt;\n## 1              TRUE             TRUE             TRUE            FALSE\n## # ... with 2 more variables: gear_is_enough_sum &lt;lgl&gt;,\n## #   carb_is_enough_sum &lt;lgl&gt;\n\n# Row as a whole\nvalidate_rows &lt;- . %&gt;% filter(vs == 1) %&gt;%\n  transmute(is_enough_sum = rowSums(.) &gt;= 200)\nmtcars_tbl %&gt;% validate_rows()\n## # A tibble: 14 x 1\n##   is_enough_sum\n##           &lt;lgl&gt;\n## 1          TRUE\n## 2          TRUE\n## 3          TRUE\n## 4          TRUE\n## 5          TRUE\n## # ... with 9 more rows\n\n# Cell\nvalidate_cells &lt;- . %&gt;%\n  transmute_if(is.numeric, funs(is_out = z_score(.) &gt; 1)) %&gt;%\n  slice(-(1:5))\nmtcars_tbl %&gt;% validate_cells()\n## # A tibble: 27 x 11\n##   mpg_is_out cyl_is_out disp_is_out hp_is_out drat_is_out wt_is_out\n##        &lt;lgl&gt;      &lt;lgl&gt;       &lt;lgl&gt;     &lt;lgl&gt;       &lt;lgl&gt;     &lt;lgl&gt;\n## 1      FALSE      FALSE       FALSE     FALSE        TRUE     FALSE\n## 2      FALSE       TRUE        TRUE      TRUE       FALSE     FALSE\n## 3      FALSE       TRUE       FALSE      TRUE       FALSE     FALSE\n## 4      FALSE       TRUE       FALSE     FALSE       FALSE     FALSE\n## 5      FALSE      FALSE       FALSE     FALSE       FALSE     FALSE\n## # ... with 22 more rows, and 5 more variables: qsec_is_out &lt;lgl&gt;,\n## #   vs_is_out &lt;lgl&gt;, am_is_out &lt;lgl&gt;, gear_is_out &lt;lgl&gt;, carb_is_out &lt;lgl&gt;"
  },
  {
    "objectID": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#tidy-data-validation-report",
    "href": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#tidy-data-validation-report",
    "title": "Rule Your Data with Tidy Validation Reports. Design",
    "section": "Tidy data validation report",
    "text": "Tidy data validation report\nAfter realizing this type of dplyr structure, I noticed the following points.\nIn order to use dplyr as tool for creating rules, there should be one extra level of abstraction for the whole functional sequence. It is not a rule but rather a several rules. In other words, it is a function that answers multiple questions about one type of data unit. I decided to call this rule pack or simply pack.\nIn order to identify, whether some data unit obeys some rule, one needs to describe that data unit, rule and result of validation. Descriptions of last two are simple: for rule it is a combination of pack and rule names (which should always be defined) and for validation result it is value TRUE or FALSE.\nDescription of data unit is trickier. After some thought, I decided that the most balanced way to do it is with two variables:\n\nvar (character) which represents the variable name of data unit:\n\nValue “.all” is reserved for “all columns as a whole”.\nValue equal to some column name indicates column of data unit.\nValue not equal to some column name indicates the name of group: it is created by uniting (with delimiter) group levels of grouping columns.\n\nid (integer) which represents the row index of data unit:\n\nValue 0 is reserved for “all rows as a whole”.\nValue not equal to 0 indicates the row index of data unit.\n\n\nCombinations of these variables describe all mentioned data units:\n\nvar == '.all' and id == 0: Data as a whole.\nvar != '.all' and id == 0: Group (var shouldn’t be an actual column name) or column (var should be an actual column name) as a whole.\nvar == '.all' and id != 0: Row as a whole.\nvar != '.all' and id != 0: Described cell.\n\nWith this knowledge in mind, I decided that the tidy data validation report should be a tibble with the following columns:\n\npack &lt;chr&gt; : Pack name.\nrule &lt;chr&gt; : Rule name inside pack.\nvar &lt;chr&gt; : Variable name of data unit.\nid &lt;int&gt; : Row index of data unit.\nvalue &lt;lgl&gt; : Whether the described data unit obeys the rule."
  },
  {
    "objectID": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#exposure",
    "href": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#exposure",
    "title": "Rule Your Data with Tidy Validation Reports. Design",
    "section": "Exposure",
    "text": "Exposure\nUsing only described report as validation output is possible if only information about breakers (data units which do not obey respective rules) is interesting. However, reproducibility is a great deal in R community, and keeping information about call can be helpful for future use.\nThis idea led to creation of another object in ruler called packs info. It is also a tibble which contains all information about exposure call:\n\nname &lt;chr&gt; : Name of the rule pack. This column is used to match column pack in tidy report.\ntype &lt;chr&gt; : Name of pack type. Indicates which data unit pack checks.\nfun &lt;list&gt; : List of actually used rule pack functions.\nremove_obeyers &lt;lgl&gt; : Value of convenience argument of the future expose function. It indicates whether rows about obeyers (data units that obey certain rule) were removed from report after applying pack.\n\nTo fully represent validation, described two tibbles should be returned together. So the actual validation result is decided to be exposure which is basically an S3 class list with two tibbles packs_info and report. This data structure is fairly easy to understand and use. For example, exposures can be binded together which is useful for combining several validation results. Also its elements are regular tibbles which can be filtered, summarised, joined, etc."
  },
  {
    "objectID": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#interpretation-of-dplyr-output",
    "href": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#interpretation-of-dplyr-output",
    "title": "Rule Your Data with Tidy Validation Reports. Design",
    "section": "Interpretation of dplyr output",
    "text": "Interpretation of dplyr output\nI was willing to use pure dplyr in creating rule packs, i.e. without extra knowledge of data unit to be validated. However, I found it impossible to do without experiencing annoying edge cases. Problem with this approach is that all of dplyr outputs are tibbles with similar structures. The only differentiating features are:\n\nsummarise without grouping returns tibble with one row and user-defined column names.\nsummarise with grouping returns tibble with number of rows equal to number of summarised groups. Columns consist from grouping and user-defined ones.\ntransmute returns tibble with number of rows as in input data frame and user-defined column names.\nScoped variants of summarise and transmute differ from regular ones in another mechanism of creating columns. They apply all supplied functions to all chosen columns. Resulting names are “the shortest … needed to uniquely identify the output”. It means that:\n\nIn case of one function they are column names.\nIn case of more than one function and one column they are function names.\nIn case of more than one column and function they are combinations of column and function names, pasted with character _ (which, unfortunately, is hardcoded). To force this behaviour in previous cases both columns and functions should be named inside of helper functions vars and funs. To match output columns with combination of validated column and rule, this option is preferred. However, there is a need of different separator between column and function names, as character _ is frequently used in column names.\n\n\nThe first attempt was to use the following algorithm to interpret (identify validated data unit) the output:\n\nIf there is at least one non-logical column then groups are validated. The reason is that in most cases grouping columns are character or factor ones. This already introduces edge case with logical grouping columns.\nCombination of whether number of rows equals 1 (n_rows_one) and presence of name separator in all column names (all_contain_sep) is used to make interpretation:\n\nIf n_rows_one == TRUE and all_contain_sep == FALSE then data is validated.\nIf n_rows_one == TRUE and all_contain_sep == TRUE then columns are validated.\nIf n_rows_one == FALSE and all_contain_sep == FALSE then rows are validated. This introduces an edge case when output has one row which is intended to be validated. It will be interpreted as ‘data as a whole’.\nIf n_rows_one == FALSE and all_contain_sep == TRUE then cells are validated. This also has edge case when output has one row in which cells are intended to be validated. It will be interpreted as ‘columns as a whole’.\n\n\nDespite of having edge cases, this algorithm is good for guessing the validated data unit, which can be useful for interactive use. Its important prerequisite is to have a simple way of forcing extended naming in scoped dplyr verbs with custom rarely used separator."
  },
  {
    "objectID": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#pack-creation",
    "href": "blog/2017-11-28-rule-your-data-with-tidy-validation-reports-design.html#pack-creation",
    "title": "Rule Your Data with Tidy Validation Reports. Design",
    "section": "Pack creation",
    "text": "Pack creation\nResearch of pure dplyr-style way of creating rule packs left no choice but to create a mechanism of supplying information about data unit of interest along with pack functions. It consists of following important principles.\nUse ruler’s function rules() instead of funs(). Its goals are to force usage of full naming in scoped dplyr verbs as much as possible and impute missing rule names (as every rule should be named for validation report). rules is just a wrapper for funs but with extra functionality of naming its every output element and adding prefix to that names (which will be used as a part of separator between column and rule name). By default prefix is a string ._.. It is chosen for its, hopefully, rare usage inside column names and symbolism (it is the Morse code of letter ‘R’).\nfuns(mean, sd)\n## &lt;fun_calls&gt;\n## $ mean: mean(.)\n## $ sd  : sd(.)\n\nrules(mean, sd)\n## &lt;fun_calls&gt;\n## $ ._.rule..1: mean(.)\n## $ ._.rule..2: sd(.)\n\nrules(mean, sd, .prefix = \"___\")\n## &lt;fun_calls&gt;\n## $ ___rule..1: mean(.)\n## $ ___rule..2: sd(.)\n\nrules(fn_1 = mean, fn_2 = sd)\n## &lt;fun_calls&gt;\n## $ ._.fn_1: mean(.)\n## $ ._.fn_2: sd(.)\nNote that in case of using only one column in scoped verb it should be named within dplyr::vars in order to force full naming.\nUse functions supported by keyholder to build rule packs. One of the main features I was going to implement is a possibility of validating only a subset of all possible data units. For example, validation of only last two rows (or columns) of data frame. There is no problem with columns: they can be specified with summarise_at. However, the default way of specifying rows is by subsetting data frame, after which all information about original row position is lost. To solve this, I needed a mechanism of tracking rows as invisibly for user as possible. This led to creation of keyholder package (which is also on CRAN now). To learn details about it go to its site or read my previous post.\nUse specific rule pack wrappers for certain data units. Their goal is to create S3 classes for rule packs in order to carry information about data unit of interest through exposing process. All of them always return a list with supplied functions but with changed attribute class (with additional group_vars and group_sep for group_packs()). Note that packs might be named inside these functions, which is recommended. If not, names will be imputed during exposing process. Also note that supplied functions are not checked to be correct in terms of validating specified data unit. This is done during exposure (exposing process).\n# Data unit. Rule pack is manually named 'my_data'\nmy_data_packs &lt;- data_packs(my_data = validate_data)\nmap(my_data_packs, class)\n## $my_data\n## [1] \"data_pack\" \"rule_pack\" \"fseq\"      \"function\"\n\n# Group unit. Need to supply grouping variables explicitly\nmy_group_packs &lt;- group_packs(validate_groups, .group_vars = c(\"vs\", \"am\"))\nmap(my_group_packs, class)\n## [[1]]\n## [1] \"group_pack\" \"rule_pack\"  \"fseq\"       \"function\"\n\n# Column unit. Need to be rewritten using `rules`\nmy_col_packs &lt;- col_packs(\n  my_col = . %&gt;%\n    summarise_if(is_integerish, rules(is_enough_sum = sum(.) &gt;= 14))\n)\nmap(my_col_packs, class)\n## $my_col\n## [1] \"col_pack\"  \"rule_pack\" \"fseq\"      \"function\"\n\n# Row unit. One can supply several rule packs\nmy_row_packs &lt;- row_packs(\n  my_row_1 = validate_rows,\n  my_row_2 = . %&gt;% transmute(is_vs_one = vs == 1)\n)\nmap(my_row_packs, class)\n## $my_row_1\n## [1] \"row_pack\"  \"rule_pack\" \"fseq\"      \"function\"\n##\n## $my_row_2\n## [1] \"row_pack\"  \"rule_pack\" \"fseq\"      \"function\"\n\n# Cell unit. Also needs to be rewritten using `rules`.\nmy_cell_packs &lt;- cell_packs(\n  my_cell = . %&gt;%\n    transmute_if(is.numeric, rules(is_out = z_score(.) &gt; 1)) %&gt;%\n    slice(-(1:5))\n)\nmap(my_cell_packs, class)\n## $my_cell\n## [1] \"cell_pack\" \"rule_pack\" \"fseq\"      \"function\""
  },
  {
    "objectID": "blog/2024-08-29-psa-adjust-`pmenusel`.html",
    "href": "blog/2024-08-29-psa-adjust-`pmenusel`.html",
    "title": "PSA for color scheme authors: you might want to adjust PmenuSel",
    "section": "",
    "text": "Originally posted on Reddit\nTL;DR: to account for possible new custom highlighting in popup menu items, you might want to switch fg and bg colors in Pmenu*Sel groups while adding reverse attribute.\n\nNeovim Nightly (0.11) recently landed new features regarding adding custom highlighting to items of built-in popup menu:\n\nWhole item via hl_group (thanks to glepnir).\nOnly “kind” field via kind_hlgroup (also thanks to glepnir).\n\nBoth of the fields combine highlight attributes with underlying highlighting from Pmenu* groups. This is a great choice because it allows plugin authors to use highlight groups which modify only foreground (a.k.a. text) and it will “reuse” background of popup menu. An alternative would have been to expose special highlight groups which each color scheme would have needed to define (while only adding its own popup menu background).\nItem highlighting usually should work well with displaying text in regular popup menu item (if it has background similar to Normal in terms of lightness), but might result into barely readable text when item is selected. This can be the case if color scheme chooses for PmenuSel (and its variants like PmenuKindSel, etc.) to have intentionally inverted lightness compared to Pmenu. I’ve encountered this with ‘mini.hues’ and default color scheme (which is planned to be fixed).\nLuckily, there is a solution, albeit a bit unintuitive one: switch fg&lt;-&gt;bg attributes and add reverse attribute. Here is an example from bundled ‘evening’ color scheme:\n\nCurrent:\n\nhi PmenuSel      guifg=#000000 guibg=#bebebe gui=NONE cterm=NONE\nhi PmenuMatchSel guifg=#8b008b guibg=#bebebe gui=NONE cterm=NONE\n\nAdjusted PmenuSel:\n\nhi PmenuSel      guifg=#bebebe guibg=#000000 gui=reverse cterm=reverse\nhi PmenuMatchSel guifg=#bebebe guibg=#8b008b gui=reverse cterm=reverse\nHere is a screenshot of before (left) and after (right)\nThis works because combining highlight attributes first combines foreground/background colors and only after that applies reverse. It will keep the same visuals if there is no highlighting, but should also work with reasonable choices of hl_group and kind_hlgroup in completion items.\n\nIf you are a color scheme author or a concerned user of one, I think making this change is worth it. You can use the test code example from PR to default color scheme. Hope it helps."
  },
  {
    "objectID": "blog/2018-05-15-general-gems-of-comperes.html",
    "href": "blog/2018-05-15-general-gems-of-comperes.html",
    "title": "General gems of comperes",
    "section": "",
    "text": "Prologue\nI am very glad to announce that my new package comperes is on CRAN now. It provides tools for managing competition results in a tidy manner as much as possible. For more information go to:\n\nPackage README.\nPackage vignettes.\nMy previous post for usage examples based on built-in hp_survey data set (results of my Harry Potter Books Survey).\n\nBesides tools for competition results, comperes offers some functions that can be useful in more general tasks. This post presents examples of their most common usage.\n\n\nOverview\nThis post covers the following themes:\n\nCompute vector levels with levels2().\nManage item summaries with summarise_item() and join_item_summary().\nConvert pairwise data with long_to_mat() and mat_to_long().\n\nFor examples we will use a shortened version of the everlasting mtcars data set. We will need the following setup:\nlibrary(comperes)\nlibrary(rlang)\n# For example analysis\nlibrary(dplyr)\nlibrary(tibble)\n\nmtcars_tbl &lt;- mtcars %&gt;%\n  rownames_to_column(var = \"car\") %&gt;%\n  select(car, cyl, vs, carb) %&gt;%\n  as_tibble()\n\n\nCompute vector levels\nWe will start with the most simple function. During comperes development, idea about the it really helped me reason more clearly about package functional API. I am talking about levels2() which computes “levels” of any non-list vector.\nIt has the following logic: if x has levels attribute then return levels(x); otherwise return character representation of vector’s sorted unique values. Notes about design and implementation of this function:\n\nI hesitated a lot about whether it should return character or same type as input vector in case x has no levels. In many practical cases there is a need in latter behavior. However, in the end I decided that type stable output (levels(x) always returns character vector or NULL) is better.\nConversion to character is done after sorting, which is really important when dealing with numeric vectors.\n\nThis function is helpful when one needs to produce unique values in standardized manner (for example, during pairwise distance computation). Some examples:\nlevels2(mtcars_tbl$cyl)\n## [1] \"4\" \"6\" \"8\"\n\n# Importance of conversion to character after sorting\ntricky_vec &lt;- c(10, 1, 2, 12)\nsort(as.character(tricky_vec))\n## [1] \"1\"  \"10\" \"12\" \"2\"\nlevels2(tricky_vec)\n## [1] \"1\"  \"2\"  \"10\" \"12\"\n\n\nManage item summaries\nArguably, the most common task in data analysis is computation of group summaries. This task is conveniently done by consecutive application of dplyr’s group_by(), summarise() and ungroup() (to return regular data frame and not grouped one). comperes offers a wrapper summarise_item() for this task (which always returns tibble instead of a data frame) with additional feature of modifying column names by adding prefix (which will be handy soon):\ncyl_vs_summary &lt;- mtcars_tbl %&gt;%\n  summarise_item(\n    item = c(\"cyl\", \"vs\"),\n    n = n(), mean_carb = mean(carb),\n    .prefix = \"cyl_vs__\"\n  )\ncyl_vs_summary\n## # A tibble: 5 x 4\n##     cyl    vs cyl_vs__n cyl_vs__mean_carb\n##   &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;             &lt;dbl&gt;\n## 1    4.    0.         1              2.00\n## 2    4.    1.        10              1.50\n## 3    6.    0.         3              4.67\n## 4    6.    1.         4              2.50\n## 5    8.    0.        14              3.50\nSometimes, there is also a need to compare actual values with their summaries across different grouping. For example, determine whether car’s number of carburetors (carb) is bigger than average value per different groupings: by number of cylinders cyl and V/S vs.\nTo simplify this task, comperes offers a join_item_summary() function for that: it computes item summary with summarise_item() and joins it (with dplyr::left_join()) to input data frame:\n# Save (with rlang magic) expression for reused summary\ncarb_summary &lt;- list(mean_carb = expr(mean(carb)))\n\n# Create new columns with joined grouped summaries\nmtcats_gear_summary &lt;- mtcars_tbl %&gt;%\n  join_item_summary(\"cyl\", !!! carb_summary, .prefix = \"cyl__\") %&gt;%\n  join_item_summary(\"vs\",  !!! carb_summary, .prefix = \"vs__\")\n\nprint(mtcats_gear_summary, width = Inf)\n## # A tibble: 32 x 6\n##   car                 cyl    vs  carb cyl__mean_carb vs__mean_carb\n##   &lt;chr&gt;             &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt;\n## 1 Mazda RX4            6.    0.    4.           3.43          3.61\n## 2 Mazda RX4 Wag        6.    0.    4.           3.43          3.61\n## 3 Datsun 710           4.    1.    1.           1.55          1.79\n## 4 Hornet 4 Drive       6.    1.    1.           3.43          1.79\n## 5 Hornet Sportabout    8.    0.    2.           3.50          3.61\n## # ... with 27 more rows\n\n# Compute comparisons\nmtcats_gear_summary %&gt;%\n  mutate_at(vars(ends_with(\"mean_carb\")), funs(carb &gt; .)) %&gt;%\n  select(car, ends_with(\"mean_carb\")) %&gt;%\n  rename_at(vars(-car), funs(gsub(\"__mean_carb$\", \"\", .)))\n## # A tibble: 32 x 3\n##   car               cyl   vs\n##   &lt;chr&gt;             &lt;lgl&gt; &lt;lgl&gt;\n## 1 Mazda RX4         TRUE  TRUE\n## 2 Mazda RX4 Wag     TRUE  TRUE\n## 3 Datsun 710        FALSE FALSE\n## 4 Hornet 4 Drive    FALSE FALSE\n## 5 Hornet Sportabout FALSE FALSE\n## # ... with 27 more rows\nAdding different prefixes helps navigating through columns with different summaries.\n\n\nConvert pairwise data\nOne of the main features of comperes is the ability to compute Head-to-Head values of players in competition. There are functions h2h_long() and h2h_mat() which produce output in “long” (tibble with row describing one ordered pair) and “matrix” (matrix with cell value describing pair in corresponding row and column) formats respectively.\nThese formats of pairwise data is quite common: “long” is better for tidy computing and “matrix” is better for result presentation. Also converting distance matrix to data frame with pair data is a theme of several Stack Overflow questions (for example, this one and that one).\nPackage comperes has functions as_h2h_long() and as_h2h_mat() for converting between those formats. They are powered by a “general usage” functions long_to_mat() and mat_to_long(). Here is an example of how they can be used to convert between different formats of pairwise distances:\n# Compute matrix of pairwise distances based on all numeric columns\ndist_mat &lt;- mtcars_tbl %&gt;%\n  select_if(is.numeric) %&gt;%\n  dist() %&gt;%\n  as.matrix()\ndist_mat[1:4, 1:4]\n##          1        2        3        4\n## 1 0.000000 0.000000 3.741657 3.162278\n## 2 0.000000 0.000000 3.741657 3.162278\n## 3 3.741657 3.741657 0.000000 2.000000\n## 4 3.162278 3.162278 2.000000 0.000000\n\n# Convert to data frame (tibble in this case)\ndist_tbl &lt;- dist_mat %&gt;%\n  mat_to_long(row_key = \"id_1\", col_key = \"id_2\", value = \"dist\")\ndist_tbl\n## # A tibble: 1,024 x 3\n##   id_1  id_2   dist\n##   &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;\n## 1 1     1      0.\n## 2 1     2      0.\n## 3 1     3      3.74\n## 4 1     4      3.16\n## 5 1     5      2.83\n## # ... with 1,019 more rows\n\n# Convert tibble back to matrix\ndist_mat_new &lt;- dist_tbl %&gt;%\n  # To make natural row and column sortings\n  mutate_at(vars(\"id_1\", \"id_2\"), as.numeric) %&gt;%\n  long_to_mat(row_key = \"id_1\", col_key = \"id_2\", value = \"dist\")\nidentical(dist_mat, dist_mat_new)\n## [1] TRUE\n\n\nConclusion\n\nPackage comperes provides not only tools for managing competition results but also functions with general purpose:\n\nCompute vector levels with levels2(). Usually used to produce unique values in standardized manner.\nManage item summaries with summarise_item() and join_item_summary(). May be used to concisely compute comparisons of values with summaries from different groupings.\nConvert pairwise data with long_to_mat() and mat_to_long(). Very helpful in converting pairwise distances between “long” and “matrix” formats.\n\n\n\n\n\n\n\n\nsessionInfo()\n\n\n\n\n\n## R version 3.4.4 (2018-03-15)\n## Platform: x86_64-pc-linux-gnu (64-bit)\n## Running under: Ubuntu 16.04.4 LTS\n##\n## Matrix products: default\n## BLAS: /usr/lib/openblas-base/libblas.so.3\n## LAPACK: /usr/lib/libopenblasp-r0.2.18.so\n##\n## locale:\n##  [1] LC_CTYPE=ru_UA.UTF-8       LC_NUMERIC=C\n##  [3] LC_TIME=ru_UA.UTF-8        LC_COLLATE=ru_UA.UTF-8\n##  [5] LC_MONETARY=ru_UA.UTF-8    LC_MESSAGES=ru_UA.UTF-8\n##  [7] LC_PAPER=ru_UA.UTF-8       LC_NAME=C\n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C\n## [11] LC_MEASUREMENT=ru_UA.UTF-8 LC_IDENTIFICATION=C\n##\n## attached base packages:\n## [1] methods   stats     graphics  grDevices utils     datasets  base\n##\n## other attached packages:\n## [1] bindrcpp_0.2.2   tibble_1.4.2     dplyr_0.7.5.9000 rlang_0.2.0\n## [5] comperes_0.2.0\n##\n## loaded via a namespace (and not attached):\n##  [1] Rcpp_0.12.16     knitr_1.20       bindr_0.1.1      magrittr_1.5\n##  [5] tidyselect_0.2.4 R6_2.2.2         stringr_1.3.0    tools_3.4.4\n##  [9] xfun_0.1         utf8_1.1.3       cli_1.0.0        htmltools_0.3.6\n## [13] yaml_2.1.17      rprojroot_1.3-2  digest_0.6.15    assertthat_0.2.0\n## [17] crayon_1.3.4     bookdown_0.7     purrr_0.2.4      glue_1.2.0\n## [21] evaluate_0.10.1  rmarkdown_1.9    blogdown_0.5     stringi_1.1.6\n## [25] compiler_3.4.4   pillar_1.2.1     backports_1.1.2  pkgconfig_2.0.1"
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html",
    "href": "blog/2017-11-05-mythical-generic-overhead.html",
    "title": "Mythical Generic Overhead",
    "section": "",
    "text": "Earlier this week I came across this tweet from Thomas (author of many useful and powerful R packages). “Generic+methods” approach is considered better for many reasons, which can be summarised as “easier code maintenance, extensibility and understandability”. Hadley’s ruthless answer confirms this.\nHowever, I was curious about looking for possible pros of “if-else” approach. The most legitimate point (in some circumstances) I was able to produce was “… method dispatch can be slower on microseconds level. But it rarely has any practical impacts”. This thought inspired me to make some analysis of possible computational overhead of using “generic+methods” approach over “if-else” and “switch” (which seems just a slight enhancement of “if-else”).\nNote that, regardless of this analysis outcome, using S3 methods is better choice over “if-else” sequence and “switch” statement in almost all practical cases."
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html#experiment",
    "href": "blog/2017-11-05-mythical-generic-overhead.html#experiment",
    "title": "Mythical Generic Overhead",
    "section": "Experiment",
    "text": "Experiment\n\nTake range of possible classes number (n_class) as 1:20.\nFor every value of n_class generate functions for “if-else” (if_get_true), “switch” (switch_get_true) and “generic+methods” (gen_get_true) approaches. Each of this function will take one argument x. They will check the class of x and perform return(TRUE) action (regardless of class).\nMeasure for particular n_class (with package microbenchmark) computation time of all possible *_get_true(x):\n\n* can be if, switch and gen.\nx is constructed independently from * and is a numeric value 1 with class equals to one of class1, …, class{n_class} (total n_class possibilities).\nArgument times of microbenchmark (actual number of times to evaluate the expression) should be quite big. As preliminary tests showed, computation time differs in microseconds, and big number of actual evaluations is needed to remove the statistical noise. In this analysis it is taken as \\(1000000\\).\nThe final benchmark of computation time is median of all actual evaluation times (in microseconds).\n\nTotally there should be \\((1 + 2 + ... + 20) \\cdot 3 = 630\\) measurements."
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html#hypotheses",
    "href": "blog/2017-11-05-mythical-generic-overhead.html#hypotheses",
    "title": "Mythical Generic Overhead",
    "section": "Hypotheses",
    "text": "Hypotheses\n\n“Generic+methods” benchmark correlates with number of classes. Search of appropriate method in bigger set should take more time.\n“Generic+methods” benchmark doesn’t correlate with class value. Just a guess.\n“If-else” benchmark positively correlates with class value. The later class is in “if-else” sequence the more time it should take to get to it.\n“If-else” and “switch” benchmarks are better than “generic+methods” with small number of classes but the opposite is true with bigger ones. This is just the initial guess which led to this analysis."
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html#setup",
    "href": "blog/2017-11-05-mythical-generic-overhead.html#setup",
    "title": "Mythical Generic Overhead",
    "section": "Setup",
    "text": "Setup\nsuppressMessages(library(tidyverse))\nsuppressMessages(library(rlang))\nsuppressMessages(library(microbenchmark))\n\nset.seed(1105)\n\nggplot2::theme_set(theme_bw())"
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html#function-generators",
    "href": "blog/2017-11-05-mythical-generic-overhead.html#function-generators",
    "title": "Mythical Generic Overhead",
    "section": "Function Generators",
    "text": "Function Generators\nFunction new_get_true_all() takes number of possible classes n_class and target environment env (by default, it is the environment from which function is called). It creates all necessary functions in env and returns it for cleaner use inside dplyr’s mutate.\n# Wrapper for creating function with one argument `x` in environment `env`\nnew_f &lt;- function(name, body, env) {\n  fun &lt;- new_function(alist(x = ), parse_expr(body), env)\n\n  assign(x = name, value = fun, envir = env)\n}\n\nnew_if_get_true &lt;- function(n_class = 1, env = caller_env()) {\n  body &lt;- paste0(\n    'if (class(x) == \"class', seq_len(n_class), '\") { return(TRUE) }',\n    collapse = \" else \"\n  )\n\n  new_f(\"if_get_true\", body, env)\n}\n\nnew_switch_get_true &lt;- function(n_class = 1, env = caller_env()) {\n  body &lt;- paste0(\n    \"switch(\\nclass(x),\\n\",\n    paste0(\"class\", seq_len(n_class), \" = return(TRUE)\",\n           collapse = \",\\n\"),\n    \"\\n)\"\n  )\n\n  new_f(\"switch_get_true\", body, env)\n}\n\nnew_gen_get_true &lt;- function(n_class = 1, env = caller_env()) {\n  # Create generic\n  new_f(\"gen_get_true\", 'UseMethod(\"gen_get_true\")', env)\n\n  # Create methods\n  method_names &lt;- paste0(\"gen_get_true.class\", seq_len(n_class))\n\n  walk(method_names, new_f, body = \"return(TRUE)\", env = env)\n}\n\nnew_get_true_all &lt;- function(n_class = 1, env = caller_env()) {\n  new_if_get_true(n_class = n_class, env = env)\n  new_switch_get_true(n_class = n_class, env = env)\n  new_gen_get_true(n_class = n_class, env = env)\n\n  env\n}\nFor example, the result of calling new_get_true_all(n_class = 2) from console is creation of the following functions in global environment (here class1 has id 1, class2 - 2 and so on):\nnew_get_true_all(n_class = 2)\n## &lt;environment: R_GlobalEnv&gt;\n\nif_get_true\n## function (x)\n## if (class(x) == \"class1\") {\n##     return(TRUE)\n## } else if (class(x) == \"class2\") {\n##     return(TRUE)\n## }\n\nswitch_get_true\n## function (x)\n## switch(class(x), class1 = return(TRUE), class2 = return(TRUE))\n\ngen_get_true\n## function (x)\n## UseMethod(\"gen_get_true\")\n\ngen_get_true.class1\n## function (x)\n## return(TRUE)\n\ngen_get_true.class2\n## function (x)\n## return(TRUE)"
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html#benchmark",
    "href": "blog/2017-11-05-mythical-generic-overhead.html#benchmark",
    "title": "Mythical Generic Overhead",
    "section": "Benchmark",
    "text": "Benchmark\nFunction for creating benchmarks for one value of n_class given already created environment env with all functions needed:\nbench_funs &lt;- function(n_class = 1, env = caller_env(), times = 1000000) {\n  bench &lt;- map(seq_len(n_class), function(class_id) {\n    assign(\"x\", structure(1, class = paste0(\"class\", class_id)), envir = env)\n    assign(\"times\", times, envir = env)\n\n    eval(\n      quote(microbenchmark(\n        'if' = if_get_true(x),\n        'switch' = switch_get_true(x),\n        gen = gen_get_true(x),\n        times = times\n      )),\n      envir = env\n    ) %&gt;%\n      as_tibble() %&gt;%\n      group_by(expr) %&gt;%\n      # Median computation time in microseconds\n      summarise(time = median(time) / 1000) %&gt;%\n      mutate(class_id = class_id)\n  }) %&gt;%\n    bind_rows() %&gt;%\n    rename(method = expr)\n\n  rm(list = c(\"x\", \"times\"), envir = env)\n\n  bench\n}\nComputing benchmarks:\n# Takes considerable amount of time to run\noverhead_bench &lt;- tibble(n_class = 1:20) %&gt;%\n  mutate(\n    env = rerun(n(), child_env(.GlobalEnv)),\n    env = map2(n_class, env, new_get_true_all),\n    bench = map2(n_class, env, bench_funs, times = 1000000)\n  ) %&gt;%\n  select(-env) %&gt;%\n  unnest(bench) %&gt;%\n  mutate(method = as.character(method)) %&gt;%\n  select(n_class, class_id, method, time)\nThe result has the following structure:\noverhead_bench\n## # A tibble: 630 x 4\n##    n_class class_id method  time\n##      &lt;int&gt;    &lt;int&gt;  &lt;chr&gt; &lt;dbl&gt;\n##  1       1        1     if 0.529\n##  2       1        1 switch 0.381\n##  3       1        1    gen 0.820\n##  4       2        1     if 0.521\n##  5       2        1 switch 0.396\n##  6       2        1    gen 0.811\n##  7       2        2     if 0.961\n##  8       2        2 switch 0.544\n##  9       2        2    gen 1.029\n## 10       3        1     if 0.554\n## # ... with 620 more rows"
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html#plots",
    "href": "blog/2017-11-05-mythical-generic-overhead.html#plots",
    "title": "Mythical Generic Overhead",
    "section": "Plots",
    "text": "Plots\nLet’s define function for plotting median computation time for every approach based on parameter param (which will be n_class and class_id).\nplot_median_time &lt;- function(tbl, param) {\n  param_enquo &lt;- enquo(param)\n\n  overhead_bench %&gt;%\n    mutate(\n      Method = case_when(\n        method == \"gen\" ~ \"generic+\\nmethods\",\n        method == \"if\" ~ \"if-else\",\n        method == \"switch\" ~ \"switch\"\n      )\n    ) %&gt;%\n    group_by(Method, param = UQ(param_enquo)) %&gt;%\n    summarise(median_time = median(time)) %&gt;%\n    ungroup() %&gt;%\n    ggplot(aes(param, median_time, group = Method, colour = Method)) +\n      geom_point() + geom_line() +\n      geom_hline(yintercept = 0, colour = \"red\")\n}\noverhead_bench %&gt;%\n  plot_median_time(n_class) +\n    labs(\n      title = \"Benchmarks for number of classes\",\n      x = \"Number of possible classes\",\n      y = \"Median computation time (microseconds)\"\n    )\n\noverhead_bench %&gt;%\n  plot_median_time(class_id) +\n    labs(\n      title = \"Benchmarks for class id\",\n      x = 'Class id (number in sequence of \"if-else\"s)',\n      y = \"Median computation time (microseconds)\"\n    )"
  },
  {
    "objectID": "blog/2017-11-05-mythical-generic-overhead.html#correlations",
    "href": "blog/2017-11-05-mythical-generic-overhead.html#correlations",
    "title": "Mythical Generic Overhead",
    "section": "Correlations",
    "text": "Correlations\nSimilarly to plotting, let’s define a function for computing correlation coefficient CI for parameter benchmarks.\nextract_cor_ci &lt;- function(cor_test) {\n  ci &lt;- round(cor_test$conf.int, digits = 4)\n\n  tibble(lower = ci[1], upper = ci[2])\n}\n\ncompute_median_time_cor_ci &lt;- function(tbl, param) {\n  param_enquo &lt;- enquo(param)\n\n  tbl %&gt;%\n    group_by(method, UQ(param_enquo)) %&gt;%\n    summarise(median_time = median(time)) %&gt;%\n    summarise(cor_test = list(cor.test(UQ(param_enquo), median_time,\n                                       conf.level = 0.95))) %&gt;%\n    mutate(cor_ci = map(cor_test, extract_cor_ci)) %&gt;%\n    select(-cor_test) %&gt;%\n    unnest(cor_ci)\n}\ncompute_median_time_cor_ci(overhead_bench, n_class)\n## # A tibble: 3 x 3\n##   method   lower  upper\n##    &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n## 1    gen -0.4407 0.4444\n## 2     if  0.9264 0.9886\n## 3 switch  0.8297 0.9726\ncompute_median_time_cor_ci(overhead_bench, class_id)\n## # A tibble: 3 x 3\n##   method   lower   upper\n##    &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n## 1    gen -0.8812 -0.4056\n## 2     if  0.9912  0.9987\n## 3 switch  0.9395  0.9907"
  },
  {
    "objectID": "blog/2023-12-03-neovim-now-has-its-own-default-color-scheme.html",
    "href": "blog/2023-12-03-neovim-now-has-its-own-default-color-scheme.html",
    "title": "Neovim now has its own default color scheme",
    "section": "",
    "text": "Originally posted on Reddit\n \nSee this PR for more details.\nAnd if you think you’ll miss magenta floating windows and completion menu, there is colorscheme vim."
  },
  {
    "objectID": "blog/2018-04-09-struggle-with-harry-potter-data.html",
    "href": "blog/2018-04-09-struggle-with-harry-potter-data.html",
    "title": "Struggle with Harry Potter Data",
    "section": "",
    "text": "Prologue\nRight now I am in the final stage of developing two packages devoted to results of abstract competitions (still not perfectly ready, so use with caution):\n\ncomperes - infrastructure package for dealing with different formats of competition results and summarising them.\ncomperank - package which implements some rating and ranking algorithms. Inspired and driven by insightful book Who’s #1?: The Science of Rating and Ranking by Amy N. Langville and Carl D. Meyer.\n\nUnderstanding of competition is quite general: it is a set of games (abstract event) in which players (abstract entity) gain some abstract scores (typically numeric).\nBoth packages use example data set called ncaa2005 which is results of an isolated group of Atlantic Coast Conference teams provided in “Who’s #1”. In order to demonstrate abstract nature of term “competition” I decided to find another data set, not from the world of sports. I thought it would be easy…\n\n\nOverview\nThis post describes my struggle with and process of creating Harry Potter Data with results of “competition” between seven original Harry Potter books. Long story short, I didn’t find suitable existing data and decided to make my own survey. The struggle hasn’t ended yet and I need your help. Please, take part in the survey at this link. It is a fairly standard Google Form with two questions: first one simulates picking random subset of books (read about that later); second offers rating choices. And that’s it. Please, note the following:\n\nIt is assumed that you’ve read all seven original J. K. Rowling Harry Potter books and are willing to give an honest feedback about your impressions.\nIn order to answer you have to sign in your Google account. This is done to ensure that one person participates in the survey only once. Your personal data won’t become public because I will not track it.\n\nThis post has the following structure:\n\nDecision about the source describes how I ended up with conducting my own survey.\nSurvey design has my thoughts about how I approached creating the survey.\nImplementation describes technical details about creating a survey using Google Forms and JavaScript.\n\n\n\nDecision about the source\nAfter all sorts of games, the process of people rating common items is, probably, the most common example of competition. Here items can be considered as “players”. “Game” is a person that reviews a set of items by rating them with numerical “score” (stars, points, etc.). To produce this data set I need data in the following format (called “long” in terminology of aforementioned packages):\n\nPerson identifier (anonymized). It will serve as game identifier.\nItem identifier which this person rated.\nNumeric score of item’s rating by person.\n\nOne person can rate multiple items but only once, to avoid bias in the output.\nAfter some thought I picked seven original J.K. Rowling “Harry Potter” books as items. Besides being interesting and popular books, I figured that it won’t be hard to find a data set with information I need. It wasn’t quite right because all available data sets don’t seem to have suitable license:\n\nThere are rating on Amazon but I didn’t find an easy way to even get the data.\nMy second candidate, Goodreads site, has an API which provides data exactly I need. However, their Developer Terms of Service doesn’t look “R package with MIT license” friendly.\nThere is also Kaggle’s goodbooks-10k data set. This comes as close to my needs as I could find. Nevertheless, it is still Goodreads data, so I am not sure about it.\n\nAll this led me to conducting my own survey. The good news is that this way I am the full owner of the data set with no license issues. The bad news - I should conduct the survey…\n\n\nSurvey design\nThe freedom in creating your own survey is overwhelming. Basically, you can do whatever you like and can wish to obtain any data you want. In grim reality, there are a couple of things to consider:\n\nSurvey should collect data that will actually help to fulfill its goals. It should be designed in a way that minimizes the chance of misinterpretation by respondents.\nSurvey should be implemented and conducted in the form that is accessible to people as much as possible. This is needed to collect more data with less efforts.\n\nThe goal of my Harry Potter Survey can be formulated as follows: collect data enough to rank Harry Potter books from worst to best. The most common way to do that is to mimic rating process from various online marketplaces. In more scientific language it means to ask a question using Likert scale. That is, an answer to some question should be chosen from some ordinal scale. Very helpful advice can be found in this SurveyMonkey article. The most common Likert scales can be found here.\nUnfortunately, I decided to read about how to conduct survey not in the beginning of constructing my own. Its core is a question that asks to associate Harry Potter Books with some numerical ratings. Evolution of this question was as follows:\n\nStage 1.\n\nQuestion: ‘Rate these BOOKS (not movies) from 1 (“very poor book”) to 7 (“exceptional book”)’. As you can see, this isn’t actually a question because at first I planned to give a task. Also I was trying to highly indicate that the books are of interest and not movies.\nScale: numeric, from 1 to 7. This seemed logical as numeric scores will be actually used in analysis.\n\nStage 2. After reading SurveyMonkey article and other sources, drastic transformation was made. I decided to actually ask a question about satisfaction after reading the book. This makes it a personal and, for my opinion, clear question.\n\nQuestion: ‘How did you like these BOOKS?’. One thing to consider here is whether it is better to use past or present tense. At this stage I decided to go with past tense because [in my opinion] it questions a fixed moment in time “just after reading a book”. Unfortunately, it wasn’t the case.\nScale: combined, from 1 to 5. It is fairly standard bipolar “Satisfaction” scale: “1 - Very dissatisfied”; “2 - Dissatisfied”; “3 - Neutral”; “4 - Satisfied”; “5 - Very satisfied”. I decided to move to 5 point scale as it is more common and should provide more reliable data. Its downside is smaller variance. I also preserved numbers for extra indication of implied linear scale.\n\nStage 3. After some thought and practical testing I decided not to invent the wheel and stick with more common “Quality” scale. This has an advantage of being more or less standard, which should provide more robust data.\n\nQuestion: ‘What is your impression of these Harry Potter BOOKS?’. Added explicit indication about Harry Potter to be able to shorten books’ names. Changed to present tense because I had mixed feedback about previous question and which moment in the past it referred to. Of course, I can add explicit reference but it might overcomplicate the question. Also, question in present tense should be easier to answer.\nScale: combined, from 1 to 5. It is fairly standard unipolar “Quality” scale: “1 - Poor”, “2 - Fair”, “3 - Good”, “4 - Very Good”, “5 - Excellent”.\n\n\nAfter designing the basic question, there are couple of other things to consider:\n\nItem names should be understandable. With seven Harry Potter books it might be confusing if they are presented only by title. So I decided to add book’s number in the series after its title. Also, explicit indication of “Harry Potter” in the title seems overcomplicating a survey, as it doesn’t add extra necessary information, so I decided to shorten it to “HP”. The resulting books are named “HP and the Philosopher’s (Sorcerer’s) Stone (#1)”, “HP and the Chamber of Secrets (#2)”, “HP and the Prisoner of Azkaban (#3)”, “HP and the Goblet of Fire (#4)”, “HP and the Order of the Phoenix (#5)”, “HP and the Half-Blood Prince (#6)”, “HP and the Deathly Hallows (#7)”.\nActual set of items can affect the outcome. For example, if person’s favourite book is present in the list, he/she might anchor his/her other ratings to this book. This can be solved by randomizing set of books asked to rate.\nActual order of items can affect the outcome. The reasoning is similar to previous note. This can be solved by randomizing the order of books presented.\n\nSo here is the final design of a survey. Respondent is asked a question “What is your impression of these Harry Potter BOOKS?” and presented with random subset (in random order) of names of seven Harry Potter books (presented above) which should be rated on pretty standard Likert “Quality” scale (with present numeric scores).\nAbout the desired number of respondents I think that hitting 100 will produce fairly usable output data set. But the more the better.\nAfter exhausting process of survey design I hoped that implementation should be easy. I again wasn’t quite right…\n\n\nImplementation\nThe main obstacle in implementing the intended survey is randomization of presented items. Also I had to keep in mind that answering process should be as easy as possible and that one person should be able to answer only once.\nAfter some Internet surfing, it seemed that the most optimal way of conducting a survey is with Google Forms. It can provide an option to participate in survey only once with a downside: one should have and be logged into a Google account. It can possibly scare off potential respondents. However, Google Forms has an option to not track user data, which I gladly used. It also has a feature to randomly shuffle the order of the items used in question, which is very helpful.\nThe biggest trouble with Google Forms is that it can’t randomly generate questions. I decided to work around this problem the following way:\n\nCreate many variants of question for all possible subsets of books. There are total of 127 non-empty subsets for 7 books. Items in every question should be shuffled.\nCreate dummy question (to be put first) which has a list of numbers - pointers to subsets of books. This list will be randomly shuffled for every respondent. Picking the first item from the list simulates generating random subset of books.\n\nAll this can be done manually. And I’ve actually done that… However, after deciding to change the question and scale (move from “Stage 1” to “Stage 2” in question evolution), I realized that it would be better to program Form creation. As it turns out, this can be done with Google Apps Script which accepts JavaScript code. After learning language basics and with great support of Internet, I came up with this solution:\n// Function to generate all non-empty subsets of array\nfunction generatePowerSet(array) {\n  var result = [];\n\n  for (var i = 1; i &lt; (1 &lt;&lt; array.length); i++) {\n    var subset = [];\n    for (var j = 0; j &lt; array.length; j++)\n      if (i & (1 &lt;&lt; j))\n      subset.push(array[j]);\n\n    result.push(subset);\n  }\n\n  return result;\n}\n\n// Function to create target survey\nfunction createHPSurvey() {\n  var form = FormApp.create('Harry Potter Books Survey')\n                    .setAllowResponseEdits(false)\n                    .setCollectEmail(false)\n                    .setLimitOneResponsePerUser(true);\n\n  // Add select list\n  var selectList = form.addListItem()\n                       .setTitle('Choose first listed number')\n                       .setHelpText('This simulates random subsetting of books.')\n                       .setRequired(true);\n\n  // Initialize main questions data\n  var questionSingular = 'What is your impression of this Harry Potter BOOK?';\n  var questionPlural   = 'What is your impression of these Harry Potter BOOKS?';\n  var likertScale = ['1 - Poor', '2 - Fair', '3 - Good',\n                     '4 - Very Good', '5 - Excellent'];\n\n  var books = [\"HP and the Philosopher's (Sorcerer's) Stone (#1)\",\n               \"HP and the Chamber of Secrets (#2)\",\n               \"HP and the Prisoner of Azkaban (#3)\",\n               \"HP and the Goblet of Fire (#4)\",\n               \"HP and the Order of the Phoenix (#5)\",\n               \"HP and the Half-Blood Prince (#6)\",\n               \"HP and the Deathly Hallows (#7)\"];\n\n  var allSubsets = generatePowerSet(books);\n\n  // Create pages with all subsets\n  var pages = []; // for collecting the choices in the list item\n  for (var n = 0; n &lt; allSubsets.length; n++) {\n\n    // Make a section for current subset\n    var newPage = form.addPageBreakItem()\n                      .setTitle('Rate books');\n\n    // Set the section to submit after completing (rather than next subset section)\n    newPage.setGoToPage(FormApp.PageNavigationType.SUBMIT)\n\n    // Add question for current subset with scale\n    var question = form.addGridItem()\n                       .setRows(allSubsets[n])\n                       .setColumns(likertScale)\n                       .setRequired(true);\n    if (allSubsets[n].length == 1) {\n      question.setTitle(questionSingular);\n    } else {\n      question.setTitle(questionPlural);\n    }\n\n    // Push our choice to the list select\n    pages.push(selectList.createChoice(n + 1, newPage));\n  }\n\n  // Add all subsets to select list\n  selectList.setChoices(pages);\n}\nThis code should be run into Google Apps Script project. It creates a Google Form named “Harry Potter Books Survey” and stores it on Google Drive.\nUnfortunately, I didn’t find option of adding shuffling programmatically for every question, so I did it manually… one by one. I thought for a while about creating questions not only for every subset but also for its every permutation. This doesn’t really the same way of randomizing because questions with more permutations will be chosen more frequently at the first step.\nVery helpful sources I used to implement this:\n\nBasic code for “branching” form\nOfficial Google Apps Script Reference for Forms\nGenerating all subsets in JavaScript (modified to not include empty subset)\n\n\n\nConclusions\n\nCreating your own data set is hard. But very rewarding.\nDesigning a survey is hard. But very insightful.\nImplementing a dynamic survey in Google Forms is hard. But very JavaScript.\nI still need your help. Please, take part in the survey at this link."
  },
  {
    "objectID": "blog/2023-08-26-floating-windows-in-neovim-can-now-have-footer.html",
    "href": "blog/2023-08-26-floating-windows-in-neovim-can-now-have-footer.html",
    "title": "Floating windows in Neovim can now have footer",
    "section": "",
    "text": "Originally posted on Reddit\n\nAfter merging this PR, nvim_open_win() now supports footer and footer_pos options. They are symmetrical to title and title_pos options, but affect the bottom part of floating window border.\nImportant: this is a part of Neovim 0.10, not current stable Neovim 0.9.1. Will also be available in next Nightly build.\nI am excited to see how Neovm community can leverage this. It seems like an appropriate place to show some help data alongside usual title. I have a few ideas for ‘mini.nvim’ myself :)"
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html",
    "title": "Harry Potter and competition results with comperes",
    "section": "",
    "text": "About a month ago I decided to add interesting data set to my almost finished (hopefully, soon to be on CRAN) comperes package. Data should represent results of some not ordinary competition. After some thought I picked a “competition” between Harry Potter books with a goal eventually to rate them from worst to best. After a series of events I ended up creating data myself. You can read more about that in my previous post.\nPost and survey in general were popularized mostly among R users with R-bloggers (which gave me ~53 respondents), Twitter (which added the rest) and Reddit (which added ~0 people as post was deleted soon after publication). Survey managed to attract 182 respondents. I want to greatly thank all people who took their time to take part in and spread a word about my survey. Special thanks goes to Mara Averick who started a Twitter wave.\nThis post has two goals:\n\nPresent and explore results of the survey.\nDemonstrate basic functionality of comperes package. To learn more go to its README and vignettes."
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#data-preparation",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#data-preparation",
    "title": "Harry Potter and competition results with comperes",
    "section": "Data preparation",
    "text": "Data preparation\nhp_suvery is a tibble (enhanced data frame) and has the following columns:\n\nperson &lt;int&gt; : Identifier of a person.\nbook &lt;chr&gt; : Identifier of a Harry Potter book. Its values are of the form “HP_x” where “x” represents book’s number in the series (from 1 to 7).\nscore &lt;chr&gt; : Book’s score. Can be one of “1 - Poor”, “2 - Fair”, “3 - Good”, “4 - Very Good”, “5 - Excellent”.\n\nFor exploration, let’s transform hp_survey for more expressive code and results:\n\nConvert scores to numerical.\nAdd book names.\n\nbook_names &lt;- c(\n    \"Philosopher's (Sorcerer's) Stone (#1)\",\n    \"Chamber of Secrets (#2)\",\n    \"Prisoner of Azkaban (#3)\",\n    \"Goblet of Fire (#4)\",\n    \"Order of the Phoenix (#5)\",\n    \"Half-Blood Prince (#6)\",\n    \"Deathly Hallows (#7)\"\n  )\nbook_name_tbl &lt;- tibble(\n  book = paste0(\"HP_\", 1:7),\n  book_name = factor(book_names, levels = book_names)\n)\n\nhp &lt;- hp_survey %&gt;%\n  # Extract numerical score\n  rename(score_chr = score) %&gt;%\n  mutate(score = as.integer(gsub(\"[^0-9].*$\", \"\", score_chr))) %&gt;%\n  # Add book names\n  left_join(y = book_name_tbl, by = \"book\")\n\nhp\n## # A tibble: 657 x 5\n##   person book  score_chr     score book_name\n##    &lt;int&gt; &lt;chr&gt; &lt;chr&gt;         &lt;int&gt; &lt;fct&gt;\n## 1      1 HP_6  5 - Excellent     5 Half-Blood Prince (#6)\n## 2      1 HP_7  5 - Excellent     5 Deathly Hallows (#7)\n## 3      2 HP_1  3 - Good          3 Philosopher's (Sorcerer's) Stone (#1)\n## 4      2 HP_4  5 - Excellent     5 Goblet of Fire (#4)\n## 5      2 HP_5  2 - Fair          2 Order of the Phoenix (#5)\n## # ... with 652 more rows"
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#subset-uniformity",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#subset-uniformity",
    "title": "Harry Potter and competition results with comperes",
    "section": "Subset uniformity",
    "text": "Subset uniformity\nThe first step in the survey was to choose the first element in the randomly shuffled list to simulate generation of random subset from all books. Each of 127 list element was connected to one subset. Lets visualize subset frequency to ensure a good faith of respondents:\n# Compute subset representations\nhp_subsets &lt;- hp %&gt;%\n  arrange(person, book) %&gt;%\n  group_by(person) %&gt;%\n  summarise(subset = paste0(book, collapse = \"-\"))\n\n# Compute the number of actually picked subsets\nn_distinct(hp_subsets$subset)\n## [1] 95\n\n# Visualize\nhp_subsets %&gt;%\n  ggplot(aes(subset)) +\n    geom_bar(fill = hp_pal[\"Gryff\"]) +\n    labs(\n      x = \"Subset\", y = \"Number of times subset was picked\",\n      title = \"Picked subsets have fairly uniform distribution\"\n    ) +\n    scale_x_discrete(labels = NULL) +\n    theme_bar() +\n    theme(axis.ticks.x = element_blank())\n\nSo there are 95 subsets actually picked and their distribution seems reasonably uniform. This is enough for me to confirm that randomization for subsets was successful."
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#book-presence",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#book-presence",
    "title": "Harry Potter and competition results with comperes",
    "section": "Book presence",
    "text": "Book presence\nOther important thing to explore is number of times book was actually rated:\nhp %&gt;%\n  ggplot(aes(book_name)) +\n    geom_bar(fill = hp_pal[\"Huffl\"]) +\n    # Cool way to wrap labels for a given width\n    scale_x_discrete(labels = function(x) str_wrap(x, width = 15)) +\n    labs(\n      x = \"\", y = \"Number of present scores\",\n      title = \"Some books were rated more times than others\",\n      subtitle = \"But it seems fine\"\n    ) +\n    theme_bar()"
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#book-scores",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#book-scores",
    "title": "Harry Potter and competition results with comperes",
    "section": "Book scores",
    "text": "Book scores\nThe most obvious way to summarise book “performance” is its mean score of numerical representation of scale. Using mean is not harmful in this study as no outlier can be present.\nhp_book_score &lt;- hp %&gt;%\n  group_by(book_name) %&gt;%\n  summarise(mean_score = round(mean(score), digits = 2)) %&gt;%\n  arrange(desc(mean_score))\n\nhp_book_score\n## # A tibble: 7 x 2\n##   book_name                             mean_score\n##   &lt;fct&gt;                                      &lt;dbl&gt;\n## 1 Prisoner of Azkaban (#3)                    4.19\n## 2 Half-Blood Prince (#6)                      4.13\n## 3 Goblet of Fire (#4)                         4.00\n## 4 Deathly Hallows (#7)                        3.96\n## 5 Philosopher's (Sorcerer's) Stone (#1)       3.91\n## 6 Order of the Phoenix (#5)                   3.90\n## 7 Chamber of Secrets (#2)                     3.55\nSo, “the best” book seems to be “Harry Potter and the Prisoner of Azkaban (#3)”.\nFor more understanding of results, lets also visualize score distribution.\nhp %&gt;%\n  # Compute share of score per book\n  count(book_name, score) %&gt;%\n  group_by(book_name) %&gt;%\n  mutate(share = n / sum(n)) %&gt;%\n  ungroup() %&gt;%\n  # Visualize\n  ggplot() +\n    geom_col(\n      aes(score, share, colour = score, fill = score),\n      show.legend = FALSE\n    ) +\n    geom_text(\n      data = hp_book_score,\n      mapping = aes(label = paste0(\"Mean = \", mean_score)),\n      x = -Inf, y = Inf,\n      hjust = -0.05, vjust = 1.3\n    ) +\n    facet_wrap(~ book_name) +\n    scale_x_continuous(\n      breaks = 1:5,\n      labels = c(\"1\\nPoor\", \"2\\nFair\", \"3\\nGood\",\n                 \"4\\nVery\\nGood\", \"5\\nExcellent\")\n    ) +\n    scale_fill_gradient(low = hp_pal[\"Raven\"], high = hp_pal[\"Raven_light\"]) +\n    scale_colour_gradient(low = hp_pal[\"Raven\"], high = hp_pal[\"Raven_light\"]) +\n    labs(\n      x = \"\", y = \"Score share per book\",\n      title = '\"Prisoner of Azkaban (#3)\" seems to be \"the best\" HP book',\n      caption = \"@echasnovski\"\n    ) +\n    theme_bar()"
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#formats-of-comperes",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#formats-of-comperes",
    "title": "Harry Potter and competition results with comperes",
    "section": "Formats of comperes",
    "text": "Formats of comperes\nUnderstanding of competition is quite general: it is a set of games (abstract event) in which players (abstract entity) gain some abstract scores (typically numeric). Inside games all players are treated equally. The most natural example is sport results, however not the only one. For example, product rating can be considered as a competition between products as “players”. Here a “game” is a customer that reviews a set of products by rating them with numerical “score” (stars, points, etc.).\nIn case of Harry Potter Books Survey results “game” is an act of respondent taking part in survey, “player” - Harry Potter book, “score” - discrete scale values converted to numerical score from 1 to 5.\nIn comperes there are two supported formats of competition results:\n\nLong format. It is the most abstract way of presenting competition results. Basically, it is a data frame (or tibble) with columns game (game identifier), player (player identifier) and score where each row represents the score of particular player in particular game. One game can consist from variable number of players which makes this format more usable. Extra columns are allowed.\nWide format is a more convenient way to store results with fixed number of players in a game. Each row represents scores of all players in particular game. Data should be organized in pairs of columns “player”-“score”. Identifier of a pair should go after respective keyword and consist only from digits. For example: player1, score1, player2, score2. Order doesn’t matter. Column game is optional. Extra columns are also allowed.\n\nProgrammatically these formats are implemented as S3 classes longcr and widecr respectively. Essentially, they are tibbles with fixed structure. Objects of these classes should be created using functions as_longcr() and as_widecr() which also do conversions to other format."
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#conversion",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#conversion",
    "title": "Harry Potter and competition results with comperes",
    "section": "Conversion",
    "text": "Conversion\nhp_survey presents results in long format.\nhp_cr &lt;- hp_survey %&gt;%\n  transmute(\n    game = person, player = book,\n    score = as.integer(gsub(\"[^0-9].*$\", \"\", score))\n  ) %&gt;%\n  as_longcr()\n\nhp_cr\n## # A longcr object:\n## # A tibble: 657 x 3\n##    game player score\n##   &lt;int&gt; &lt;chr&gt;  &lt;int&gt;\n## 1     1 HP_6       5\n## 2     1 HP_7       5\n## 3     2 HP_1       3\n## 4     2 HP_4       5\n## 5     2 HP_5       2\n## # ... with 652 more rows\nHere is the demonstration of conversion to wide format. It detects the maximum number of players in a game, which is 7, and assumes that data is missing in games with less number of players.\nas_widecr(hp_cr)\n## # A widecr object:\n## # A tibble: 182 x 15\n##    game player1 score1 player2 score2 player3 score3 player4 score4\n##   &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n## 1     1 HP_6         5 HP_7         5 &lt;NA&gt;        NA &lt;NA&gt;        NA\n## 2     2 HP_1         3 HP_4         5 HP_5         2 HP_6         4\n## 3     3 HP_1         3 HP_3         4 HP_5         1 &lt;NA&gt;        NA\n## 4     4 HP_6         5 HP_7         5 &lt;NA&gt;        NA &lt;NA&gt;        NA\n## 5     5 HP_4         4 HP_5         3 &lt;NA&gt;        NA &lt;NA&gt;        NA\n## # ... with 177 more rows, and 6 more variables: player5 &lt;chr&gt;,\n## #   score5 &lt;int&gt;, player6 &lt;chr&gt;, score6 &lt;int&gt;, player7 &lt;chr&gt;, score7 &lt;int&gt;"
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#functionality-of-comperes",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#functionality-of-comperes",
    "title": "Harry Potter and competition results with comperes",
    "section": "Functionality of comperes",
    "text": "Functionality of comperes\nHead-to-Head value is a summary statistic of direct confrontation between two players. It is assumed that this value can be computed based only on the players’ matchups (results for ordered pairs of players from one game). In other words, every game is converted into series of “subgames” between ordered pairs of players (including selfplay) which is stored as widecr object. After that, summary of item, defined by columns player1 and player2, is computed.\ncomperes has function get_matchups() for computing matchups:\nget_matchups(hp_cr)\n## # A widecr object:\n## # A tibble: 2,697 x 5\n##    game player1 score1 player2 score2\n##   &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt;\n## 1     1 HP_6         5 HP_6         5\n## 2     1 HP_6         5 HP_7         5\n## 3     1 HP_7         5 HP_6         5\n## 4     1 HP_7         5 HP_7         5\n## 5     2 HP_1         3 HP_1         3\n## # ... with 2,692 more rows\nTo compute multiple Head-to-Head values, use h2h_long() supplying competition results and summarizing expressions in dplyr::summarise() fashion. They will be applied to a data frame of matchups.\nhp_cr_h2h &lt;- hp_cr %&gt;% h2h_long(\n  # Number of macthups\n  n = n(),\n  # Number of wins plus half the number of ties\n  # num_wins() is a function from comperes to compute number of times\n  # first score is bigger than second one\n  num_wins = num_wins(score1, score2, half_for_draw = TRUE),\n  # Mean rating of a book scored in matchups with other books\n  mean_score = mean(score1),\n  # Mean rating difference of books scored in direct matchups\n  mean_score_diff = mean(score1 - score2)\n) %&gt;%\n  mutate_if(is.numeric, funs(round(., 2)))\n\nhp_cr_h2h\n## # A long format of Head-to-Head values:\n## # A tibble: 49 x 6\n##   player1 player2     n num_wins mean_score mean_score_diff\n##   &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;\n## 1 HP_1    HP_1      88.     44.0       3.91          0.\n## 2 HP_1    HP_2      42.     29.5       3.88          0.500\n## 3 HP_1    HP_3      51.     19.5       3.92         -0.390\n## 4 HP_1    HP_4      48.     24.0       3.79          0.0400\n## 5 HP_1    HP_5      42.     21.5       3.79          0.\n## # ... with 44 more rows\nSo here we see, for example, that HP_1 and HP_2 had 42 matchups, i.e. they were rated by the same person 42 times. HP_1 “won” 29.5 (respecting ties) times, gained mean score of 3.88 in those matchups and had, on average, 0.5 points more.\nThere is also an h2h_mat() function which computes a matrix of Head-to-Head values for one expression.\nhp_cr %&gt;% h2h_mat(num_wins(score1, score2, half_for_draw = TRUE))\n## # A matrix format of Head-to-Head values:\n##      HP_1 HP_2 HP_3 HP_4 HP_5 HP_6 HP_7\n## HP_1 44.0 29.5 19.5 24.0 21.5 17.0 24.0\n## HP_2 12.5 40.0 12.0 11.5 10.5 12.0 19.0\n## HP_3 31.5 32.0 49.0 31.5 28.0 25.0 33.5\n## HP_4 24.0 33.5 26.5 49.5 23.5 30.5 31.5\n## HP_5 20.5 25.5 15.0 24.5 42.0 23.0 24.5\n## HP_6 25.0 30.0 20.0 27.5 24.0 50.0 34.0\n## HP_7 26.0 34.0 21.5 29.5 25.5 26.0 54.0\nFor more convenient usage, comperes has a list h2h_funs of some common Head-to-Head functions stored as expressions. To use them you need a little bit of rlang’s unquoting magic.\nh2h_funs[1:3]\n## $mean_score_diff\n## mean(score1 - score2)\n##\n## $mean_score_diff_pos\n## max(mean(score1 - score2), 0)\n##\n## $mean_score\n## mean(score1)\n\nhp_cr %&gt;% h2h_long(!!! h2h_funs)\n## # A long format of Head-to-Head values:\n## # A tibble: 49 x 11\n##   player1 player2 mean_score_diff mean_score_diff_pos mean_score\n##   &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;               &lt;dbl&gt;      &lt;dbl&gt;\n## 1 HP_1    HP_1             0.                  0.           3.91\n## 2 HP_1    HP_2             0.500               0.500        3.88\n## 3 HP_1    HP_3            -0.392               0.           3.92\n## 4 HP_1    HP_4             0.0417              0.0417       3.79\n## 5 HP_1    HP_5             0.                  0.           3.79\n## # ... with 44 more rows, and 6 more variables: sum_score_diff &lt;int&gt;,\n## #   sum_score_diff_pos &lt;dbl&gt;, sum_score &lt;int&gt;, num_wins &lt;dbl&gt;,\n## #   num_wins2 &lt;dbl&gt;, num &lt;int&gt;"
  },
  {
    "objectID": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#harry-potter-books",
    "href": "blog/2018-05-09-harry-potter-and-competition-results-with-comperes.html#harry-potter-books",
    "title": "Harry Potter and competition results with comperes",
    "section": "Harry Potter books",
    "text": "Harry Potter books\nHead-to-Head “performance” of Harry Potter books is summarised in the following plot:\nhp_cr_h2h %&gt;%\n  gather(h2h_fun, value, -player1, -player2) %&gt;%\n  # Manually produce a dummy colour variable to use in facets\n  group_by(h2h_fun) %&gt;%\n  mutate(col = (value - min(value)) / (max(value) - min(value))) %&gt;%\n  ungroup() %&gt;%\n  # Make factors for correct orders\n  mutate(\n    player1 = factor(player1, levels = rev(sort(unique(player1)))),\n    player2 = factor(player2, levels = sort(unique(player2))),\n    h2h_fun = factor(h2h_fun,\n                     levels = c(\"n\", \"num_wins\",\n                                \"mean_score\", \"mean_score_diff\")),\n    h2h_fun = recode(\n      h2h_fun,\n      n = \"Number of matchups (ratings by common person)\",\n      num_wins = 'Number of \"wins\" in matchups (half for ties)',\n      mean_score = \"Mean score in matchups\",\n      mean_score_diff = \"Mean score difference in matchups\"\n    )\n  ) %&gt;%\n  # Visualize\n  ggplot(aes(player1, player2)) +\n    geom_text(\n      aes(label = value, colour = col),\n      size = 5, fontface = \"bold\", show.legend = FALSE\n    ) +\n    facet_wrap(~ h2h_fun, scales = \"free\") +\n    # To coordinate well with matrix form of Head-to-Head results\n    coord_flip() +\n    scale_colour_gradient(low = hp_pal[\"Slyth\"], high = hp_pal[\"Gryff\"]) +\n    labs(\n      x = \"\", y = \"\",\n      title = \"Head-to-Head performance of Harry Potter books\",\n      subtitle = paste0(\n        '\"HP_x\" means Harry Potter book number \"x\" in series\\n',\n        \"Numbers are Head-to-Head values of book in row against book in column\"\n      ),\n      caption = \"@echasnovski\"\n    ) +\n    theme_classic() +\n    theme(strip.text = element_text(face = \"bold\"))\n\nThere is a lot of information hidden in this plot. The most obvious discoveries:\n\nIt happened that book “HP_7” (“Deathly Hallows”) was rated with “HP_4” (“Goblet of Fire”) by one person the most: 61 times.\n“HP_7” scored over “HP_2” (“Chamber of Secrets”) the most wins (34, half for ties) as did “HP_6” (“Half-Blood Prince”) over “HP_7”.\nBook “HP_6” made the highest mean score of 4.36 in matchups with “HP_2”, which is bigger by 0.23 from its overall mean score.\nIn terms of score differences, “HP_3” (“Prisoner of Azkaban”) did best in matchups with “HP_2”, scoring on average 0.77 points more. This pair also represents “the best” and “the worst” books in terms of mean score."
  },
  {
    "objectID": "blog/2022-07-20-i-contributed-to-toprated-neovim-color-schemes.html",
    "href": "blog/2022-07-20-i-contributed-to-toprated-neovim-color-schemes.html",
    "title": "I contributed to (mostly) 14 top-rated Neovim color schemes. Here are some observations",
    "section": "",
    "text": "Originally posted on Reddit\nSome time ago I decided to improve user experience for my mini.nvim plugin by adding its explicit support to popular color schemes. Although out of the box experience should be pretty good (most coloring is done by linking to carefully selected builtin highlight groups), having them tailored to color scheme choices is certainly better.\nSo I went to rockerBoo/awesome-neovim and selected a handful of color schemes subjectively judging by the number of Github stars, time since latest commit, and language of plugin (Lua preferred). It was not a very scientifically rigorous process which lead to 14 chosen color schemes.\nHere is a table of how those contributions went. Now, I want to state right away that this is no way, shape, or form should be used to judge how actively those maintainers watch for their projects. Everyone is different and has limited amount of time in a day. This is only a data about my experience at some point in past.\n\n\n\nColor scheme PR\nLanguage\nTime to first reaction\nOutcome\n\n\n\n\nbluz71/vim-moonfly-colors\nVimscript\n~ 1 day\nRevised and accepted\n\n\nbluz71/vim-nightfly-guicolors\nVimscript\n~ 1 day\nRevised and accepted\n\n\ncatppuccin/nvim\nLua\n~ 6 days\nMerged\n\n\nEdenEast/nightfox.nvim\nLua\n~ 1 day\nMerged\n\n\nfolke/tokyonight.nvim\nLua\n&gt; 14 days (no reaction yet)\nOpen\n\n\nmarko-cerovac/material.nvim\nLua\n~ 3 days\nMerged\n\n\nnavarasu/onedark.nvim\nLua\n~ 1 day\nMerged\n\n\nprojekt0n/github-nvim-theme\nLua\n~ 1 day\nMerged\n\n\nrebelot/kanagawa.nvim\nLua\n~ 7 days\nWaiting for approval\n\n\nsainnhe/edge\nVimscript\n~ 1 day\nMerged\n\n\nsainnhe/everforest\nVimscript\n~ 1 day\nMerged\n\n\nsainnhe/gruvbox-material\nVimscript\n~ 1 day\nMerged\n\n\nsainnhe/sonokai\nVimscript\n~ 1 day\nMerged\n\n\nshaunsingh/nord.nvim\nLua\n~ 1 day\nMerged\n\n\n\nSome personal observations after all this was done:\n\nHaving the easily found documentation about how to add plugin support is really useful. Most of contributions were pretty straightforward, but I did end up searching for commits to get a sense of preferred workflow.\nHaving the ability to link new highlight groups to existing ones makes contributing considerably easier. I understand its possible absence, but nevertheless.\nHaving explicit support for many plugins really helps with adding new ones. It adds more information about design principles of color scheme.\nMy two personal favorites from aesthetic point of view are ‘Terafox’ from ‘EdenEast/nightfox.nvim’ and ‘sainnhe/everforest’. This even prompted me to create a new color scheme for ‘mini.nvim’ which I am currently testing.\n\nHope it was interesting. Also, please consider adding similar ‘mini.nvim’ explicit support for other color schemes (yours or ones you are using). Thanks!"
  },
  {
    "objectID": "blog/2022-07-07-stylua-now-supports-collapsing-simple-statements.html",
    "href": "blog/2022-07-07-stylua-now-supports-collapsing-simple-statements.html",
    "title": "StyLua now supports collapsing simple statements",
    "section": "",
    "text": "Originally posted on Reddit\nStarting from version 0.14.0, StyLua (the Lua code formatter in Neovim world) implements option collapse_simple_statement. From release notes:\n\nIt can take the values Never (default), FunctionOnly, ConditionalOnly or Always. When enabled, “simple” functions or if statements (ones where they only return a value or have a simple statement such as a function call) will be collapsed onto a single line where possible.\n\nSo now with collapse_simple_statement = \"Always\" instead of this …\nlocal f = function()\n  return true\nend\n\nlocal g = function()\n  f()\nend\n\nif is_bad then\n  return\nend\n\nif is_good then\n  a = 1\nend\n… it will be formatted as …\nlocal f = function() return true end\n\nlocal g = function() f() end\n\nif is_bad then return end\n\nif is_good then a = 1 end\nThis is great news for Neovim plugin authors and general users as this type of code is very common. Having it on single line improves readability (in my opinion).\nI happily switched to this option. Although using 120 width seems to be quite high sometimes in these cases, so I might start using 100 or even 80.\n(Note: I am not an author, JohnnyMorganz is. He is a thoughtful creator and maintainer of StyLua.)."
  },
  {
    "objectID": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html",
    "href": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html",
    "title": "How to Scrape Pdf and Rmd to Get Inspiration",
    "section": "",
    "text": "This post is a story about how I came up with an idea of QuestionFlow name by performing text analysis. Beware of a lot of code.\n\n\nMy name is Evgeni and I am a mathematician with strong passion for data analysis and R.\nThe first experience in the field came to me in the early 2013, in the middle of my university graduate year. I decided to take a “Data Analysis” course at Coursera (taught by amazing Jeff Leek) which later transformed into Data Science Specialization from John Hopkins University. After watching some videos, it came to me that knowledge of some weird programming language called R is needed in order to complete the course. That insight led me to another Coursera course “Computing for Data Analysis” (offered by no less amazing Roger D. Peng). Thanks to them, I fell in love with data analysis and R, which really helped me to get a wonderful job as mathematician-analyst after graduation.\nYears went by until autumn 2016 (about a year ago), when I suddenly discovered the R for Data Science book and other works by Hadley Wickham. At the time I had heard only about ggplot2 but nothing more, as base R was pretty enough for my work. After this discovery I fell into a rabbit hole: tidyverse, R Markdown, reproducible research, writing R packages and so on. Finally, after blogdown reached CRAN, I realized that it is time to give back to the great R community by being more web present. Basically it means creating and maintaining own blog/site.\n\n\n\nBeing perfectionist, I spent quite some time figuring out the name of my future site. In ideal world it should have the next features:\n\nBe representative of the site content.\nBe memorable to audience. In general it means to have a pleasant pronunciation, to be quite short and to evoke colourful images in people’s minds.\nBe available as domain name and twitter account.\n\nI had some possible versions of the name, but none of them actually felt right. Eventually, I came up with an idea of giving tribute to three persons mentioned above for sharing their knowledge with the world (and me in particular). The goal was to take one book authored by each of them and perform some text analysis in order to get inspiration. This also felt like a good opportunity to try tidytext package.\nNext books were chosen (which had big influence on me):\n\nThe Elements of Data Analytic Style (EDAS) by Jeff Leek.\nThe Art of Data Science (ADS) by Roger D. Peng and Elizabeth Matsui.\nR for Data Science (R4DS) by Garrett Grolemund and Hadley Wickham.\n\nI highly recommend everybody interested in data analysis to read these books. All of them can be obtained for free (but I strongly encourage to reward authors): EDAS and ADS as pdf files, R4DS as bookdown repository from Hadley’s github."
  },
  {
    "objectID": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#introduction",
    "href": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#introduction",
    "title": "How to Scrape Pdf and Rmd to Get Inspiration",
    "section": "",
    "text": "My name is Evgeni and I am a mathematician with strong passion for data analysis and R.\nThe first experience in the field came to me in the early 2013, in the middle of my university graduate year. I decided to take a “Data Analysis” course at Coursera (taught by amazing Jeff Leek) which later transformed into Data Science Specialization from John Hopkins University. After watching some videos, it came to me that knowledge of some weird programming language called R is needed in order to complete the course. That insight led me to another Coursera course “Computing for Data Analysis” (offered by no less amazing Roger D. Peng). Thanks to them, I fell in love with data analysis and R, which really helped me to get a wonderful job as mathematician-analyst after graduation.\nYears went by until autumn 2016 (about a year ago), when I suddenly discovered the R for Data Science book and other works by Hadley Wickham. At the time I had heard only about ggplot2 but nothing more, as base R was pretty enough for my work. After this discovery I fell into a rabbit hole: tidyverse, R Markdown, reproducible research, writing R packages and so on. Finally, after blogdown reached CRAN, I realized that it is time to give back to the great R community by being more web present. Basically it means creating and maintaining own blog/site."
  },
  {
    "objectID": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#goal",
    "href": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#goal",
    "title": "How to Scrape Pdf and Rmd to Get Inspiration",
    "section": "",
    "text": "Being perfectionist, I spent quite some time figuring out the name of my future site. In ideal world it should have the next features:\n\nBe representative of the site content.\nBe memorable to audience. In general it means to have a pleasant pronunciation, to be quite short and to evoke colourful images in people’s minds.\nBe available as domain name and twitter account.\n\nI had some possible versions of the name, but none of them actually felt right. Eventually, I came up with an idea of giving tribute to three persons mentioned above for sharing their knowledge with the world (and me in particular). The goal was to take one book authored by each of them and perform some text analysis in order to get inspiration. This also felt like a good opportunity to try tidytext package.\nNext books were chosen (which had big influence on me):\n\nThe Elements of Data Analytic Style (EDAS) by Jeff Leek.\nThe Art of Data Science (ADS) by Roger D. Peng and Elizabeth Matsui.\nR for Data Science (R4DS) by Garrett Grolemund and Hadley Wickham.\n\nI highly recommend everybody interested in data analysis to read these books. All of them can be obtained for free (but I strongly encourage to reward authors): EDAS and ADS as pdf files, R4DS as bookdown repository from Hadley’s github."
  },
  {
    "objectID": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#tidy-pdf",
    "href": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#tidy-pdf",
    "title": "How to Scrape Pdf and Rmd to Get Inspiration",
    "section": "Tidy pdf",
    "text": "Tidy pdf\nHere is the code of function for tidying pdf file at the file path. At first, it, with help of pdftools package, converts pdf file into list of strings (text per page). Then this list is converted into tibble with the following structure:\n\npage &lt;int&gt; : Word’s page number.\nline &lt;int&gt; : Word’s line number on page (empty lines are ignored).\nword &lt;chr&gt; : Word.\n\ntidy_pdf &lt;- function(file) {\n  pages_list &lt;- file %&gt;%\n    pdftools::pdf_text() %&gt;%\n    lapply(. %&gt;% strsplit(\"\\n\") %&gt;% `[[`(1))\n\n  lapply(seq_along(pages_list), function(i) {\n    page_lines &lt;- pages_list[[i]]\n    n_lines &lt;- length(page_lines)\n\n    tibble(\n      page = rep(i, n_lines),\n      line = seq_len(n_lines),\n      text = page_lines\n    ) %&gt;%\n      tidytext::unnest_tokens(word, text, token = \"words\")\n  }) %&gt;%\n    bind_rows()\n}\nThis function is enough to scrape text from both EDAS and ADS."
  },
  {
    "objectID": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#tidy-rmd",
    "href": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#tidy-rmd",
    "title": "How to Scrape Pdf and Rmd to Get Inspiration",
    "section": "Tidy Rmd",
    "text": "Tidy Rmd\nThe R4DS book can be obtained as a collection of Rmd files. The goal is to extract text, not code or metadata. Unfortunately, I didn’t find an easy way to do that with knitr or rmarkdown, so I wrote function myself. The tidy_rmd() reads file line by line at location file and collapses them into one string. After that, with some magic of regular expressions and str_replace_all() from stringr, it removes YAML header, code and LaTeX blocks. Note that this function is designed to only handle Rmd files with relatively simple formatting and can return undesirable output in some edge cases.\nThe output is a tibble with the following structure:\n\nname &lt;int&gt; : Document name which is given as function argument. Default is file’s base name without extension.\nword &lt;chr&gt; : Word.\n\ntidy_rmd &lt;- function(file, name = file_base_name(file)) {\n  file_string &lt;- file %&gt;%\n    readLines() %&gt;%\n    paste0(collapse = \" \") %&gt;%\n    # Remove YAML header\n    str_replace_all(\"^--- .*?--- \", \"\") %&gt;%\n    # Remove code\n    str_replace_all(\"```.*?```\", \"\") %&gt;%\n    str_replace_all(\"`.*?`\", \"\") %&gt;%\n    # Remove LaTeX\n    str_replace_all(\"[^\\\\\\\\]\\\\$\\\\$.*?[^\\\\\\\\]\\\\$\\\\$\", \"\") %&gt;%\n    str_replace_all(\"[^\\\\\\\\]\\\\$.*?[^\\\\\\\\]\\\\$\", \"\")\n\n  tibble(name = name, text = file_string) %&gt;%\n    tidytext::unnest_tokens(word, text, token = \"words\")\n}\n\nfile_base_name &lt;- function(file) {\n  file %&gt;%\n    basename() %&gt;%\n    str_replace_all(\"\\\\..*?$\", \"\")\n}"
  },
  {
    "objectID": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#filter-good-words",
    "href": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#filter-good-words",
    "title": "How to Scrape Pdf and Rmd to Get Inspiration",
    "section": "Filter good words",
    "text": "Filter good words\nOnly meaningful words should be used in text analysis. They are:\n\nNot stop words. tidytext package has a data frame stop_words with the most common stop words.\nWords that contain only alphabetic characters. This feature is, of course, arguable. I decided to add it because otherwise data can contain some links and words with ' : “we’ll”, “word’s” etc. Moreover, this is not very strict project, so I think it is appropriate decision.\n\nWords scraped from Rmd files should be processed a little more carefully: they can contain emphasis characters “_” and “*” at the beginning or end. So, before filtering good words, these symbols should be removed.\nremove_md_emphasis &lt;- function(x) {\n  str_replace_all(x, \"^[_*]+|[_*]+$\", \"\")\n}\n\nfilter_good_words &lt;- function(word_tbl) {\n  word_tbl %&gt;%\n    anti_join(tidytext::stop_words, by = \"word\") %&gt;%\n    filter(str_detect(word, pattern = \"^[[:alpha:]]+$\"))\n}\nfilter_good_words() should be applied to data frames with character column word."
  },
  {
    "objectID": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#scrape-books",
    "href": "blog/2017-10-13-how-to-scrape-pdf-and-rmd-to-get-inspiration.html#scrape-books",
    "title": "How to Scrape Pdf and Rmd to Get Inspiration",
    "section": "Scrape books",
    "text": "Scrape books\n\nJHU pdf files\nprepare_book &lt;- function(book_name) {\n  # File should be in the data-raw folder of working directory\n  file.path(\"data-raw\", paste0(book_name, \".pdf\")) %&gt;%\n    tidy_pdf() %&gt;%\n    filter_good_words() %&gt;%\n    mutate(\n      id = seq_len(n()),\n      book = rep(book_name, n())\n    ) %&gt;%\n    select(id, book, everything())\n}\n\nedas &lt;- prepare_book(\"EDAS\")\nads &lt;- prepare_book(\"ADS\")\nedas and ads have the following structure:\n\nid &lt;int&gt; : Index of word inside the book.\nbook &lt;chr&gt; : Name of the book.\npage, line, word : the same as in tidy_pdf() output.\n\n\n\nR4DS bookdown folder\nIn order to obtain whole R4DS book as one tibble, one should apply tidy_rmd() to all Rmd files listed in ’_bookdown.yml’ file and bind results with dplyr::bind_rows(). This also ensures that words are arranged in the order according to their appearance in the book.\n# Get file names used to knit the book\n# r4ds folder should be in the data-raw folder of working directory\nr4ds_file_names &lt;- readLines(file.path(\"data-raw\", \"r4ds\", \"_bookdown.yml\")) %&gt;%\n  paste0(collapse = \" \") %&gt;%\n  str_extract_all('\\\\\".*\\\\.[Rr]md\\\\\"') %&gt;%\n  `[[`(1) %&gt;%\n  str_replace_all('\"', '') %&gt;%\n  str_split(\",[:space:]*\") %&gt;%\n  `[[`(1)\n\n# Orginize book data into tibble\nr4ds_pages &lt;- tibble(\n  page = seq_len(length(r4ds_file_names)),\n  file = r4ds_file_names,\n  pageName = file_base_name(r4ds_file_names)\n)\n\n# Scrape book\nr4ds &lt;- file.path(\"data-raw\", \"r4ds\", r4ds_pages[[\"file\"]]) %&gt;%\n  lapply(tidy_rmd) %&gt;%\n  bind_rows() %&gt;%\n  rename(pageName = name) %&gt;%\n  # Remove md emphasis before filtering words with only alphabetic characters\n  mutate(word = remove_md_emphasis(word)) %&gt;%\n  filter_good_words() %&gt;%\n  mutate(\n    id = seq_len(n()),\n    book = rep(\"R4DS\", n())\n  ) %&gt;%\n  left_join(y = r4ds_pages %&gt;% select(page, pageName),\n            by = \"pageName\") %&gt;%\n  select(id, book, page, pageName, word)\nDue to html format of r4ds, one file represents both chapter and page. For consistency with edas and ads, name ‘page’ was chosen. With this, r4ds has the same structure as JHU pdf books but column line is replaced with pageName (the name chapter/page file)."
  },
  {
    "objectID": "blog/2022-12-08-results-of-neovim-builtin-options-survey.html",
    "href": "blog/2022-12-08-results-of-neovim-builtin-options-survey.html",
    "title": "Results of “Neovim built-in options survey”",
    "section": "",
    "text": "Originally posted on Reddit\n\nHello, Neovim users!\nAround two weeks ago I decided to make a Neovim built-in options survey. Now it is closed, as it doesn’t seem to have an intensive answer supply (last one was more than 24 hours ago).\nThere were total 227 legible answers. Not as many as I had hoped starting it, but it is what it is.\nMain summary of basic options are in the post image. To answer questions in the survey announcement:\n\nWhat Leader key is used the most? - Space.\nTabs or spaces? - Spaces (based on 82% of expandtab=true)\nAbsolute, relative, or no line numbers? - Seems like both absolute and relative. But probably more absolute ones, based on 86% of number=true.\nTraditional or global statusline? - Global, but with a tight margin.\nPermanent tabline or not? - Not permanent, default one.\nUse persistent undo or not? - Yes to persistent undo\nshowmode or noshowmode? - noshowmode\nwrap or nowrap? - wrap.\n\nHere is a gist with full results along with description of how to read them and scripts used\nI also created a GitHub issue in Neovim repository to discuss possibility of default values change. If you agree with changing options which reached 80% for some non-default value (as per commentary of Justin M. Keys), please upvote initial issue message.\nThanks for reading and participating!"
  },
  {
    "objectID": "blog/2021-08-20-useful-functions-to-explore-lua-objects.html",
    "href": "blog/2021-08-20-useful-functions-to-explore-lua-objects.html",
    "title": "Useful functions to explore Lua objects",
    "section": "",
    "text": "Originally posted on Reddit\nTo look at contents of Lua object a you can execute :lua print(vim.inspect(a)). This will print content inside command line. Following nvim-lua-guide’s tip (edit: after making PR to nvim-lua-guide, it is currently in sync with edited version of this post), this can be wrapped into _G.dump() function and become :lua dump(a) (and even :lua dump(a, b)). However, in most cases it doesn’t print nil, which is a shame. This can be solved by sticking with single argument instead of ..., but it proved useful in certain cases. So I came up with alternative implementation and decided to share with everyone (edit: renamed previous dump for a somewhat more pleasant name):\nfunction _G.put(...)\n  local objects = {}\n  for i = 1, select('#', ...) do\n    local v = select(i, ...)\n    table.insert(objects, vim.inspect(v))\n  end\n\n  print(table.concat(objects, '\\n'))\n  return ...\nend\nNow :lua put(nil) will actually print nil instead of just doing nothing. Also :lua put(nil, 1, nil) will print nil, 1, nil on separate lines (instead of nil 1).\nBut there is more. Sometimes you want to explore a big nested table (like LSP related stuff). It would be nicer to explore as regular text. And so put_text() was born:\nfunction _G.put_text(...)\n  local objects = {}\n  for i = 1, select('#', ...) do\n    local v = select(i, ...)\n    table.insert(objects, vim.inspect(v))\n  end\n\n  local lines = vim.split(table.concat(objects, '\\n'), '\\n')\n  local lnum = vim.api.nvim_win_get_cursor(0)[1]\n  vim.fn.append(lnum, lines)\n  return ...\nend\nWhen called, this will try to add inspection content under the current cursor position. So now if you want to conveniently explore all fields of vim.loop, just execute :lua put_text(vim.loop).\nHope this will help somebody.\nP.S.: To use put() and put_text() inside Neovim session, you need to source this Lua code. Easiest way is to put it inside Lua files sourced on startup (‘init.lua’, for example), and you are good to go."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html",
    "href": "blog/2019-08-08-arguments-of-stats-density.html",
    "title": "Arguments of stats::density()",
    "section": "",
    "text": "In R, one of the “go to” functions for kernel density estimation is density() from base R package ‘stats’. Given numeric sample, it returns a set of x-y pairs on estimated density curve. It is also a main “workhorse” for estimating continuous distributions in my ‘pdqr’ package.\nHowever, output of density() highly depends on values of its arguments. Some of them define kernel density estimation algorithm, and the others are responsible for different properties of output set of x-y pairs.\nIn this post I illustrate with animations how changing arguments of density() changes the output."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#bandwidth-bw",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#bandwidth-bw",
    "title": "Arguments of stats::density()",
    "section": "Bandwidth bw",
    "text": "Bandwidth bw\nArgument bw is responsible for computing bandwith of kernel density estimation: one of the main parameters that greatly affect the output. It can be specified as either algorithm of computation or directly as number. Because actual bandwidth is computed as adjust*bw (adjust is another density() argument, which is explored in the next section), here we will see how different algorithms compute bandwidths, and the effect of changing numeric value of bandwidth will be shown in section about adjust.\nThere are 5 available algorithms: “nrd0”, “nrd”, “ucv”, “bcv”, “SJ”. Here is an animation of their effect:\ndensity_anim(bw = c(\"nrd0\", \"nrd\", \"ucv\", \"bcv\", \"SJ\"), duration = 5)\n\nAs you can see, density curve changes, but not drastically. At least the whole shape seems to be somewhat preserved."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#adjusting-bandwidth-adjust",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#adjusting-bandwidth-adjust",
    "title": "Arguments of stats::density()",
    "section": "Adjusting bandwidth adjust",
    "text": "Adjusting bandwidth adjust\nTo easily “specify values like ‘half the default’ bandwidth”, there is an argument adjust. Bigger values indicate greater bandwidth that is actually used. Zero results into zero bandwidth (so never should be used), one (default) - into originally computed by chosen algorithm.\ndensity_anim(adjust = seq(0.1, 2, by = 0.05))\n\nChanging adjust leads to very noticeable changes in output shape of density curve: bigger values give smoother curves."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#kernel-type-kernel",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#kernel-type-kernel",
    "title": "Arguments of stats::density()",
    "section": "Kernel type kernel",
    "text": "Kernel type kernel\nArgument kernel defines the shape of kernel which will be used. There are 7 possible kernels in total which are shown in the following animation:\ndensity_anim(\n  kernel = c(\"gaussian\", \"epanechnikov\", \"rectangular\", \"triangular\",\n             \"biweight\", \"cosine\", \"optcosine\"),\n  duration = 7\n)\n\nChoice of kernel function also considerably affects the output shape of density curve, but not as greatly as adjust. Most notable difference from default “gaussian” kernel is with “rectangular” kernel: result, unsurprisingly, is “more rectangular”."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#observation-importance-weights",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#observation-importance-weights",
    "title": "Arguments of stats::density()",
    "section": "Observation importance weights",
    "text": "Observation importance weights\nArgument weights should be used if some observations are considered to be “more important” and are “more reference” than the other ones. It should be a numeric vector with the same length as input sample x. Note that for output to be a true density plot, sum of weights should be 1.\nTo illustrate its effect, lets construct a sequence of weights each of which makes one observation having 10 times more weight than any other (that contribute equally). Order of those observations we will choose so that they progress from the smallest to the biggest.\nweights_list &lt;- lapply(order(mtcars$mpg), function(i) {\n  res &lt;- c(rep(1, times = i-1), 10, rep(1, times = length(mtcars$mpg)-i))\n\n  res / sum(res)\n})\n\ndensity_anim(weights = weights_list)\n\nAs expected from weights_list construction, one can see a “spike” that “travels” from left to right, indicating the position of “most important” observation."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#number-of-points-n",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#number-of-points-n",
    "title": "Arguments of stats::density()",
    "section": "Number of points n",
    "text": "Number of points n\nn is responsible for number of returned x-y points. They are equally spaced on the specified range (see next sections). Default value is 512 (taken as a power of two for performance reasons of fft()), but to illustrate its effect on output lets use sequence from 2 to 50.\ndensity_anim(n = 2:50)\n\nAs you can see, in this case values higher than 40 already give considerably small errors (compared to “true curve” when n is infinity). However, if the underlying curve isn’t that smooth (for example, in case of low adjust) it is a good idea to use more points with default 512 being enough for most practical situations."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#left-edge-from",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#left-edge-from",
    "title": "Arguments of stats::density()",
    "section": "Left edge from",
    "text": "Left edge from\nIf specified, from defines a left edge of sequence of “x” points, at which density curve is evaluated. For illustration, lets use sequence from minimum to maximum values of mtcars$mpg.\nfrom_vec &lt;- seq(from = min(mtcars$mpg), to = max(mtcars$mpg), length.out = 20)\n\ndensity_anim(from = round(from_vec, digits = 1))\n\nNote that when from is equal to the smallest value of input numeric vector, it is still greater than left edge of default density curve. For explanation of this “extending property” of default curve, see section about cut."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#right-edge-to",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#right-edge-to",
    "title": "Arguments of stats::density()",
    "section": "Right edge to",
    "text": "Right edge to\nArgument to has the same nature as from, but defines right edge of points.\nto_vec &lt;- rev(from_vec)\n\ndensity_anim(to = round(to_vec, digits = 1))\n\nNote again, that value of to equal to maximum of input numeric vector is not enough to see the default behavior."
  },
  {
    "objectID": "blog/2019-08-08-arguments-of-stats-density.html#range-extension-cut",
    "href": "blog/2019-08-08-arguments-of-stats-density.html#range-extension-cut",
    "title": "Arguments of stats::density()",
    "section": "Range extension cut",
    "text": "Range extension cut\nBy default, sequence of n x-y points is computed as follows:\n\nEqually spaced grid of n “x” points is computed between from and to which by default are computed as being “cut*bw outside of range(x)”. In other words, default range is extended to left and right of range(x) by the amount of “canonical bandwidth” (computed by bw algorithm) multiplied by argument cut (default being 3).\n“Y” points are taken from “true curve” of kernel density estimate.\n\ndensity_anim(cut = seq(3, 0, by = -0.2))\n\nWhen cut changes from 3 to 0, range of computed points shrinks from default one to range of input numeric vector. Default call to density() computes set of points outside of original range. I call this behavior an “extending property” of density(). Note that cut can also be negative, which means reduced input range.\nSo by default, density() extends range of input vector. The problem is that it can contradict natural constraints on input. For example, what if you want to estimate density for probability distribution of value that can only be positive? Dealing with boundary values during kernel density estimation is an important topic and it is called a boundary correction problem. One of possible solutions is to use form_resupport() function from ‘pdqr’ package."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Text editing with ‘mini.nvim’ - Neovimconf 2024\nHow to write 35+ plugins and not become more insane - Neovimconf 2023"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Evgeni Chasnovski",
    "section": "",
    "text": "My name is Evgeni Chasnovski. I am a mathematician and a data scientist from Kharkiv (Ukraine) with a strong passion for data analysis, Neovim, and R.\nI enjoy maintaining and frequently contributing to Open Source Software, as well as occasionally writing blog posts.\n\nProjects\nBlog\nTalks"
  }
]